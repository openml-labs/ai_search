{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAG Pipeline for OpenML","text":"<ul> <li>This repository contains the code for the RAG pipeline for OpenML. </li> <li>Documentation page</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Clone the repository</li> <li>Create a virtual environment and activate it (<code>python -m venv venv &amp;&amp;source venv/bin/activate</code>)</li> <li>Run <code>poetry install</code> to install the packages </li> <li>If you are a dev and have a data/ folder with you, skip this step.</li> <li>Run training.py (for the first time/to update the model). This takes care of basically everything.</li> <li>Install Ollama (https://ollama.com/) and download the models <code>ollama pull llama3</code></li> <li>Run <code>./start_local.sh</code> to start all the services, and navigate to <code>http://localhost:8501</code> to access the Streamlit frontend.</li> <li>WAIT for a few minutes for the services to start. The terminal should show \"sending first query to avoid cold start\". Things will only work after this message.</li> <li>Do not open the uvicorn server directly, as it will not work.</li> <li>To stop the services, run <code>./stop_local.sh</code> from a different terminal.</li> <li>Enjoy :)</li> </ul>"},{"location":"#cli-access-to-the-api","title":"CLI access to the API","text":"<ul> <li>We all are lazy sometimes and don't want to use the interface sometimes. Or just want to test out different parts of the API without any hassle. To that end, you can either test out the individual components like so: </li> <li>Note that the <code>%20</code> are spaces in the URL. </li> </ul>"},{"location":"#ollama","title":"Ollama","text":"<ul> <li>This is the server that runs an Ollama server (This is basically an optimized version of a local LLM. It does not do anything of itself but runs as a background service so you can use the LLM). </li> <li>You can start it by running <code>cd ollama &amp;&amp; ./get_ollama.sh &amp;</code></li> </ul>"},{"location":"#llm-service","title":"LLM Service","text":"<ul> <li>This component is the one that runs the query processing using LLMs module. It uses the Ollama server, runs queries and processes them. </li> <li>You can start it by running <code>cd llm_service &amp;&amp; uvicorn llm_service:app --host 0.0.0.0 --port 8081 &amp;</code></li> <li>Curl Example : <code>curl http://0.0.0.0:8081/llmquery/find%20me%20a%20mushroom%20dataset%20with%20less%20than%203000%20classes</code></li> </ul>"},{"location":"#backend","title":"Backend","text":"<ul> <li>This component runs the RAG pipeline. It returns a JSON with dataset ids of the OpenML datasets that match the query.</li> <li>You can start it by running <code>cd backend &amp;&amp; uvicorn backend:app --host 0.0.0.0 --port 8000 &amp;</code></li> <li>Curl Example : <code>curl http://0.0.0.0:8000/dataset/find%20me%20a%20mushroom%20dataset</code></li> </ul>"},{"location":"#frontend","title":"Frontend","text":"<ul> <li>This component runs the Streamlit frontend. It is the UI that you see when you navigate to <code>http://localhost:8501</code>.</li> <li>You can start it by running <code>cd frontend &amp;&amp; streamlit run ui.py &amp;</code></li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#rag","title":"RAG","text":"<ul> <li>Multi-threaded downloading of OpenML datasets</li> <li>Combining all information about a dataset to enable searching</li> <li>Chunking of the data to prevent OOM errors </li> <li>Hashed entries by document content to prevent duplicate entries to the database</li> <li>RAG pipeline for OpenML datasets/flows</li> <li>Specify config[\"training\"] = True/False to switch between training and inference mode</li> <li>Option to modify the prompt before querying the database (Example: HTML string replacements)</li> <li>Results are returned in either JSON</li> <li>Easily customizable pipeline</li> <li>Easily run multiple queries and aggregate the results</li> <li>LLM based query processing to enable \"smart\" filters</li> <li>Streamlit frontend for now</li> </ul>"},{"location":"#enhancements","title":"Enhancements","text":"<ul> <li>One config file to rule them all</li> <li>GUI search interface using FastAPI with extra search bar for the results</li> <li>Option for Long Context Reordering of results (https://arxiv.org/abs//2307.03172)</li> <li>Option for FlashRank Reranking of results</li> <li>Caching queries in a database for faster retrieval</li> <li>Auto detect and use GPU/MPS if available for training</li> <li>Auto retry failed queries for a set number of times (2)</li> </ul>"},{"location":"#example-usage","title":"Example usage","text":"<ul> <li>Note that in this picture, I am using a very very tiny model for demonstration purposes. The actual results would be a lot better :)</li> <li></li> </ul>"},{"location":"#where-do-i-go-from-here","title":"Where do I go from here?","text":""},{"location":"#i-am-a-developer-and-i-want-to-contribute-to-the-project","title":"I am a developer and I want to contribute to the project","text":"<ul> <li>Hello! We are glad you are here. To get started, refer to the tutorials in the developer tutorial section.</li> <li>If you have any questions, feel free to ask or post an issue.</li> </ul>"},{"location":"#i-just-want-to-use-the-pipeline","title":"I just want to use the pipeline","text":"<ul> <li>You can use the pipeline by running the Streamlit frontend. Refer to the getting started section above for more details.</li> </ul>"},{"location":"Documentation%20Bot/","title":"Documentation Bot","text":"<ul> <li>This bot reads the documentation of OpenML and trains an LLM model to answer questions about the project.</li> <li>It consists of two main parts</li> <li>A crawler that reads the documentation and stores the links and text information.</li> <li>The LLM utils -&gt; Vectore store, conversational langchain, history.</li> </ul>"},{"location":"Documentation%20Bot/#how-to-run","title":"How to run","text":"<ul> <li>First run the crawler to get the documentation from OpenML. This will create a <code>data</code> folder with the documentation in it. <code>python run_crawler.py</code></li> <li>For inference, run <code>uvicorn documentation_query:app --host 0.0.0.0 --port 8083 &amp;</code></li> </ul>"},{"location":"Documentation%20Bot/api_reference/","title":"Api reference","text":""},{"location":"Documentation%20Bot/api_reference/#documentation-bot","title":"Documentation Bot","text":""},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.ChromaStore","title":"<code>ChromaStore</code>","text":"Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>class ChromaStore:\n    def __init__(\n        self,\n        rag_model_name,\n        crawled_files_data_path,\n        chroma_file_path,\n        generation_llm,\n    ) -&gt; None:\n        self.rag_model_name = rag_model_name\n        self.device = find_device()\n        self.hf_embedding_function = HuggingFaceBgeEmbeddings(\n            model_name=self.rag_model_name,\n            model_kwargs={\"device\": self.device},\n            encode_kwargs={\"normalize_embeddings\": True},\n        )\n        self.crawled_files_data_path = crawled_files_data_path\n        self.chroma_file_path = chroma_file_path\n\n        self.system_prompt = (\n            \"You are an assistant for question-answering tasks related to OpenML. \"\n            \"Use the following pieces of retrieved context to answer \"\n            \"the question. You must return instructions with code, always with code\"\n            \"\\n\\n\"\n            \"{context}\"\n        )\n        self.contextualize_q_system_prompt = (\n            \"Given a chat history and the latest user question \"\n            \"which might reference context in the chat history about OpenML, \"\n            \"formulate a standalone question which can be understood \"\n            \"without the chat history. If the question can be answer withou previous context\"\n            \"also reformulate the question. Do NOT answer the question, \"\n            \"just reformulate it if needed and otherwise return it as is.\"\n        )\n        self.generation_llm = generation_llm\n\n\n    def read_data_and_embed(self):  # inference\n        \"\"\"\n        Description: This function is used to read the crawled data and embed it using the Hugging Face model.\n\n        \"\"\"\n        if not os.path.exists(self.crawled_files_data_path):\n            print(\"Crawled data does not exist. Please run the crawler first.\")\n            return\n\n        df = pd.read_csv(self.crawled_files_data_path)\n        df[\"joined\"] = df.apply(self._join_columns, axis=1)\n        docs = DataFrameLoader(df, page_content_column=\"joined\").load()\n\n\n        # Splitting the document texts into smaller chunks\n        docs_texts = self._split_documents(docs)\n\n        # Convert metadata values to strings\n        for doc in docs_texts:\n            doc.metadata = {k: str(v) for k, v in doc.metadata.items()}\n\n        print(\"Creating the vector store\")\n        Chroma.from_documents(\n            documents=docs_texts,\n            embedding=self.hf_embedding_function,\n            persist_directory=self.chroma_file_path,\n        )\n\n    def _join_columns(self, row) -&gt; str:\n        \"\"\"\n        Joins specified columns of a row into a single string.\n        \"\"\"\n        columns = [\n            \"URL\",\n            \"Body Text\",\n            \"Header Links Text\",\n            \"H1\",\n            \"H2\",\n            \"H3\",\n            \"H4\",\n            \"Title\",\n        ]\n        return \", \".join(\n            f\"{col.lower().replace(' ', '_')}: {row[col]}\" for col in columns\n        )\n\n    def _split_documents(self, docs):\n        \"\"\"\n        Splits documents into chunks using RecursiveCharacterTextSplitter.\n        \"\"\"\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=5000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n        )\n        return splitter.split_documents(docs)\n\n    def setup_inference(self, session_id: str) -&gt; None:\n        \"\"\"\n        Description: This function is used to setup the inference for the bot.\n\n        \"\"\"\n        self.store = {}\n        self.session_id = session_id\n        self.vectorstore = Chroma(\n            persist_directory=self.chroma_file_path,\n            embedding_function=self.hf_embedding_function,\n        )\n        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 1})\n\n    def openml_page_search(self, input: str):\n\n        \"\"\"\n        Description: Use the Chroma vector store to search for the most relevant page to the input question , contextualize the question and answer it.\n\n        \"\"\"\n\n        ### Contextualize question ###\n        contextualize_q_prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", self.contextualize_q_system_prompt),\n                MessagesPlaceholder(\"chat_history\"),\n                (\"human\", \"{input}\"),\n            ]\n        )\n\n        history_aware_retriever = create_history_aware_retriever(\n            self.generation_llm, self.retriever, contextualize_q_prompt\n        )\n\n        ### Answer question ###\n\n        qa_prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", self.system_prompt),\n                MessagesPlaceholder(\"chat_history\"),\n                (\"human\", \"{input}\"),\n            ]\n        )\n\n        question_answer_chain = create_stuff_documents_chain(\n            self.generation_llm, qa_prompt\n        )\n\n        rag_chain = create_retrieval_chain(\n            history_aware_retriever, question_answer_chain\n        )\n\n        conversational_rag_chain = RunnableWithMessageHistory(\n            rag_chain,\n            get_session_history,\n            input_messages_key=\"input\",\n            history_messages_key=\"chat_history\",\n            output_messages_key=\"answer\",\n        )\n\n\n        answer = conversational_rag_chain.stream(\n            {\"input\": f\"{input}\"},\n            config={\n                \"configurable\": {\"session_id\": self.session_id}\n            },  # constructs a key \"abc123\" in `store`.\n        )\n        return answer\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.ChromaStore.openml_page_search","title":"<code>openml_page_search(input)</code>","text":"<p>Description: Use the Chroma vector store to search for the most relevant page to the input question , contextualize the question and answer it.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def openml_page_search(self, input: str):\n\n    \"\"\"\n    Description: Use the Chroma vector store to search for the most relevant page to the input question , contextualize the question and answer it.\n\n    \"\"\"\n\n    ### Contextualize question ###\n    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", self.contextualize_q_system_prompt),\n            MessagesPlaceholder(\"chat_history\"),\n            (\"human\", \"{input}\"),\n        ]\n    )\n\n    history_aware_retriever = create_history_aware_retriever(\n        self.generation_llm, self.retriever, contextualize_q_prompt\n    )\n\n    ### Answer question ###\n\n    qa_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", self.system_prompt),\n            MessagesPlaceholder(\"chat_history\"),\n            (\"human\", \"{input}\"),\n        ]\n    )\n\n    question_answer_chain = create_stuff_documents_chain(\n        self.generation_llm, qa_prompt\n    )\n\n    rag_chain = create_retrieval_chain(\n        history_aware_retriever, question_answer_chain\n    )\n\n    conversational_rag_chain = RunnableWithMessageHistory(\n        rag_chain,\n        get_session_history,\n        input_messages_key=\"input\",\n        history_messages_key=\"chat_history\",\n        output_messages_key=\"answer\",\n    )\n\n\n    answer = conversational_rag_chain.stream(\n        {\"input\": f\"{input}\"},\n        config={\n            \"configurable\": {\"session_id\": self.session_id}\n        },  # constructs a key \"abc123\" in `store`.\n    )\n    return answer\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.ChromaStore.read_data_and_embed","title":"<code>read_data_and_embed()</code>","text":"<p>Description: This function is used to read the crawled data and embed it using the Hugging Face model.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def read_data_and_embed(self):  # inference\n    \"\"\"\n    Description: This function is used to read the crawled data and embed it using the Hugging Face model.\n\n    \"\"\"\n    if not os.path.exists(self.crawled_files_data_path):\n        print(\"Crawled data does not exist. Please run the crawler first.\")\n        return\n\n    df = pd.read_csv(self.crawled_files_data_path)\n    df[\"joined\"] = df.apply(self._join_columns, axis=1)\n    docs = DataFrameLoader(df, page_content_column=\"joined\").load()\n\n\n    # Splitting the document texts into smaller chunks\n    docs_texts = self._split_documents(docs)\n\n    # Convert metadata values to strings\n    for doc in docs_texts:\n        doc.metadata = {k: str(v) for k, v in doc.metadata.items()}\n\n    print(\"Creating the vector store\")\n    Chroma.from_documents(\n        documents=docs_texts,\n        embedding=self.hf_embedding_function,\n        persist_directory=self.chroma_file_path,\n    )\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.ChromaStore.setup_inference","title":"<code>setup_inference(session_id)</code>","text":"<p>Description: This function is used to setup the inference for the bot.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def setup_inference(self, session_id: str) -&gt; None:\n    \"\"\"\n    Description: This function is used to setup the inference for the bot.\n\n    \"\"\"\n    self.store = {}\n    self.session_id = session_id\n    self.vectorstore = Chroma(\n        persist_directory=self.chroma_file_path,\n        embedding_function=self.hf_embedding_function,\n    )\n    self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 1})\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.Crawler","title":"<code>Crawler</code>","text":"<p>Description: This class is used to crawl the OpenML website and gather both code and general information for a bot.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>class Crawler:\n    \"\"\"\n    Description: This class is used to crawl the OpenML website and gather both code and general information for a bot.\n    \"\"\"\n\n    def __init__(\n        self,\n        crawled_files_data_path,\n        recrawl_websites=False,\n        num_of_websites_to_crawl=None,\n    ):\n        with open(\"./base_urls.txt\", \"r\") as f:\n            self.base_urls = f.read().splitlines()\n        self.crawled_files_data_path = crawled_files_data_path\n        self.recrawl_websites = recrawl_websites\n        self.num_of_websites_to_crawl = num_of_websites_to_crawl\n        self.crawl_count = 0\n        self.visited = set()\n        self.data_queue = []\n\n    def extract_text_from_tags(self, soup, tags):\n        \"\"\"Extract and return the concatenated text from all given tags.\"\"\"\n        return {\n            tag: \" \".join(\n                element.get_text(strip=True) for element in soup.find_all(tag)\n            )\n            for tag in tags\n        }\n\n    def fetch_soup(self, url):\n        \"\"\"Fetch and return a BeautifulSoup object for the given URL.\"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            return BeautifulSoup(response.text, \"html.parser\")\n        except requests.RequestException as e:\n            print(f\"Failed to retrieve {url}: {e}\")\n            return None\n\n    def extract_data(self, url):\n        \"\"\"Extract and return relevant data from the given URL.\"\"\"\n        soup = self.fetch_soup(url)\n        if not soup:\n            return None\n\n        tags_to_extract = [\"h1\", \"h2\", \"h3\", \"h4\"]\n        header_data = self.extract_text_from_tags(soup, tags_to_extract)\n\n        return {\n            \"url\": url,\n            \"title\": soup.title.string if soup.title else \"No title\",\n            \"body_text\": (\n                soup.body.get_text(separator=\" \", strip=True)\n                if soup.body\n                else \"No body text\"\n            ),\n            \"header_links_text\": \" \".join(\n                link.get_text(strip=True) for link in soup.find_all(\"a\", href=True)\n            ),\n            **header_data,\n        }\n\n    def save_data(self, writer):\n        \"\"\"Save all extracted data in the queue to the CSV file.\"\"\"\n        for data in self.data_queue:\n            writer.writerow(\n                [\n                    data[\"url\"],\n                    data[\"body_text\"],\n                    data[\"header_links_text\"],\n                    data[\"h1\"],\n                    data[\"h2\"],\n                    data[\"h3\"],\n                    data[\"h4\"],\n                    data[\"title\"],\n                ]\n            )\n\n    def crawl(self, url, progress_bar):\n        \"\"\"Crawl the given URL and its linked pages.\"\"\"\n        try:\n            if url in self.visited or (\n                self.num_of_websites_to_crawl\n                and self.crawl_count &gt;= self.num_of_websites_to_crawl\n            ):\n                return\n\n            self.visited.add(url)\n            self.crawl_count += 1\n            progress_bar.update(1)  # Update progress bar\n\n            data = self.extract_data(url)\n            if data:\n                self.data_queue.append(data)\n\n            soup = self.fetch_soup(url)\n            if soup:\n                for link in soup.find_all(\"a\", href=True):\n                    full_url = urljoin(url, link[\"href\"])\n                    if any(\n                        full_url.startswith(base_url) for base_url in self.base_urls\n                    ):\n                        self.crawl(full_url, progress_bar)\n        except RecursionError:\n            print(f\"Recursion error while crawling {url}\")\n\n    def do_crawl(self):\n        \"\"\"Manage the entire crawling and saving process with a progress bar.\"\"\"\n        if not self.recrawl_websites and os.path.exists(self.crawled_files_data_path):\n            print(\"Data already exists. Set recrawl_websites=True to recrawl.\")\n            return\n\n        os.makedirs(os.path.dirname(self.crawled_files_data_path), exist_ok=True)\n\n        print(\"Crawling the websites...\")\n\n        # the progress bar is not accurate because we don't know the total number of URLs to crawl. this is just to see if the script is running or not\n\n        total_urls = self.num_of_websites_to_crawl or len(\n            self.base_urls\n        )  # Estimate total URLs for progress bar\n        with tqdm(total=total_urls, desc=\"Crawling URLs\") as progress_bar:\n            for start_url in self.base_urls:\n                self.crawl(start_url, progress_bar)\n\n        with open(\n            self.crawled_files_data_path, mode=\"w\", newline=\"\", encoding=\"utf-8\"\n        ) as file:\n            writer = csv.writer(file)\n            writer.writerow(\n                [\n                    \"URL\",\n                    \"Body Text\",\n                    \"Header Links Text\",\n                    \"H1\",\n                    \"H2\",\n                    \"H3\",\n                    \"H4\",\n                    \"Title\",\n                ]\n            )\n\n            self.save_data(writer)\n\n        print(\"Crawling complete.\")\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.Crawler.crawl","title":"<code>crawl(url, progress_bar)</code>","text":"<p>Crawl the given URL and its linked pages.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def crawl(self, url, progress_bar):\n    \"\"\"Crawl the given URL and its linked pages.\"\"\"\n    try:\n        if url in self.visited or (\n            self.num_of_websites_to_crawl\n            and self.crawl_count &gt;= self.num_of_websites_to_crawl\n        ):\n            return\n\n        self.visited.add(url)\n        self.crawl_count += 1\n        progress_bar.update(1)  # Update progress bar\n\n        data = self.extract_data(url)\n        if data:\n            self.data_queue.append(data)\n\n        soup = self.fetch_soup(url)\n        if soup:\n            for link in soup.find_all(\"a\", href=True):\n                full_url = urljoin(url, link[\"href\"])\n                if any(\n                    full_url.startswith(base_url) for base_url in self.base_urls\n                ):\n                    self.crawl(full_url, progress_bar)\n    except RecursionError:\n        print(f\"Recursion error while crawling {url}\")\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.Crawler.do_crawl","title":"<code>do_crawl()</code>","text":"<p>Manage the entire crawling and saving process with a progress bar.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def do_crawl(self):\n    \"\"\"Manage the entire crawling and saving process with a progress bar.\"\"\"\n    if not self.recrawl_websites and os.path.exists(self.crawled_files_data_path):\n        print(\"Data already exists. Set recrawl_websites=True to recrawl.\")\n        return\n\n    os.makedirs(os.path.dirname(self.crawled_files_data_path), exist_ok=True)\n\n    print(\"Crawling the websites...\")\n\n    # the progress bar is not accurate because we don't know the total number of URLs to crawl. this is just to see if the script is running or not\n\n    total_urls = self.num_of_websites_to_crawl or len(\n        self.base_urls\n    )  # Estimate total URLs for progress bar\n    with tqdm(total=total_urls, desc=\"Crawling URLs\") as progress_bar:\n        for start_url in self.base_urls:\n            self.crawl(start_url, progress_bar)\n\n    with open(\n        self.crawled_files_data_path, mode=\"w\", newline=\"\", encoding=\"utf-8\"\n    ) as file:\n        writer = csv.writer(file)\n        writer.writerow(\n            [\n                \"URL\",\n                \"Body Text\",\n                \"Header Links Text\",\n                \"H1\",\n                \"H2\",\n                \"H3\",\n                \"H4\",\n                \"Title\",\n            ]\n        )\n\n        self.save_data(writer)\n\n    print(\"Crawling complete.\")\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.Crawler.extract_data","title":"<code>extract_data(url)</code>","text":"<p>Extract and return relevant data from the given URL.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def extract_data(self, url):\n    \"\"\"Extract and return relevant data from the given URL.\"\"\"\n    soup = self.fetch_soup(url)\n    if not soup:\n        return None\n\n    tags_to_extract = [\"h1\", \"h2\", \"h3\", \"h4\"]\n    header_data = self.extract_text_from_tags(soup, tags_to_extract)\n\n    return {\n        \"url\": url,\n        \"title\": soup.title.string if soup.title else \"No title\",\n        \"body_text\": (\n            soup.body.get_text(separator=\" \", strip=True)\n            if soup.body\n            else \"No body text\"\n        ),\n        \"header_links_text\": \" \".join(\n            link.get_text(strip=True) for link in soup.find_all(\"a\", href=True)\n        ),\n        **header_data,\n    }\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.Crawler.extract_text_from_tags","title":"<code>extract_text_from_tags(soup, tags)</code>","text":"<p>Extract and return the concatenated text from all given tags.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def extract_text_from_tags(self, soup, tags):\n    \"\"\"Extract and return the concatenated text from all given tags.\"\"\"\n    return {\n        tag: \" \".join(\n            element.get_text(strip=True) for element in soup.find_all(tag)\n        )\n        for tag in tags\n    }\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.Crawler.fetch_soup","title":"<code>fetch_soup(url)</code>","text":"<p>Fetch and return a BeautifulSoup object for the given URL.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def fetch_soup(self, url):\n    \"\"\"Fetch and return a BeautifulSoup object for the given URL.\"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        return BeautifulSoup(response.text, \"html.parser\")\n    except requests.RequestException as e:\n        print(f\"Failed to retrieve {url}: {e}\")\n        return None\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.Crawler.save_data","title":"<code>save_data(writer)</code>","text":"<p>Save all extracted data in the queue to the CSV file.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def save_data(self, writer):\n    \"\"\"Save all extracted data in the queue to the CSV file.\"\"\"\n    for data in self.data_queue:\n        writer.writerow(\n            [\n                data[\"url\"],\n                data[\"body_text\"],\n                data[\"header_links_text\"],\n                data[\"h1\"],\n                data[\"h2\"],\n                data[\"h3\"],\n                data[\"h4\"],\n                data[\"title\"],\n            ]\n        )\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.find_device","title":"<code>find_device()</code>","text":"<p>Determines the best available device: 'cuda', 'mps', or 'cpu'.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def find_device() -&gt; str:\n    \"\"\"\n    Determines the best available device: 'cuda', 'mps', or 'cpu'.\n    \"\"\"\n    if torch.cuda.is_available():\n        return \"cuda\"\n    if torch.backends.mps.is_available():\n        return \"mps\"\n    return \"cpu\"\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.get_session_history","title":"<code>get_session_history(session_id)</code>","text":"<p>Description: This function is used to get the chat history of a session.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def get_session_history(session_id: str) -&gt; BaseChatMessageHistory:\n    \"\"\"\n    Description: This function is used to get the chat history of a session.\n\n    \"\"\"\n    # print(\"this is the session id\", session_id)\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    # print(\"this is the store id\", store[session_id])\n    return store[session_id]\n</code></pre>"},{"location":"Documentation%20Bot/api_reference/#documentation_query_utils.stream_response","title":"<code>stream_response(response)</code>","text":"<p>Description: This function is used to stream the response from the model.</p> Source code in <code>documentation_bot/documentation_query_utils.py</code> <pre><code>def stream_response(response):\n    \"\"\"\n    Description: This function is used to stream the response from the model.\n\n    \"\"\"\n    for line in response:\n        try:\n            yield str(line[\"answer\"])\n        except GeneratorExit:\n            break\n        except:\n            yield \"\"\n</code></pre>"},{"location":"Inference/docker/","title":"Docker container","text":""},{"location":"Inference/docker/#building","title":"Building","text":"<ul> <li>Run <code>docker compose build --progress=plain</code></li> </ul>"},{"location":"Inference/docker/#running","title":"Running","text":"<ul> <li>Run <code>./start_docker.sh</code></li> <li>This uses the docker compose file to run the docker process in the background.</li> <li>The required LLM model is also pulled from the docker hub and the container is started.</li> </ul>"},{"location":"Inference/docker/#stopping","title":"Stopping","text":"<ul> <li>Run <code>./stop_docker.sh</code></li> </ul>"},{"location":"Inference/docker/#potential-errors","title":"Potential Errors","text":"<ul> <li>Permission errors : Run <code>chmod +x *.sh</code></li> <li>If you get a memory error you can run <code>docker system prune</code>. Please be careful with this command as it will remove all stopped containers, all dangling images, and all unused networks. So ensure you have no important data in any of the containers before running this command.</li> <li>On docker desktop for Mac, increase memory limits to as much as your system can handle.</li> </ul>"},{"location":"Inference/inference/","title":"Inference","text":"<ul> <li>Just run ./start_local.sh and it will take care of everything.</li> <li>The UI should either pop up or you can navigate to http://localhost:8501/ in your browser.</li> <li>Note that it takes a decent bit of time to load everything. </li> </ul>"},{"location":"Inference/inference/#stopping","title":"Stopping","text":"<ul> <li>Run ./stop_local.sh</li> <li>./start_local.sh stores the PIDs of all the processes it starts in files in all the directories it starts them in. stop_local.sh reads these files and kills the processes.</li> </ul>"},{"location":"Inference/inference/#cli-access-to-the-api","title":"CLI access to the API","text":"<ul> <li>We all are lazy sometimes and don't want to use the interface sometimes. Or just want to test out different parts of the API without any hassle. To that end, you can either test out the individual components like so: </li> <li>Note that the <code>%20</code> are spaces in the URL. </li> </ul>"},{"location":"Inference/inference/#ollama","title":"Ollama","text":"<ul> <li>This is the server that runs an Ollama server (This is basically an optimized version of a local LLM. It does not do anything of itself but runs as a background service so you can use the LLM). </li> <li>You can start it by running <code>cd ollama &amp;&amp; ./get_ollama.sh &amp;</code></li> </ul>"},{"location":"Inference/inference/#llm-service","title":"LLM Service","text":"<ul> <li>This component is the one that runs the query processing using LLMs module. It uses the Ollama server, runs queries and processes them. </li> <li>You can start it by running <code>cd llm_service &amp;&amp; uvicorn llm_service:app --host 0.0.0.0 --port 8081 &amp;</code></li> <li>Curl Example : <code>curl http://0.0.0.0:8081/llmquery/find%20me%20a%20mushroom%20dataset%20with%20less%20than%203000%20classes</code></li> </ul>"},{"location":"Inference/inference/#backend","title":"Backend","text":"<ul> <li>This component runs the RAG pipeline. It returns a JSON with dataset ids of the OpenML datasets that match the query.</li> <li>You can start it by running <code>cd backend &amp;&amp; uvicorn backend:app --host 0.0.0.0 --port 8000 &amp;</code></li> <li>Curl Example : <code>curl http://0.0.0.0:8000/dataset/find%20me%20a%20mushroom%20dataset</code></li> </ul>"},{"location":"Inference/inference/#frontend","title":"Frontend","text":"<ul> <li>This component runs the Streamlit frontend. It is the UI that you see when you navigate to <code>http://localhost:8501</code>.</li> <li>You can start it by running <code>cd frontend &amp;&amp; streamlit run ui.py &amp;</code></li> </ul>"},{"location":"Inference/inference/#errors","title":"Errors","text":"<ul> <li>If you get an error about file permissions, run <code>chmod +x start_local.sh</code> and <code>chmod +x stop_local.sh</code> to make them executable.</li> </ul>"},{"location":"Inference/inference/#ui_utils.LLMResponseParser","title":"<code>LLMResponseParser</code>","text":"<p>Description: Parse the response from the LLM service and update the columns based on the response.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>class LLMResponseParser:\n    \"\"\"\n    Description: Parse the response from the LLM service and update the columns based on the response.\n    \"\"\"\n\n    def __init__(self, llm_response):\n        self.llm_response = llm_response\n        self.subset_cols = [\"did\", \"name\"]\n        self.size_sort = None\n        self.classification_type = None\n        self.uploader_name = None\n\n    def process_size_attribute(self, attr_size: str):\n        size, sort = attr_size.split(\",\") if \",\" in attr_size else (attr_size, None)\n        if size == \"yes\":\n            self.subset_cols.append(\"NumberOfInstances\")\n        if sort:\n            self.size_sort = sort\n\n    def missing_values_attribute(self, attr_missing: str):\n        if attr_missing == \"yes\":\n            self.subset_cols.append(\"NumberOfMissingValues\")\n\n    def classification_type_attribute(self, attr_classification: str):\n        if attr_classification != \"none\":\n            self.subset_cols.append(\"NumberOfClasses\")\n            self.classification_type = attr_classification\n\n    def uploader_attribute(self, attr_uploader: str):\n        if attr_uploader != \"none\":\n            self.subset_cols.append(\"uploader\")\n            self.uploader_name = attr_uploader.split(\"=\")[1].strip()\n\n    def get_attributes_from_response(self):\n        attribute_processors = {\n            \"size_of_dataset\": self.process_size_attribute,\n            \"missing_values\": self.missing_values_attribute,\n            \"classification_type\": self.classification_type_attribute,\n            \"uploader\": self.uploader_attribute,\n        }\n\n        for attribute, value in self.llm_response.items():\n            if attribute in attribute_processors:\n                attribute_processors[attribute](value)\n\n    def update_subset_cols(self, metadata: pd.DataFrame):\n        \"\"\"\n        Description: Filter the metadata based on the updated subset columns and extra conditions\n        \"\"\"\n        if self.classification_type is not None:\n            if \"multi\" in self.classification_type:\n                metadata = metadata[metadata[\"NumberOfClasses\"] &gt; 2]\n            elif \"binary\" in self.classification_type:\n                metadata = metadata[metadata[\"NumberOfClasses\"] == 2]\n        if self.uploader_name is not None:\n            try:\n                uploader = int(self.uploader_name)\n                metadata = metadata[metadata[\"uploader\"] == uploader]\n            except:\n                pass\n\n        return metadata[self.subset_cols]\n</code></pre>"},{"location":"Inference/inference/#ui_utils.LLMResponseParser.update_subset_cols","title":"<code>update_subset_cols(metadata)</code>","text":"<p>Description: Filter the metadata based on the updated subset columns and extra conditions</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def update_subset_cols(self, metadata: pd.DataFrame):\n    \"\"\"\n    Description: Filter the metadata based on the updated subset columns and extra conditions\n    \"\"\"\n    if self.classification_type is not None:\n        if \"multi\" in self.classification_type:\n            metadata = metadata[metadata[\"NumberOfClasses\"] &gt; 2]\n        elif \"binary\" in self.classification_type:\n            metadata = metadata[metadata[\"NumberOfClasses\"] == 2]\n    if self.uploader_name is not None:\n        try:\n            uploader = int(self.uploader_name)\n            metadata = metadata[metadata[\"uploader\"] == uploader]\n        except:\n            pass\n\n    return metadata[self.subset_cols]\n</code></pre>"},{"location":"Inference/inference/#ui_utils.ResponseParser","title":"<code>ResponseParser</code>","text":"<p>Description : This classe is used to decide the order of operations and run the response parsing. It loads the paths, fetches the Query parsing LLM response, the rag response, loads the metadatas and then based on the config, decides the order in which to apply each of them.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>class ResponseParser:\n    \"\"\"\n    Description : This classe is used to decide the order of operations and run the response parsing.\n    It loads the paths, fetches the Query parsing LLM response, the rag response, loads the metadatas and then based on the config, decides the order in which to apply each of them.\n    \"\"\"\n\n    def __init__(self, query_type: str, apply_llm_before_rag: bool = False):\n        self.query_type = query_type\n        self.paths = self.load_paths()\n        self.rag_response = None\n        self.llm_response = None\n        self.apply_llm_before_rag = apply_llm_before_rag\n        self.database_filtered = None\n        self.structured_query_response = None\n\n    def load_paths(self):\n        \"\"\"\n        Description: Load paths from paths.json\n        \"\"\"\n        with open(\"paths.json\", \"r\") as file:\n            return json.load(file)\n\n    def fetch_llm_response(self, query: str):\n        \"\"\"\n        Description: Fetch the response from the query parsing LLM service as a json\n        \"\"\"\n        llm_response_path = self.paths[\"llm_response\"]\n        try:\n            self.llm_response = requests.get(\n                f\"{llm_response_path['docker']}{query}\"\n            ).json()\n        except:\n            self.llm_response = requests.get(\n                f\"{llm_response_path['local']}{query}\"\n            ).json()\n        return self.llm_response\n\n    def fetch_structured_query(self, query_type: str, query: str):\n        \"\"\"\n        Description: Fetch the response for a structured query from the LLM service as a JSON\n        \"\"\"\n        structured_response_path = self.paths[\"structured_query\"]\n        try:\n            self.structured_query_response = requests.get(\n                f\"{structured_response_path['docker']}{query}\",\n                json={\"query\": query},\n            ).json()\n        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n            # Print the error for debugging purposes\n            print(f\"Error occurred: {e}\")\n            # Set structured_query_response to None on error\n            self.structured_query_response = None\n        try:\n            self.structured_query_response = requests.get(\n                f\"{structured_response_path['local']}{query}\",\n                json={\"query\": query},\n            ).json()\n        except Exception as e:\n            # Print the error for debugging purposes\n            print(f\"Error occurred while fetching from local endpoint: {e}\")\n            # Set structured_query_response to None if the local request also fails\n            self.structured_query_response = None\n\n        return self.structured_query_response\n\n    def database_filter(self, filter_condition, collec):\n        \"\"\"\n        Apply database filter on the rag_response\n        \"\"\"\n        ids = list(map(str, self.rag_response[\"initial_response\"]))\n        self.database_filtered = collec.get(ids=ids, where=filter_condition)[\"ids\"]\n        self.database_filtered = list(map(int, self.database_filtered))\n        # print(self.database_filtered)\n        return self.database_filtered\n\n    def fetch_rag_response(self, query_type, query):\n        \"\"\"\n        Description: Fetch the response from RAG pipeline\n\n        \"\"\"\n        rag_response_path = self.paths[\"rag_response\"]\n        try:\n            self.rag_response = requests.get(\n                f\"{rag_response_path['docker']}{query_type.lower()}/{query}\",\n                json={\"query\": query, \"type\": query_type.lower()},\n            ).json()\n        except:\n            self.rag_response = requests.get(\n                f\"{rag_response_path['local']}{query_type.lower()}/{query}\",\n                json={\"query\": query, \"type\": query_type.lower()},\n            ).json()\n        ordered_set = self._order_results()\n        self.rag_response[\"initial_response\"] = ordered_set\n\n        return self.rag_response\n\n    def _order_results(self):\n        doc_set = set()\n        ordered_set = []\n        for docid in self.rag_response[\"initial_response\"]:\n            if docid not in doc_set:\n                ordered_set.append(docid)\n            doc_set.add(docid)\n        return ordered_set\n\n    def parse_and_update_response(self, metadata: pd.DataFrame):\n        \"\"\"\n         Description: Parse the response from the RAG and LLM services and update the metadata based on the response.\n         Decide which order to apply them\n         -  self.apply_llm_before_rag == False\n             - Metadata is filtered based on the rag response first and then by the Query parsing LLM\n        -  self.apply_llm_before_rag == False\n             - Metadata is filtered based by the Query parsing LLM first and the rag response second\n        - in case structured_query == true, take results are applying data filters.\n        \"\"\"\n        if self.apply_llm_before_rag is None or self.llm_response is None:\n            print(\"No LLM filter.\")\n            # print(self.rag_response, flush=True)\n            filtered_metadata = self._no_filter(metadata)\n\n            # print(filtered_metadata)\n            # if no llm response is required, return the initial response\n            return filtered_metadata\n\n        elif (\n            self.rag_response is not None and self.llm_response is not None\n        ) and not config[\"structured_query\"]:\n            if not self.apply_llm_before_rag:\n                filtered_metadata, llm_parser = self._rag_before_llm(metadata)\n\n                if self.query_type.lower() == \"dataset\":\n                    llm_parser.get_attributes_from_response()\n                    return llm_parser.update_subset_cols(filtered_metadata)\n\n            elif self.apply_llm_before_rag:\n                filtered_metadata = self._filter_before_rag(metadata)\n                return filtered_metadata\n\n        elif (\n            self.rag_response is not None and self.structured_query_response is not None\n        ):\n            col_name = [\n                \"status\",\n                \"NumberOfClasses\",\n                \"NumberOfFeatures\",\n                \"NumberOfInstances\",\n            ]\n            # print(self.structured_query_response)  # Only for debugging. Comment later.\n            if self.structured_query_response[0] is not None and isinstance(\n                self.structured_query_response[1], dict\n            ):\n                # Safely attempt to access the \"filter\" key in the first element\n\n                self._structured_query_on_success(metadata)\n\n            else:\n                filtered_metadata = self._structured_query_on_fail(metadata)\n                # print(\"Showing only rag response\")\n            return filtered_metadata[[\"did\", \"name\", *col_name]]\n\n    def _structured_query_on_fail(self, metadata):\n        filtered_metadata = metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n        return filtered_metadata\n\n    def _structured_query_on_success(self, metadata):\n        if (\n            self.structured_query_response[0].get(\"filter\", None)\n            and self.database_filtered\n        ):\n            filtered_metadata = metadata[metadata[\"did\"].isin(self.database_filtered)]\n            # print(\"Showing database filtered data\")\n        else:\n            filtered_metadata = metadata[\n                metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n            ]\n            # print(\n            #     \"Showing only rag response as filter is empty or none of the rag data satisfies filter conditions.\"\n            # )\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n    def _filter_before_rag(self, metadata):\n        print(\"LLM filter before RAG\")\n        llm_parser = LLMResponseParser(self.llm_response)\n        llm_parser.get_attributes_from_response()\n        filtered_metadata = llm_parser.update_subset_cols(metadata)\n        filtered_metadata = filtered_metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n        return filtered_metadata\n\n    def _rag_before_llm(self, metadata):\n        print(\"RAG before LLM filter.\")\n        filtered_metadata = metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n        llm_parser = LLMResponseParser(self.llm_response)\n        return filtered_metadata, llm_parser\n\n    def _no_filter(self, metadata):\n        filtered_metadata = metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n        return filtered_metadata\n</code></pre>"},{"location":"Inference/inference/#ui_utils.ResponseParser.database_filter","title":"<code>database_filter(filter_condition, collec)</code>","text":"<p>Apply database filter on the rag_response</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def database_filter(self, filter_condition, collec):\n    \"\"\"\n    Apply database filter on the rag_response\n    \"\"\"\n    ids = list(map(str, self.rag_response[\"initial_response\"]))\n    self.database_filtered = collec.get(ids=ids, where=filter_condition)[\"ids\"]\n    self.database_filtered = list(map(int, self.database_filtered))\n    # print(self.database_filtered)\n    return self.database_filtered\n</code></pre>"},{"location":"Inference/inference/#ui_utils.ResponseParser.fetch_llm_response","title":"<code>fetch_llm_response(query)</code>","text":"<p>Description: Fetch the response from the query parsing LLM service as a json</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_llm_response(self, query: str):\n    \"\"\"\n    Description: Fetch the response from the query parsing LLM service as a json\n    \"\"\"\n    llm_response_path = self.paths[\"llm_response\"]\n    try:\n        self.llm_response = requests.get(\n            f\"{llm_response_path['docker']}{query}\"\n        ).json()\n    except:\n        self.llm_response = requests.get(\n            f\"{llm_response_path['local']}{query}\"\n        ).json()\n    return self.llm_response\n</code></pre>"},{"location":"Inference/inference/#ui_utils.ResponseParser.fetch_rag_response","title":"<code>fetch_rag_response(query_type, query)</code>","text":"<p>Description: Fetch the response from RAG pipeline</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_rag_response(self, query_type, query):\n    \"\"\"\n    Description: Fetch the response from RAG pipeline\n\n    \"\"\"\n    rag_response_path = self.paths[\"rag_response\"]\n    try:\n        self.rag_response = requests.get(\n            f\"{rag_response_path['docker']}{query_type.lower()}/{query}\",\n            json={\"query\": query, \"type\": query_type.lower()},\n        ).json()\n    except:\n        self.rag_response = requests.get(\n            f\"{rag_response_path['local']}{query_type.lower()}/{query}\",\n            json={\"query\": query, \"type\": query_type.lower()},\n        ).json()\n    ordered_set = self._order_results()\n    self.rag_response[\"initial_response\"] = ordered_set\n\n    return self.rag_response\n</code></pre>"},{"location":"Inference/inference/#ui_utils.ResponseParser.fetch_structured_query","title":"<code>fetch_structured_query(query_type, query)</code>","text":"<p>Description: Fetch the response for a structured query from the LLM service as a JSON</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_structured_query(self, query_type: str, query: str):\n    \"\"\"\n    Description: Fetch the response for a structured query from the LLM service as a JSON\n    \"\"\"\n    structured_response_path = self.paths[\"structured_query\"]\n    try:\n        self.structured_query_response = requests.get(\n            f\"{structured_response_path['docker']}{query}\",\n            json={\"query\": query},\n        ).json()\n    except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n        # Print the error for debugging purposes\n        print(f\"Error occurred: {e}\")\n        # Set structured_query_response to None on error\n        self.structured_query_response = None\n    try:\n        self.structured_query_response = requests.get(\n            f\"{structured_response_path['local']}{query}\",\n            json={\"query\": query},\n        ).json()\n    except Exception as e:\n        # Print the error for debugging purposes\n        print(f\"Error occurred while fetching from local endpoint: {e}\")\n        # Set structured_query_response to None if the local request also fails\n        self.structured_query_response = None\n\n    return self.structured_query_response\n</code></pre>"},{"location":"Inference/inference/#ui_utils.ResponseParser.load_paths","title":"<code>load_paths()</code>","text":"<p>Description: Load paths from paths.json</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def load_paths(self):\n    \"\"\"\n    Description: Load paths from paths.json\n    \"\"\"\n    with open(\"paths.json\", \"r\") as file:\n        return json.load(file)\n</code></pre>"},{"location":"Inference/inference/#ui_utils.ResponseParser.parse_and_update_response","title":"<code>parse_and_update_response(metadata)</code>","text":"<p>Description: Parse the response from the RAG and LLM services and update the metadata based on the response.  Decide which order to apply them  -  self.apply_llm_before_rag == False      - Metadata is filtered based on the rag response first and then by the Query parsing LLM -  self.apply_llm_before_rag == False      - Metadata is filtered based by the Query parsing LLM first and the rag response second - in case structured_query == true, take results are applying data filters.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def parse_and_update_response(self, metadata: pd.DataFrame):\n    \"\"\"\n     Description: Parse the response from the RAG and LLM services and update the metadata based on the response.\n     Decide which order to apply them\n     -  self.apply_llm_before_rag == False\n         - Metadata is filtered based on the rag response first and then by the Query parsing LLM\n    -  self.apply_llm_before_rag == False\n         - Metadata is filtered based by the Query parsing LLM first and the rag response second\n    - in case structured_query == true, take results are applying data filters.\n    \"\"\"\n    if self.apply_llm_before_rag is None or self.llm_response is None:\n        print(\"No LLM filter.\")\n        # print(self.rag_response, flush=True)\n        filtered_metadata = self._no_filter(metadata)\n\n        # print(filtered_metadata)\n        # if no llm response is required, return the initial response\n        return filtered_metadata\n\n    elif (\n        self.rag_response is not None and self.llm_response is not None\n    ) and not config[\"structured_query\"]:\n        if not self.apply_llm_before_rag:\n            filtered_metadata, llm_parser = self._rag_before_llm(metadata)\n\n            if self.query_type.lower() == \"dataset\":\n                llm_parser.get_attributes_from_response()\n                return llm_parser.update_subset_cols(filtered_metadata)\n\n        elif self.apply_llm_before_rag:\n            filtered_metadata = self._filter_before_rag(metadata)\n            return filtered_metadata\n\n    elif (\n        self.rag_response is not None and self.structured_query_response is not None\n    ):\n        col_name = [\n            \"status\",\n            \"NumberOfClasses\",\n            \"NumberOfFeatures\",\n            \"NumberOfInstances\",\n        ]\n        # print(self.structured_query_response)  # Only for debugging. Comment later.\n        if self.structured_query_response[0] is not None and isinstance(\n            self.structured_query_response[1], dict\n        ):\n            # Safely attempt to access the \"filter\" key in the first element\n\n            self._structured_query_on_success(metadata)\n\n        else:\n            filtered_metadata = self._structured_query_on_fail(metadata)\n            # print(\"Showing only rag response\")\n        return filtered_metadata[[\"did\", \"name\", *col_name]]\n</code></pre>"},{"location":"Inference/inference/#ui_utils.UILoader","title":"<code>UILoader</code>","text":"<p>Description : Create the chat interface</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>class UILoader:\n    \"\"\"\n    Description : Create the chat interface\n    \"\"\"\n\n    def __init__(self, config_path):\n        with open(config_path, \"r\") as file:\n            # Load config\n            self.config = json.load(file)\n        # Paths and display information\n\n        # Load metadata chroma database for structured query\n        self.collec = load_chroma_metadata()\n\n        # Metadata paths\n        self.data_metadata_path = (\n            Path(config[\"data_dir\"]) / \"all_dataset_description.csv\"\n        )\n        self.flow_metadata_path = Path(config[\"data_dir\"]) / \"all_flow_description.csv\"\n\n        # Read metadata\n        self.data_metadata = pd.read_csv(self.data_metadata_path)\n        self.flow_metadata = pd.read_csv(self.flow_metadata_path)\n\n        # defaults\n        self.query_type = \"Dataset\"\n        self.llm_filter = False\n        self.paths = self.load_paths()\n        self.info = \"\"\"\n        &lt;p style='text-align: center; color: white;'&gt;Machine learning research should be easily accessible and reusable. &lt;a href = \"https://openml.org/\"&gt;OpenML&lt;/a&gt; is an open platform for sharing datasets, algorithms, and experiments - to learn how to learn better, together. &lt;/p&gt;\n        \"\"\"\n        self.logo = \"images/favicon.ico\"\n        self.chatbot_display = \"How do I do X using OpenML? / Find me a dataset about Y\"\n\n        if \"messages\" not in st.session_state:\n            st.session_state.messages = []\n\n    # container for company description and logo\n    def _generate_logo_header(\n        self,\n    ):\n\n        col1, col2 = st.columns([1, 4])\n        with col1:\n            st.image(self.logo, width=100)\n        with col2:\n            st.markdown(\n                self.info,\n                unsafe_allow_html=True,\n            )\n\n    def generate_complete_ui(self):\n\n        self._generate_logo_header()\n        chat_container = st.container()\n        # self.disclaimer_dialog()\n        with chat_container:\n            with st.form(key=\"chat_form\"):\n                user_input = st.text_input(\n                    label=\"Query\", placeholder=self.chatbot_display\n                )\n                query_type = st.selectbox(\n                    \"Select Query Type\",\n                    [\"General Query\", \"Dataset\", \"Flow\"],\n                    help=\"Are you looking for a dataset or a flow or just have a general query?\",\n                )\n                ai_filter = st.toggle(\n                    \"Use AI powered filtering\",\n                    value=True,\n                    help=\"Uses an AI model to identify what columns might be useful to you.\",\n                )\n                st.form_submit_button(label=\"Search\")\n\n            self.create_chat_interface(user_input=None)\n            if user_input:\n                self.create_chat_interface(\n                    user_input, query_type=query_type, ai_filter=ai_filter\n                )\n\n    def create_chat_interface(self, user_input, query_type=None, ai_filter=False):\n        \"\"\"\n        Description: Create the chat interface and display the chat history and results. Show the user input and the response from the OpenML Agent.\n\n        \"\"\"\n        self.query_type = query_type\n        self.ai_filter = ai_filter\n\n        if user_input is None:\n            with st.chat_message(name=\"ai\"):\n                st.write(\"OpenML Agent: \", \"Hello! How can I help you today?\")\n                st.write(\n                    \":warning: Note that results are powered by local LLM models and may not be accurate. Please refer to the official OpenML website for accurate information.\"\n                )\n\n        # Handle user input\n        if user_input:\n            self._handle_user_input(user_input, query_type)\n\n    def _handle_user_input(self, user_input, query_type):\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n        with st.spinner(\"Waiting for results...\"):\n            results = self.process_query_chat(user_input)\n\n        if not self.query_type == \"General Query\":\n            st.session_state.messages.append(\n                    {\"role\": \"OpenML Agent\", \"content\": results}\n                )\n        else:\n            self._stream_results(results)\n\n            # reverse messages to show the latest message at the top\n        reversed_messages = self._reverse_session_history()\n\n            # Display chat history\n        self._display_chat_history(query_type, reversed_messages)\n        self.create_download_button()\n\n    def _display_chat_history(self, query_type, reversed_messages):\n        for message in reversed_messages:\n            if query_type == \"General Query\":\n                pass\n            if message[\"role\"] == \"user\":\n                with st.chat_message(name=\"user\"):\n                    self.display_results(message[\"content\"], \"user\")\n            else:\n                with st.chat_message(name=\"ai\"):\n                    self.display_results(message[\"content\"], \"ai\")\n\n    def _reverse_session_history(self):\n        reversed_messages = []\n        for index in range(0, len(st.session_state.messages), 2):\n            reversed_messages.insert(0, st.session_state.messages[index])\n            reversed_messages.insert(1, st.session_state.messages[index + 1])\n        return reversed_messages\n\n    def _stream_results(self, results):\n        with st.spinner(\"Fetching results...\"):\n            with requests.get(results, stream=True) as r:\n                resp_contain = st.empty()\n                streamed_response = \"\"\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk:\n                        streamed_response += chunk.decode(\"utf-8\")\n                        resp_contain.markdown(streamed_response)\n                resp_contain.empty()\n            st.session_state.messages.append(\n                {\"role\": \"OpenML Agent\", \"content\": streamed_response}\n            )\n\n    @st.experimental_fragment()\n    def create_download_button(self):\n        data = \"\\n\".join(\n            [str(message[\"content\"]) for message in st.session_state.messages]\n        )\n        st.download_button(\n            label=\"Download chat history\",\n            data=data,\n            file_name=\"chat_history.txt\",\n        )\n\n    def display_results(self, initial_response, role):\n        \"\"\"\n        Description: Display the results in a DataFrame\n        \"\"\"\n        try:\n            st.dataframe(initial_response)\n        except:\n            st.write(initial_response)\n\n    # Function to handle query processing\n    def process_query_chat(self, query):\n        \"\"\"\n        Description: Process the query and return the results based on the query type and the LLM filter.\n\n        \"\"\"\n        apply_llm_before_rag = None if not self.llm_filter else False\n        response_parser = ResponseParser(\n            self.query_type, apply_llm_before_rag=apply_llm_before_rag\n        )\n\n        if self.query_type == \"Dataset\" or self.query_type == \"Flow\":\n            if not self.ai_filter:\n                response_parser.fetch_rag_response(self.query_type, query)\n                return response_parser.parse_and_update_response(self.data_metadata)\n            else:\n                # get structured query\n                self._display_structured_query_results(query, response_parser)\n\n            results = response_parser.parse_and_update_response(self.data_metadata)\n            return results\n\n        elif self.query_type == \"General Query\":\n            # Return documentation response path\n            return self.paths[\"documentation_query\"][\"local\"] + query\n\n    def _display_structured_query_results(self, query, response_parser):\n        response_parser.fetch_structured_query(self.query_type, query)\n        try:\n            # get rag response\n            # using original query instead of extracted topics.\n            response_parser.fetch_rag_response(\n                self.query_type,\n                response_parser.structured_query_response[0][\"query\"],\n            )\n\n            if response_parser.structured_query_response:\n                st.write(\n                    \"Detected Filter(s): \",\n                    json.dumps(\n                        response_parser.structured_query_response[0].get(\"filter\", None)\n                    ),\n                )\n            else:\n                st.write(\"Detected Filter(s): \", None)\n            if response_parser.structured_query_response[1].get(\"filter\"):\n                with st.spinner(\"Applying LLM Detected Filter(s)...\"):\n                    response_parser.database_filter(\n                        response_parser.structured_query_response[1][\"filter\"],\n                        collec,\n                    )\n        except:\n            # fallback to RAG response\n            response_parser.fetch_rag_response(self.query_type, query)\n\n    def load_paths(self):\n        \"\"\"\n        Description: Load paths from paths.json\n        \"\"\"\n        with open(\"paths.json\", \"r\") as file:\n            return json.load(file)\n</code></pre>"},{"location":"Inference/inference/#ui_utils.UILoader.create_chat_interface","title":"<code>create_chat_interface(user_input, query_type=None, ai_filter=False)</code>","text":"<p>Description: Create the chat interface and display the chat history and results. Show the user input and the response from the OpenML Agent.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def create_chat_interface(self, user_input, query_type=None, ai_filter=False):\n    \"\"\"\n    Description: Create the chat interface and display the chat history and results. Show the user input and the response from the OpenML Agent.\n\n    \"\"\"\n    self.query_type = query_type\n    self.ai_filter = ai_filter\n\n    if user_input is None:\n        with st.chat_message(name=\"ai\"):\n            st.write(\"OpenML Agent: \", \"Hello! How can I help you today?\")\n            st.write(\n                \":warning: Note that results are powered by local LLM models and may not be accurate. Please refer to the official OpenML website for accurate information.\"\n            )\n\n    # Handle user input\n    if user_input:\n        self._handle_user_input(user_input, query_type)\n</code></pre>"},{"location":"Inference/inference/#ui_utils.UILoader.display_results","title":"<code>display_results(initial_response, role)</code>","text":"<p>Description: Display the results in a DataFrame</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def display_results(self, initial_response, role):\n    \"\"\"\n    Description: Display the results in a DataFrame\n    \"\"\"\n    try:\n        st.dataframe(initial_response)\n    except:\n        st.write(initial_response)\n</code></pre>"},{"location":"Inference/inference/#ui_utils.UILoader.load_paths","title":"<code>load_paths()</code>","text":"<p>Description: Load paths from paths.json</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def load_paths(self):\n    \"\"\"\n    Description: Load paths from paths.json\n    \"\"\"\n    with open(\"paths.json\", \"r\") as file:\n        return json.load(file)\n</code></pre>"},{"location":"Inference/inference/#ui_utils.UILoader.process_query_chat","title":"<code>process_query_chat(query)</code>","text":"<p>Description: Process the query and return the results based on the query type and the LLM filter.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def process_query_chat(self, query):\n    \"\"\"\n    Description: Process the query and return the results based on the query type and the LLM filter.\n\n    \"\"\"\n    apply_llm_before_rag = None if not self.llm_filter else False\n    response_parser = ResponseParser(\n        self.query_type, apply_llm_before_rag=apply_llm_before_rag\n    )\n\n    if self.query_type == \"Dataset\" or self.query_type == \"Flow\":\n        if not self.ai_filter:\n            response_parser.fetch_rag_response(self.query_type, query)\n            return response_parser.parse_and_update_response(self.data_metadata)\n        else:\n            # get structured query\n            self._display_structured_query_results(query, response_parser)\n\n        results = response_parser.parse_and_update_response(self.data_metadata)\n        return results\n\n    elif self.query_type == \"General Query\":\n        # Return documentation response path\n        return self.paths[\"documentation_query\"][\"local\"] + query\n</code></pre>"},{"location":"Ollama%20server/","title":"Ollama Server","text":"<ul> <li>This is the server that runs an Ollama server (This is basically an optimized version of a local LLM. It does not do anything of itself but runs as a background service so you can use the LLM). </li> <li>You can start it by running <code>cd ollama &amp;&amp; ./get_ollama.sh &amp;</code></li> </ul>"},{"location":"Query%20processing%20LLM/","title":"LLM Query parsing","text":"<ul> <li>This page is only an overview. Please refer to the api reference for more detailed information.</li> <li>The query parsing LLM reads the query and parses it into a list of filters based on a prompt. The expected result is a JSON with a list of filters to be applied to the metadata and the query.</li> <li>This is done by providing a prompt to the RAG and telling it to extract the filters/etc and either structure it or not.</li> <li>This implementation is served as a FastAPI service that can be queried quite easily.</li> </ul>"},{"location":"Query%20processing%20LLM/#structured-implementation","title":"Structured Implementation","text":"<ul> <li>TBD</li> </ul>"},{"location":"Query%20processing%20LLM/#unstructured-implementation","title":"Unstructured Implementation","text":"<ul> <li>This implementation is independent of <code>langchain</code>, and takes a more manual approach to parsing the filters. At the moment, this does not separate the query from the filters either. (The structured query implementation attempts to do that.)</li> <li>The response of the the LLM parser does not take into account how to apply the filters, it just provides a list of the ones that the LLM considered relevant to the UI.</li> <li>This component is the one that runs the query processing using LLMs module. It uses the Ollama server, runs queries and processes them. </li> <li>You can start it by running <code>cd llm_service &amp;&amp; uvicorn llm_service:app --host 0.0.0.0 --port 8081 &amp;</code></li> <li>Curl Example : <code>curl http://0.0.0.0:8081/llmquery/find%20me%20a%20mushroom%20dataset%20with%20less%20than%203000%20classes</code></li> </ul>"},{"location":"Query%20processing%20LLM/#llm_servicepy","title":"llm_service.py","text":"<ul> <li>A prompt template is used to tell the RAG what to do. </li> <li>The prompt_dict defines a list of filters and their respective prompts for the LLM. This is concatenated with the prompt template.</li> <li>The response is parsed quite simply. Since the LLM is asked to provide it's answers line by line, each line is parsed for the required information according to a list of patterns provided. </li> <li>Thus, if you want to add a new type of answer, add it to the patterns list and it should be taken care of.</li> </ul>"},{"location":"Query%20processing%20LLM/#llm_service_utilspy","title":"llm_service_utils.py","text":"<ul> <li>The main logic of the above is defined here.</li> </ul>"},{"location":"Query%20processing%20LLM/#additional-information","title":"Additional information","text":"<ul> <li>In the process of testing this implementation, a blog was written about how the temperature parameter affects the results of the model. This can be found here.</li> </ul>"},{"location":"Query%20processing%20LLM/api_reference/","title":"Structured query","text":""},{"location":"Query%20processing%20LLM/api_reference/#vector-store","title":"Vector store","text":""},{"location":"Query%20processing%20LLM/api_reference/#structured-query_1","title":"Structured Query","text":""},{"location":"Query%20processing%20LLM/api_reference/#llm_service_structured_query.get_structured_query","title":"<code>get_structured_query(query)</code>  <code>async</code>","text":"<p>Description: Get the query, replace %20 with space and invoke the chain to get the answers based on the prompt.</p> Source code in <code>structured_query/llm_service_structured_query.py</code> <pre><code>@app.get(\"/structuredquery/{query}\", response_class=JSONResponse)\n@retry(stop=stop_after_attempt(3), retry=retry_if_exception_type(ConnectTimeout))\nasync def get_structured_query(query: str):\n    \"\"\"\n    Description: Get the query, replace %20 with space and invoke the chain to get the answers based on the prompt.\n\n    \"\"\"\n    response, filter_condition = None, None\n    try:\n        query = query.replace(\"%20\", \" \")\n        response = chain.invoke({\"query\": query})\n        print(response)\n        obj = ChromaTranslator()\n        filter_condition = obj.visit_structured_query(structured_query=response)[1]\n\n    except Exception as e:\n        print(\n            f\"An error occurred: \",\n            HTTPException(status_code=500, detail=f\"An error occurred: {e}\"),\n        )\n\n    return response, filter_condition\n</code></pre>"},{"location":"Query%20processing%20LLM/api_reference/#deprecated","title":"Deprecated","text":"<ul> <li>This section has the API reference that does not use Structured query processing from langchain. It is not used but is left in for future reference. </li> </ul>"},{"location":"Query%20processing%20LLM/api_reference/#llm_service.get_llm_query","title":"<code>get_llm_query(query)</code>  <code>async</code>","text":"<p>Description: Get the query, replace %20 (url spacing) with space and invoke the chain to get the answers based on the prompt</p> Source code in <code>llm_service/llm_service.py</code> <pre><code>@app.get(\"/llmquery/{query}\", response_class=JSONResponse)\n@retry(stop=stop_after_attempt(3), retry=retry_if_exception_type(ConnectTimeout))\nasync def get_llm_query(query: str):\n    \"\"\"\n    Description: Get the query, replace %20 (url spacing) with space and invoke the chain to get the answers based on the prompt\n    \"\"\"\n    query = query.replace(\"%20\", \" \")\n    print(f\"Query: {query}\")\n    try:\n        response = chain_docker.invoke({\"query\": query})\n    except:\n        response = chain.invoke({\"query\": query})\n    answers = parse_answers_initial(response, patterns, prompt_dict)\n    return JSONResponse(content=answers)\n</code></pre>"},{"location":"Query%20processing%20LLM/api_reference/#llm_service_utils.create_chain","title":"<code>create_chain(prompt, model='llama3', temperature=0, base_url='http://localhost:11434')</code>","text":"<p>Description: Create a langchain chain with the given prompt and model and the temperature. The lower the temperature, the less \"creative\" the model will be.</p> Source code in <code>llm_service/llm_service_utils.py</code> <pre><code>def create_chain(\n    prompt,\n    model: str = \"llama3\",\n    temperature: int = 0,\n    base_url: str = \"http://localhost:11434\",\n):\n    \"\"\"\n    Description: Create a langchain chain with the given prompt and model and the temperature.\n    The lower the temperature, the less \"creative\" the model will be.\n    \"\"\"\n    llm = ChatOllama(model=model, temperature=temperature, base_url=base_url)\n    prompt = ChatPromptTemplate.from_template(prompt)\n\n    return prompt | llm | StrOutputParser()\n</code></pre>"},{"location":"Query%20processing%20LLM/api_reference/#llm_service_utils.parse_answers_initial","title":"<code>parse_answers_initial(response, patterns, prompt_dict)</code>","text":"<p>Description: Parse the answers from the initial response - if the response contains a ? and a new line then join the next line with it (sometimes the LLM adds a new line after the ? instead of just printing it on the same line)</p> Source code in <code>llm_service/llm_service_utils.py</code> <pre><code>def parse_answers_initial(response: str, patterns: list, prompt_dict: dict) -&gt; dict:\n    \"\"\"\n    Description: Parse the answers from the initial response\n    - if the response contains a ? and a new line then join the next line with it (sometimes the LLM adds a new line after the ? instead of just printing it on the same line)\n    \"\"\"\n\n    answers = []\n    response = response.replace(\"?\\n\", \"?\")\n\n    # convert the response to lowercase and split it into lines\n    lines = response.lower().split(\"\\n\")\n\n    for line in lines:\n        if \"?\" in line:\n            # Extract the part of the line after the question mark\n            potential_answer = line.split(\"?\")[1].strip()\n        else:\n            potential_answer = line.strip()\n\n        # Check if the potential answer matches any of the patterns\n        for pattern in patterns:\n            if re.match(pattern, potential_answer):\n                answers.append(potential_answer)\n                break  # Stop checking other patterns if a match is found\n\n    # return answers as a dict using the prompt_dict keys\n    answers_dict = {}\n    for i, key in enumerate(prompt_dict.keys()):\n        answers_dict[key] = answers[i]\n\n    return answers_dict\n</code></pre>"},{"location":"Rag%20Pipeline/","title":"RAG Pipeline","text":"<ul> <li>The RAG pipeline is the main service of the AI search. At the moment though, we are not doing the Generation part and only using the RAG to find relevant datasets for the given query.</li> <li>The RAG pipeline code here is divided into two parts - training (<code>training.py</code>) and inference (<code>backend.py</code>). The first is used to gather data from the OpenML API and then preprocess it and store it in a vector database, the second is used for inference.</li> </ul>"},{"location":"Rag%20Pipeline/#training","title":"Training","text":"<ul> <li>All the modules you are looking for are in <code>backend/modules</code>. To modify/understand any of the behavior, you should look at the corresponding documentation for each of the ones that you want to modify.</li> <li><code>config.json</code> : JSON with the main config used for training and inference - documentation</li> <li><code>results_gen.py</code> : Code for creating the output and running parts of the other modules during inference - documentation</li> <li><code>general_utils.py</code> : Code for device configuration (gpu/cpu/mps) - documentation</li> <li><code>metadata_utils.py</code> : Getting/formatting/loading metadata from OpenML - documentation</li> <li><code>rag_llm.py</code> : Langchain code for the RAG pipeline - documentation</li> <li><code>utils.py</code> : Just imports all the utility files </li> <li><code>vector_store_utils.py</code> : Code for loading data into the vector store. - documentation </li> </ul>"},{"location":"Rag%20Pipeline/#inference","title":"Inference","text":"<ul> <li>This component runs the RAG pipeline. It returns a JSON with dataset ids of the OpenML datasets that match the query.</li> <li>You can start it by running <code>cd backend &amp;&amp; uvicorn backend:app --host 0.0.0.0 --port 8000 &amp;</code></li> <li>Curl Example : <code>curl http://0.0.0.0:8000/dataset/find%20me%20a%20mushroom%20dataset</code></li> </ul>"},{"location":"Rag%20Pipeline/configuration/","title":"Configuration","text":"<ul> <li>The main config file is <code>config.json</code>. Since this is loaded in every training/evaluation script, you can use this to modify the behavior inline. </li> </ul>"},{"location":"Rag%20Pipeline/configuration/#possible-options","title":"Possible options","text":"<ul> <li>rqa_prompt_template: The template for the RAG pipeline search prompt. This is used by the model to query the database. </li> <li>llm_prompt_template: The template for the summary generator LLM prompt.</li> <li>num_return_documents: Number of documents to return for a query. Too high a number can lead to Out of Memory errors. (Defaults to 50)</li> <li>embedding_model: THIS IS FROM HUGGINGFACE. The model to use for generating embeddings. This is used to generate embeddings for the documents as a means of comparison using the LLM's embeddings. (Defaults to BAAI/bge-large-en-v1.5)<ul> <li>Other possible tested models<ul> <li>BAAI/bge-base-en-v1.5</li> <li>BAAI/bge-large-en-v1.5</li> </ul> </li> </ul> </li> <li>llm_model: THIS IS FROM OLLAMA. The model used for generating the result summary. (Defaults to qwen2:1.5b)</li> <li>data_dir: The directory to store the intermediate data like tables/databases etc. (Defaults to ./data/)</li> <li>persist_dir: The directory to store the cached data. Defaults to ./data/chroma_db/ and stores the embeddings for the documents with a unique hash. (Defaults to ./data/chroma_db/)</li> <li>testing_flag: Enables testing mode by using subsets of the data for quick debugging. This is used to test the pipeline and is not recommended for normal use. (Defaults to False)</li> <li>test_subset: Uses a tiny subset of the data for testing.</li> <li>data_download_n_jobs: Number of jobs to run in parallel for downloading data. (Defaults to 20)</li> <li>training: Whether to train the model or not. (Defaults to False) this is automatically set to True when when running the training.py script. Do NOT set this to True manually.</li> <li>search_type : The type of vector comparison to use. (Defaults to \"similarity\")</li> <li>reraanking: Whether to rerank the results using the FlashRank algorithm. (Defaults to False)</li> <li>long_context_reordering: Whether to reorder the results using the Long Context Reordering algorithm. (Defaults to False)</li> <li>chunk_size: Size of the chunks for the RAG document chunking</li> </ul>"},{"location":"Rag%20Pipeline/general_utils/","title":"General utilities","text":"<ul> <li>The logic for which device to use. At the moment, this does not support multiple GPUs, but it should be pretty easy to add in if required.</li> </ul>"},{"location":"Rag%20Pipeline/general_utils/#general_utils.find_device","title":"<code>find_device(training=False)</code>","text":"<p>Description: Find the device to use for the pipeline. If cuda is available, use it. If not, check if MPS is available and use it. If not, use CPU.</p> Source code in <code>backend/modules/general_utils.py</code> <pre><code>def find_device(training: bool = False) -&gt; str:\n    \"\"\"\n    Description: Find the device to use for the pipeline. If cuda is available, use it. If not, check if MPS is available and use it. If not, use CPU.\n    \"\"\"\n    print(\"[INFO] Finding device.\")\n    if torch.cuda.is_available():\n        return \"cuda\"\n    elif torch.backends.mps.is_available():\n        return \"mps\"\n    else:\n        return \"cpu\"\n</code></pre>"},{"location":"Rag%20Pipeline/general_utils/#general_utils.load_config_and_device","title":"<code>load_config_and_device(config_file, training=False)</code>","text":"<p>Description: Load the config file and find the device to use for the pipeline.</p> Source code in <code>backend/modules/general_utils.py</code> <pre><code>def load_config_and_device(config_file: str, training: bool = False) -&gt; dict:\n    \"\"\"\n    Description: Load the config file and find the device to use for the pipeline.\n    \"\"\"\n    # Check if the config file exists and load it\n    if not os.path.exists(config_file):\n        raise Exception(\"Config file does not exist.\")\n    with open(config_file, \"r\") as f:\n        config = json.load(f)\n\n    # Find device and set it in the config between cpu and cuda and mps if available\n    config[\"device\"] = find_device(training)\n    print(f\"[INFO] Device found: {config['device']}\")\n    return config\n</code></pre>"},{"location":"Rag%20Pipeline/llm_module/","title":"RAG LLM","text":"<ul> <li>Setting up the retrival and using Lanchain APIs</li> </ul>"},{"location":"Rag%20Pipeline/llm_module/#modify-llm-chain","title":"Modify LLM Chain","text":"<ul> <li>At the moment the LLM chain is a retriver, if you want to add functionality, you will need to modify the <code>LLMChainInitializer</code> function.</li> <li>To change the way vectorstore is used, modify the <code>QASetup</code> function.</li> <li>To change the way Ollama works, caching works and add generation and stuff, modify the <code>LLMChainCreator</code> function. </li> </ul>"},{"location":"Rag%20Pipeline/llm_module/#rag_llm.LLMChainCreator","title":"<code>LLMChainCreator</code>","text":"<p>Description: Gets Ollama, sends query, enables query caching</p> Source code in <code>backend/modules/rag_llm.py</code> <pre><code>class LLMChainCreator:\n    \"\"\"\n    Description: Gets Ollama, sends query, enables query caching\n    \"\"\"\n\n    def __init__(self, config: dict, local: bool = False):\n        self.config = config\n        self.local = local\n\n    def get_llm_chain(self) -&gt; LLMChain | bool:\n        \"\"\"\n        Description: Send a query to Ollama using the paths.\n        \"\"\"\n        base_url = \"http://127.0.0.1:11434\" if self.local else \"http://ollama:11434\"\n        llm = Ollama(model=self.config[\"llm_model\"], base_url=base_url)\n        map_template = self.config[\"llm_prompt_template\"]\n        map_prompt = PromptTemplate.from_template(map_template)\n        return map_prompt | llm | StrOutputParser()\n\n    def enable_cache(self):\n        \"\"\"\n        Description: Enable a cache for queries to prevent running the same query again for no reason.\n        \"\"\"\n        set_llm_cache(\n            SQLiteCache(\n                database_path=os.path.join(self.config[\"data_dir\"], \".langchain.db\")\n            )\n        )\n</code></pre>"},{"location":"Rag%20Pipeline/llm_module/#rag_llm.LLMChainCreator.enable_cache","title":"<code>enable_cache()</code>","text":"<p>Description: Enable a cache for queries to prevent running the same query again for no reason.</p> Source code in <code>backend/modules/rag_llm.py</code> <pre><code>def enable_cache(self):\n    \"\"\"\n    Description: Enable a cache for queries to prevent running the same query again for no reason.\n    \"\"\"\n    set_llm_cache(\n        SQLiteCache(\n            database_path=os.path.join(self.config[\"data_dir\"], \".langchain.db\")\n        )\n    )\n</code></pre>"},{"location":"Rag%20Pipeline/llm_module/#rag_llm.LLMChainCreator.get_llm_chain","title":"<code>get_llm_chain()</code>","text":"<p>Description: Send a query to Ollama using the paths.</p> Source code in <code>backend/modules/rag_llm.py</code> <pre><code>def get_llm_chain(self) -&gt; LLMChain | bool:\n    \"\"\"\n    Description: Send a query to Ollama using the paths.\n    \"\"\"\n    base_url = \"http://127.0.0.1:11434\" if self.local else \"http://ollama:11434\"\n    llm = Ollama(model=self.config[\"llm_model\"], base_url=base_url)\n    map_template = self.config[\"llm_prompt_template\"]\n    map_prompt = PromptTemplate.from_template(map_template)\n    return map_prompt | llm | StrOutputParser()\n</code></pre>"},{"location":"Rag%20Pipeline/llm_module/#rag_llm.LLMChainInitializer","title":"<code>LLMChainInitializer</code>","text":"<p>Description: Setup the vectordb (Chroma) as a retriever with parameters</p> Source code in <code>backend/modules/rag_llm.py</code> <pre><code>class LLMChainInitializer:\n    \"\"\"\n    Description: Setup the vectordb (Chroma) as a retriever with parameters\n    \"\"\"\n\n    @staticmethod\n    def initialize_llm_chain(\n        vectordb: Chroma, config: dict\n    ) -&gt; langchain.chains.retrieval_qa.base.RetrievalQA:\n        if config[\"search_type\"] == \"similarity_score_threshold\":\n            return vectordb.as_retriever(\n                search_type=config[\"search_type\"],\n                search_kwargs={\n                    \"k\": config[\"num_return_documents\"],\n                    \"score_threshold\": 0.5,\n                },\n            )\n        else:\n            return vectordb.as_retriever(\n                search_type=config[\"search_type\"],\n                search_kwargs={\"k\": config[\"num_return_documents\"]},\n            )\n</code></pre>"},{"location":"Rag%20Pipeline/llm_module/#rag_llm.QASetup","title":"<code>QASetup</code>","text":"<p>Description: Setup the VectorDB, QA and initalize the LLM for each type of data</p> Source code in <code>backend/modules/rag_llm.py</code> <pre><code>class QASetup:\n    \"\"\"\n    Description: Setup the VectorDB, QA and initalize the LLM for each type of data\n    \"\"\"\n\n    def __init__(\n        self, config: dict, data_type: str, client: ClientAPI, subset_ids: list = None\n    ):\n        self.config = config\n        self.data_type = data_type\n        self.client = client\n        self.subset_ids = subset_ids\n\n    def setup_vector_db_and_qa(self):\n        self.config[\"type_of_data\"] = self.data_type\n\n        metadata_processor = OpenMLMetadataProcessor(config=self.config)\n        openml_data_object, data_id, all_metadata, handler = (\n            metadata_processor.get_all_metadata_from_openml()\n        )\n        metadata_df, all_metadata = metadata_processor.create_metadata_dataframe(\n            handler,\n            openml_data_object,\n            data_id,\n            all_metadata,\n            subset_ids=self.subset_ids,\n        )\n\n        vector_store_manager = VectorStoreManager(self.client, self.config)\n        vectordb = vector_store_manager.create_vector_store(metadata_df)\n        qa = LLMChainInitializer.initialize_llm_chain(vectordb, self.config)\n\n        return qa, all_metadata\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/","title":"Metadata Module","text":"<ul> <li>Logic for Getting/formatting/loading metadata from OpenML.</li> <li>If you want to modify the logic for the data ingestion pipeline : Refer to <code>OpenMLObjectHandler</code> , <code>OpenMLDatasetHandler</code></li> <li>If you want to chane the pipeline itself, refer to <code>OpenMLMetadataProcessor</code></li> </ul>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLDatasetHandler","title":"<code>OpenMLDatasetHandler</code>","text":"<p>               Bases: <code>OpenMLObjectHandler</code></p> <p>Description: The class for handling OpenML dataset objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>class OpenMLDatasetHandler(OpenMLObjectHandler):\n    \"\"\"\n    Description: The class for handling OpenML dataset objects.\n    \"\"\"\n\n    def get_description(self, data_id: int):\n        return openml.datasets.get_dataset(\n            dataset_id=data_id,\n            download_data=False,\n            download_qualities=True,\n            download_features_meta_data=True,\n        )\n\n    def get_openml_objects(self):\n        return openml.datasets.list_datasets(output_format=\"dataframe\")\n\n    def process_metadata(\n        self,\n        openml_data_object: Sequence[openml.datasets.dataset.OpenMLDataset],\n        data_id: Sequence[int],\n        all_dataset_metadata: pd.DataFrame,\n        file_path: str,\n        subset_ids=None,\n    ):\n        \"\"\"\n        Description: Combine the metadata attributes into a single string and save it to a CSV / ChromaDB file. Subset the data if given a list of IDs to subset by.\n        \"\"\"\n\n        # Metadata\n        descriptions = [\n            self.extract_attribute(attr, \"description\") for attr in openml_data_object\n        ]\n        joined_qualities = [\n            self.join_attributes(attr, \"qualities\") for attr in openml_data_object\n        ]\n        joined_features = [\n            self.join_attributes(attr, \"features\") for attr in openml_data_object\n        ]\n\n        # Combine them\n\n        all_data_description_df = self.create_combined_information_df_for_datasets(\n            data_id, descriptions, joined_qualities, joined_features\n        )\n        all_dataset_metadata = self.combine_metadata(\n            all_dataset_metadata, all_data_description_df\n        )\n\n        # subset the metadata if subset_ids is not None\n        all_dataset_metadata = self.subset_metadata(subset_ids, all_dataset_metadata)\n\n        # Save to a CSV\n        all_dataset_metadata.to_csv(file_path)\n\n        # Save to chroma if needed\n        if self.config.get(\"use_chroma_for_saving_metadata\"):\n            client = chromadb.PersistentClient(\n                path=self.config[\"persist_dir\"] + \"metadata_db\"\n            )\n            vecmanager = VectorStoreManager(client, self.config)\n            vecmanager.add_df_chunks_to_db(all_dataset_metadata)\n\n        return (\n            all_dataset_metadata[[\"did\", \"name\", \"Combined_information\"]],\n            all_dataset_metadata,\n        )\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLDatasetHandler.process_metadata","title":"<code>process_metadata(openml_data_object, data_id, all_dataset_metadata, file_path, subset_ids=None)</code>","text":"<p>Description: Combine the metadata attributes into a single string and save it to a CSV / ChromaDB file. Subset the data if given a list of IDs to subset by.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def process_metadata(\n    self,\n    openml_data_object: Sequence[openml.datasets.dataset.OpenMLDataset],\n    data_id: Sequence[int],\n    all_dataset_metadata: pd.DataFrame,\n    file_path: str,\n    subset_ids=None,\n):\n    \"\"\"\n    Description: Combine the metadata attributes into a single string and save it to a CSV / ChromaDB file. Subset the data if given a list of IDs to subset by.\n    \"\"\"\n\n    # Metadata\n    descriptions = [\n        self.extract_attribute(attr, \"description\") for attr in openml_data_object\n    ]\n    joined_qualities = [\n        self.join_attributes(attr, \"qualities\") for attr in openml_data_object\n    ]\n    joined_features = [\n        self.join_attributes(attr, \"features\") for attr in openml_data_object\n    ]\n\n    # Combine them\n\n    all_data_description_df = self.create_combined_information_df_for_datasets(\n        data_id, descriptions, joined_qualities, joined_features\n    )\n    all_dataset_metadata = self.combine_metadata(\n        all_dataset_metadata, all_data_description_df\n    )\n\n    # subset the metadata if subset_ids is not None\n    all_dataset_metadata = self.subset_metadata(subset_ids, all_dataset_metadata)\n\n    # Save to a CSV\n    all_dataset_metadata.to_csv(file_path)\n\n    # Save to chroma if needed\n    if self.config.get(\"use_chroma_for_saving_metadata\"):\n        client = chromadb.PersistentClient(\n            path=self.config[\"persist_dir\"] + \"metadata_db\"\n        )\n        vecmanager = VectorStoreManager(client, self.config)\n        vecmanager.add_df_chunks_to_db(all_dataset_metadata)\n\n    return (\n        all_dataset_metadata[[\"did\", \"name\", \"Combined_information\"]],\n        all_dataset_metadata,\n    )\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLFlowHandler","title":"<code>OpenMLFlowHandler</code>","text":"<p>               Bases: <code>OpenMLObjectHandler</code></p> <p>Description: The class for handling OpenML flow objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>class OpenMLFlowHandler(OpenMLObjectHandler):\n    \"\"\"\n    Description: The class for handling OpenML flow objects.\n    \"\"\"\n\n    def get_description(self, data_id: int):\n        return openml.flows.get_flow(flow_id=data_id)\n\n    def get_openml_objects(self):\n        all_objects = openml.flows.list_flows(output_format=\"dataframe\")\n        return all_objects.rename(columns={\"id\": \"did\"})\n\n    def process_metadata(\n        self,\n        openml_data_object: Sequence[openml.flows.flow.OpenMLFlow],\n        data_id: Sequence[int],\n        all_dataset_metadata: pd.DataFrame,\n        file_path: str,\n        subset_ids=None,\n    ):\n        descriptions = [\n            self.extract_attribute(attr, \"description\") for attr in openml_data_object\n        ]\n        names = [self.extract_attribute(attr, \"name\") for attr in openml_data_object]\n        tags = [self.extract_attribute(attr, \"tags\") for attr in openml_data_object]\n\n        all_data_description_df = pd.DataFrame(\n            {\n                \"did\": data_id,\n                \"description\": descriptions,\n                \"name\": names,\n                \"tags\": tags,\n            }\n        )\n\n        all_data_description_df[\"Combined_information\"] = all_data_description_df.apply(\n            self.merge_all_columns_to_string, axis=1\n        )\n        # subset the metadata if subset_ids is not None\n\n        all_dataset_metadata = self.subset_metadata(subset_ids, all_dataset_metadata)\n\n        all_data_description_df.to_csv(file_path)\n\n        return (\n            all_data_description_df[[\"did\", \"name\", \"Combined_information\"]],\n            all_data_description_df,\n        )\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLMetadataProcessor","title":"<code>OpenMLMetadataProcessor</code>","text":"<p>Description: Process metadata using the OpenMLHandlers</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>class OpenMLMetadataProcessor:\n    \"\"\"\n    Description: Process metadata using the OpenMLHandlers\n    \"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n        self.save_filename = os.path.join(\n            config[\"data_dir\"], f\"all_{config['type_of_data']}_metadata.pkl\"\n        )\n        self.description_filename = os.path.join(\n            config[\"data_dir\"], f\"all_{config['type_of_data']}_description.csv\"\n        )\n\n    def get_all_metadata_from_openml(self):\n        \"\"\"\n        Description: Gets all the metadata from OpenML for the type of data specified in the config.\n        If training is set to False, it loads the metadata from the files. If training is set to True, it gets the metadata from OpenML.\n\n        This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.\n        \"\"\"\n        if not self.config.get(\"training\", False) or self.config.get(\n            \"ignore_downloading_data\", False\n        ):\n            if not os.path.exists(self.save_filename):\n                raise Exception(\n                    \"Metadata files do not exist. Please run the training pipeline first.\"\n                )\n            print(\"[INFO] Loading metadata from file.\")\n            return load_metadata_from_file(self.save_filename)\n\n        print(\"[INFO] Training is set to True.\")\n        handler = (\n            OpenMLDatasetHandler(self.config)\n            if self.config[\"type_of_data\"] == \"dataset\"\n            else OpenMLFlowHandler(self.config)\n        )\n\n        all_objects = handler.get_openml_objects()\n\n        if self.config.get(\"test_subset\", False):\n            print(\"[INFO] Subsetting the data.\")\n            all_objects = all_objects[:500]\n\n        data_id = [int(all_objects.iloc[i][\"did\"]) for i in range(len(all_objects))]\n\n        print(\"[INFO] Initializing cache.\")\n        handler.initialize_cache(data_id)\n\n        print(f\"[INFO] Getting {self.config['type_of_data']} metadata from OpenML.\")\n        openml_data_object = handler.get_metadata(data_id)\n\n        print(\"[INFO] Saving metadata to file.\")\n        save_metadata_to_file(\n            (openml_data_object, data_id, all_objects, handler), self.save_filename\n        )\n\n        return openml_data_object, data_id, all_objects, handler\n\n    def create_metadata_dataframe(\n        self,\n        handler: Union[\"OpenMLDatasetHandler\", \"OpenMLFlowHandler\"],\n        openml_data_object: Sequence[\n            Union[openml.datasets.dataset.OpenMLDataset, openml.flows.flow.OpenMLFlow]\n        ],\n        data_id: Sequence[int],\n        all_dataset_metadata: pd.DataFrame,\n        subset_ids=None,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Description: Creates a dataframe with all the metadata, joined columns with all information\n        for the type of data specified in the config. If training is set to False,\n        the dataframes are loaded from the files. If training is set to True, the\n        dataframes are created and then saved to the files.\n        \"\"\"\n        if not self.config.get(\"training\", False):\n            return (\n                handler.load_metadata(self.description_filename),\n                all_dataset_metadata,\n            )\n\n        return handler.process_metadata(\n            openml_data_object,\n            data_id,\n            all_dataset_metadata,\n            self.description_filename,\n            subset_ids,\n        )\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLMetadataProcessor.create_metadata_dataframe","title":"<code>create_metadata_dataframe(handler, openml_data_object, data_id, all_dataset_metadata, subset_ids=None)</code>","text":"<p>Description: Creates a dataframe with all the metadata, joined columns with all information for the type of data specified in the config. If training is set to False, the dataframes are loaded from the files. If training is set to True, the dataframes are created and then saved to the files.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def create_metadata_dataframe(\n    self,\n    handler: Union[\"OpenMLDatasetHandler\", \"OpenMLFlowHandler\"],\n    openml_data_object: Sequence[\n        Union[openml.datasets.dataset.OpenMLDataset, openml.flows.flow.OpenMLFlow]\n    ],\n    data_id: Sequence[int],\n    all_dataset_metadata: pd.DataFrame,\n    subset_ids=None,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Description: Creates a dataframe with all the metadata, joined columns with all information\n    for the type of data specified in the config. If training is set to False,\n    the dataframes are loaded from the files. If training is set to True, the\n    dataframes are created and then saved to the files.\n    \"\"\"\n    if not self.config.get(\"training\", False):\n        return (\n            handler.load_metadata(self.description_filename),\n            all_dataset_metadata,\n        )\n\n    return handler.process_metadata(\n        openml_data_object,\n        data_id,\n        all_dataset_metadata,\n        self.description_filename,\n        subset_ids,\n    )\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLMetadataProcessor.get_all_metadata_from_openml","title":"<code>get_all_metadata_from_openml()</code>","text":"<p>Description: Gets all the metadata from OpenML for the type of data specified in the config. If training is set to False, it loads the metadata from the files. If training is set to True, it gets the metadata from OpenML.</p> <p>This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_all_metadata_from_openml(self):\n    \"\"\"\n    Description: Gets all the metadata from OpenML for the type of data specified in the config.\n    If training is set to False, it loads the metadata from the files. If training is set to True, it gets the metadata from OpenML.\n\n    This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.\n    \"\"\"\n    if not self.config.get(\"training\", False) or self.config.get(\n        \"ignore_downloading_data\", False\n    ):\n        if not os.path.exists(self.save_filename):\n            raise Exception(\n                \"Metadata files do not exist. Please run the training pipeline first.\"\n            )\n        print(\"[INFO] Loading metadata from file.\")\n        return load_metadata_from_file(self.save_filename)\n\n    print(\"[INFO] Training is set to True.\")\n    handler = (\n        OpenMLDatasetHandler(self.config)\n        if self.config[\"type_of_data\"] == \"dataset\"\n        else OpenMLFlowHandler(self.config)\n    )\n\n    all_objects = handler.get_openml_objects()\n\n    if self.config.get(\"test_subset\", False):\n        print(\"[INFO] Subsetting the data.\")\n        all_objects = all_objects[:500]\n\n    data_id = [int(all_objects.iloc[i][\"did\"]) for i in range(len(all_objects))]\n\n    print(\"[INFO] Initializing cache.\")\n    handler.initialize_cache(data_id)\n\n    print(f\"[INFO] Getting {self.config['type_of_data']} metadata from OpenML.\")\n    openml_data_object = handler.get_metadata(data_id)\n\n    print(\"[INFO] Saving metadata to file.\")\n    save_metadata_to_file(\n        (openml_data_object, data_id, all_objects, handler), self.save_filename\n    )\n\n    return openml_data_object, data_id, all_objects, handler\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler","title":"<code>OpenMLObjectHandler</code>","text":"<p>Description: The base class for handling OpenML objects. The logic for handling datasets/flows are subclasses from this.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>class OpenMLObjectHandler:\n    \"\"\"\n    Description: The base class for handling OpenML objects. The logic for handling datasets/flows are subclasses from this.\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n    def get_description(self, data_id: int):\n        \"\"\"\n        Description: Get the description of the OpenML object.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_openml_objects(self):\n        \"\"\"\n        Description: Get the OpenML objects.\n        \"\"\"\n        raise NotImplementedError\n\n    def initialize_cache(self, data_id: Sequence[int]) -&gt; None:\n        \"\"\"\n        Description: Initialize the cache for the OpenML objects.\n        \"\"\"\n        self.get_description(data_id[0])\n\n    def get_metadata(self, data_id: Sequence[int]):\n        \"\"\"\n        Description: Get metadata from OpenML using parallel processing.\n        \"\"\"\n        return pqdm(\n            data_id, self.get_description, n_jobs=self.config[\"data_download_n_jobs\"]\n        )\n\n    def process_metadata(\n        self,\n        openml_data_object,\n        data_id: Sequence[int],\n        all_dataset_metadata: pd.DataFrame,\n        file_path: str,\n        subset_ids=None,\n    ):\n        \"\"\"\n        Description: Process the metadata.\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def load_metadata(file_path: str):\n        \"\"\"\n        Description: Load metadata from a file.\n        \"\"\"\n        try:\n            return pd.read_csv(file_path)\n        except FileNotFoundError:\n            raise Exception(\n                \"Metadata files do not exist. Please run the training pipeline first.\"\n            )\n\n    @staticmethod\n    def extract_attribute(attribute: object, attr_name: str) -&gt; str:\n        \"\"\"\n        Description: Extract an attribute from the OpenML object.\n        \"\"\"\n        return getattr(attribute, attr_name, \"\")\n\n    @staticmethod\n    def join_attributes(attribute: object, attr_name: str) -&gt; str:\n        \"\"\"\n        Description: Join the attributes of the OpenML object.\n        \"\"\"\n        return (\n            \" \".join(\n                [f\"{k} : {v},\" for k, v in getattr(attribute, attr_name, {}).items()]\n            )\n            if hasattr(attribute, attr_name)\n            else \"\"\n        )\n\n    @staticmethod\n    def create_combined_information_df_for_datasets(\n        data_id: int | Sequence[int],\n        descriptions: Sequence[str],\n        joined_qualities: Sequence[str],\n        joined_features: Sequence[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Description: Create a dataframe with the combined information of the OpenML object.\n        \"\"\"\n        return pd.DataFrame(\n            {\n                \"did\": data_id,\n                \"description\": descriptions,\n                \"qualities\": joined_qualities,\n                \"features\": joined_features,\n            }\n        )\n\n    @staticmethod\n    def merge_all_columns_to_string(row: pd.Series) -&gt; str:\n        \"\"\"\n        Description: Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n        \"\"\"\n        return \" \".join([f\"{col} - {val},\" for col, val in zip(row.index, row.values)])\n\n    def combine_metadata(\n        self, all_dataset_metadata: pd.DataFrame, all_data_description_df: pd.DataFrame\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Description: Combine the descriptions with the metadata table.\n        \"\"\"\n        all_dataset_metadata = pd.merge(\n            all_dataset_metadata, all_data_description_df, on=\"did\", how=\"inner\"\n        )\n        all_dataset_metadata[\"Combined_information\"] = all_dataset_metadata.apply(\n            self.merge_all_columns_to_string, axis=1\n        )\n        return all_dataset_metadata\n\n    @staticmethod\n    def subset_metadata(\n        subset_ids: Sequence[int] | None, all_dataset_metadata: pd.DataFrame\n    ):\n        if subset_ids is not None:\n            subset_ids = [int(x) for x in subset_ids]\n            all_dataset_metadata = all_dataset_metadata[\n                all_dataset_metadata[\"did\"].isin(subset_ids)\n            ]\n        return all_dataset_metadata\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.combine_metadata","title":"<code>combine_metadata(all_dataset_metadata, all_data_description_df)</code>","text":"<p>Description: Combine the descriptions with the metadata table.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def combine_metadata(\n    self, all_dataset_metadata: pd.DataFrame, all_data_description_df: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Combine the descriptions with the metadata table.\n    \"\"\"\n    all_dataset_metadata = pd.merge(\n        all_dataset_metadata, all_data_description_df, on=\"did\", how=\"inner\"\n    )\n    all_dataset_metadata[\"Combined_information\"] = all_dataset_metadata.apply(\n        self.merge_all_columns_to_string, axis=1\n    )\n    return all_dataset_metadata\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.create_combined_information_df_for_datasets","title":"<code>create_combined_information_df_for_datasets(data_id, descriptions, joined_qualities, joined_features)</code>  <code>staticmethod</code>","text":"<p>Description: Create a dataframe with the combined information of the OpenML object.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>@staticmethod\ndef create_combined_information_df_for_datasets(\n    data_id: int | Sequence[int],\n    descriptions: Sequence[str],\n    joined_qualities: Sequence[str],\n    joined_features: Sequence[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Create a dataframe with the combined information of the OpenML object.\n    \"\"\"\n    return pd.DataFrame(\n        {\n            \"did\": data_id,\n            \"description\": descriptions,\n            \"qualities\": joined_qualities,\n            \"features\": joined_features,\n        }\n    )\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.extract_attribute","title":"<code>extract_attribute(attribute, attr_name)</code>  <code>staticmethod</code>","text":"<p>Description: Extract an attribute from the OpenML object.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>@staticmethod\ndef extract_attribute(attribute: object, attr_name: str) -&gt; str:\n    \"\"\"\n    Description: Extract an attribute from the OpenML object.\n    \"\"\"\n    return getattr(attribute, attr_name, \"\")\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.get_description","title":"<code>get_description(data_id)</code>","text":"<p>Description: Get the description of the OpenML object.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_description(self, data_id: int):\n    \"\"\"\n    Description: Get the description of the OpenML object.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.get_metadata","title":"<code>get_metadata(data_id)</code>","text":"<p>Description: Get metadata from OpenML using parallel processing.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_metadata(self, data_id: Sequence[int]):\n    \"\"\"\n    Description: Get metadata from OpenML using parallel processing.\n    \"\"\"\n    return pqdm(\n        data_id, self.get_description, n_jobs=self.config[\"data_download_n_jobs\"]\n    )\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.get_openml_objects","title":"<code>get_openml_objects()</code>","text":"<p>Description: Get the OpenML objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_openml_objects(self):\n    \"\"\"\n    Description: Get the OpenML objects.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.initialize_cache","title":"<code>initialize_cache(data_id)</code>","text":"<p>Description: Initialize the cache for the OpenML objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def initialize_cache(self, data_id: Sequence[int]) -&gt; None:\n    \"\"\"\n    Description: Initialize the cache for the OpenML objects.\n    \"\"\"\n    self.get_description(data_id[0])\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.join_attributes","title":"<code>join_attributes(attribute, attr_name)</code>  <code>staticmethod</code>","text":"<p>Description: Join the attributes of the OpenML object.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>@staticmethod\ndef join_attributes(attribute: object, attr_name: str) -&gt; str:\n    \"\"\"\n    Description: Join the attributes of the OpenML object.\n    \"\"\"\n    return (\n        \" \".join(\n            [f\"{k} : {v},\" for k, v in getattr(attribute, attr_name, {}).items()]\n        )\n        if hasattr(attribute, attr_name)\n        else \"\"\n    )\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.load_metadata","title":"<code>load_metadata(file_path)</code>  <code>staticmethod</code>","text":"<p>Description: Load metadata from a file.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>@staticmethod\ndef load_metadata(file_path: str):\n    \"\"\"\n    Description: Load metadata from a file.\n    \"\"\"\n    try:\n        return pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise Exception(\n            \"Metadata files do not exist. Please run the training pipeline first.\"\n        )\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.merge_all_columns_to_string","title":"<code>merge_all_columns_to_string(row)</code>  <code>staticmethod</code>","text":"<p>Description: Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>@staticmethod\ndef merge_all_columns_to_string(row: pd.Series) -&gt; str:\n    \"\"\"\n    Description: Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n    \"\"\"\n    return \" \".join([f\"{col} - {val},\" for col, val in zip(row.index, row.values)])\n</code></pre>"},{"location":"Rag%20Pipeline/metadata_module/#metadata_utils.OpenMLObjectHandler.process_metadata","title":"<code>process_metadata(openml_data_object, data_id, all_dataset_metadata, file_path, subset_ids=None)</code>","text":"<p>Description: Process the metadata.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def process_metadata(\n    self,\n    openml_data_object,\n    data_id: Sequence[int],\n    all_dataset_metadata: pd.DataFrame,\n    file_path: str,\n    subset_ids=None,\n):\n    \"\"\"\n    Description: Process the metadata.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Rag%20Pipeline/result_gen/","title":"Result gen","text":""},{"location":"Rag%20Pipeline/result_gen/#results-generation","title":"Results Generation","text":"<ul> <li>Code for creating the output dataframes/ids</li> <li>This is also for adding extra functionality like Long Context Reordering, Flash Ranking etc.</li> </ul>"},{"location":"Rag%20Pipeline/result_gen/#results_gen.QueryProcessor","title":"<code>QueryProcessor</code>","text":"Source code in <code>backend/modules/results_gen.py</code> <pre><code>class QueryProcessor:\n    def __init__(self, query: str, qa: RetrievalQA, type_of_query: str, config: dict):\n        self.query = query\n        self.qa = qa\n        self.type_of_query = type_of_query\n        self.config = config\n\n    def fetch_results(self):\n        \"\"\"\n        Fetch results for the query using the QA chain.\n        \"\"\"\n        results = self.qa.invoke(\n            input=self.query,\n            config={\n                \"temperature\": self.config[\"temperature\"],\n                \"top-p\": self.config[\"top_p\"],\n            },\n        )\n        if self.config[\"long_context_reorder\"]:\n            results = long_context_reorder(results)\n        id_column = {\"dataset\": \"did\", \"flow\": \"id\", \"data\": \"did\"}[self.type_of_query]\n\n        if self.config[\"reranking\"]:\n            try:\n                print(\"[INFO] Reranking results...\")\n                ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/tmp/\")\n                rerankrequest = RerankRequest(\n                    query=self.query,\n                    passages=[\n                        {\"id\": result.metadata[id_column], \"text\": result.page_content}\n                        for result in results\n                    ],\n                )\n                ranking = ranker.rerank(rerankrequest)\n                ids = [result[\"id\"] for result in ranking]\n                ranked_results = [\n                    result for result in results if result.metadata[id_column] in ids\n                ]\n                print(\"[INFO] Reranking complete.\")\n                return ranked_results\n            except Exception as e:\n                print(f\"[ERROR] Reranking failed: {e}\")\n                return results\n        else:\n            return results\n\n    @staticmethod\n    def process_documents(\n        source_documents: Sequence[Document],\n    ) -&gt; Tuple[OrderedDict, list]:\n        \"\"\"\n        Process the source documents and create a dictionary with the key_name as the key and the name and page content as the values.\n        \"\"\"\n        dict_results = OrderedDict()\n        for result in source_documents:\n            dict_results[result.metadata[\"did\"]] = {\n                \"name\": result.metadata[\"name\"],\n                \"page_content\": result.page_content,\n            }\n        ids = [result.metadata[\"did\"] for result in source_documents]\n        return dict_results, ids\n\n    @staticmethod\n    def make_clickable(val: str) -&gt; str:\n        \"\"\"\n        Make the URL clickable in the dataframe.\n        \"\"\"\n        return '&lt;a href=\"{}\"&gt;{}&lt;/a&gt;'.format(val, val)\n\n    def create_output_dataframe(\n        self, dict_results: dict, type_of_data: str, ids_order: list\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Create an output dataframe with the results. The URLs are API calls to the OpenML API for the specific type of data.\n        \"\"\"\n        output_df = pd.DataFrame(dict_results).T.reset_index()\n        output_df[\"index\"] = output_df[\"index\"].astype(int)\n        output_df = output_df.set_index(\"index\").loc[ids_order].reset_index()\n        output_df[\"urls\"] = output_df[\"index\"].apply(\n            lambda x: f\"https://www.openml.org/search?type={type_of_data}&amp;id={x}\"\n        )\n        output_df[\"urls\"] = output_df[\"urls\"].apply(self.make_clickable)\n\n        if type_of_data == \"data\":\n            output_df[\"command\"] = output_df[\"index\"].apply(\n                lambda x: f\"dataset = openml.datasets.get_dataset({x})\"\n            )\n        elif type_of_data == \"flow\":\n            output_df[\"command\"] = output_df[\"index\"].apply(\n                lambda x: f\"flow = openml.flows.get_flow({x})\"\n            )\n        output_df = output_df.drop_duplicates(subset=[\"name\"])\n        replace_dict = {\n            \"index\": \"id\",\n            \"command\": \"Command\",\n            \"urls\": \"OpenML URL\",\n            \"page_content\": \"Description\",\n        }\n        for col in [\"index\", \"command\", \"urls\", \"page_content\"]:\n            if col in output_df.columns:\n                output_df = output_df.rename(columns={col: replace_dict[col]})\n        return output_df\n\n    @staticmethod\n    def check_query(query: str) -&gt; str:\n        \"\"\"\n        Performs checks on the query:\n        - Replaces %20 with space character (browsers do this automatically when spaces are in the URL)\n        - Removes leading and trailing spaces\n        - Limits the query to 200 characters\n        \"\"\"\n        if query == \"\":\n            raise ValueError(\"Query cannot be empty.\")\n        query = query.replace(\"%20\", \" \")\n        query = query.strip()\n        query = query[:200]\n        return query\n\n    def get_result_from_query(self) -&gt; Tuple[pd.DataFrame, Sequence[Document]]:\n        \"\"\"\n        Get the result from the query using the QA chain and return the results in a dataframe that is then sent to the frontend.\n        \"\"\"\n        if self.type_of_query == \"dataset\":\n            type_of_query = \"data\"\n        elif self.type_of_query == \"flow\":\n            type_of_query = \"flow\"\n        else:\n            raise ValueError(f\"Unsupported type_of_data: {self.type_of_query}\")\n\n        query = self.check_query(self.query)\n        if query == \"\":\n            return pd.DataFrame(), []\n\n        source_documents = self.fetch_results()\n        dict_results, ids_order = self.process_documents(source_documents)\n        output_df = self.create_output_dataframe(dict_results, type_of_query, ids_order)\n\n        return output_df, ids_order\n</code></pre>"},{"location":"Rag%20Pipeline/result_gen/#results_gen.QueryProcessor.check_query","title":"<code>check_query(query)</code>  <code>staticmethod</code>","text":"<p>Performs checks on the query: - Replaces %20 with space character (browsers do this automatically when spaces are in the URL) - Removes leading and trailing spaces - Limits the query to 200 characters</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>@staticmethod\ndef check_query(query: str) -&gt; str:\n    \"\"\"\n    Performs checks on the query:\n    - Replaces %20 with space character (browsers do this automatically when spaces are in the URL)\n    - Removes leading and trailing spaces\n    - Limits the query to 200 characters\n    \"\"\"\n    if query == \"\":\n        raise ValueError(\"Query cannot be empty.\")\n    query = query.replace(\"%20\", \" \")\n    query = query.strip()\n    query = query[:200]\n    return query\n</code></pre>"},{"location":"Rag%20Pipeline/result_gen/#results_gen.QueryProcessor.create_output_dataframe","title":"<code>create_output_dataframe(dict_results, type_of_data, ids_order)</code>","text":"<p>Create an output dataframe with the results. The URLs are API calls to the OpenML API for the specific type of data.</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def create_output_dataframe(\n    self, dict_results: dict, type_of_data: str, ids_order: list\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create an output dataframe with the results. The URLs are API calls to the OpenML API for the specific type of data.\n    \"\"\"\n    output_df = pd.DataFrame(dict_results).T.reset_index()\n    output_df[\"index\"] = output_df[\"index\"].astype(int)\n    output_df = output_df.set_index(\"index\").loc[ids_order].reset_index()\n    output_df[\"urls\"] = output_df[\"index\"].apply(\n        lambda x: f\"https://www.openml.org/search?type={type_of_data}&amp;id={x}\"\n    )\n    output_df[\"urls\"] = output_df[\"urls\"].apply(self.make_clickable)\n\n    if type_of_data == \"data\":\n        output_df[\"command\"] = output_df[\"index\"].apply(\n            lambda x: f\"dataset = openml.datasets.get_dataset({x})\"\n        )\n    elif type_of_data == \"flow\":\n        output_df[\"command\"] = output_df[\"index\"].apply(\n            lambda x: f\"flow = openml.flows.get_flow({x})\"\n        )\n    output_df = output_df.drop_duplicates(subset=[\"name\"])\n    replace_dict = {\n        \"index\": \"id\",\n        \"command\": \"Command\",\n        \"urls\": \"OpenML URL\",\n        \"page_content\": \"Description\",\n    }\n    for col in [\"index\", \"command\", \"urls\", \"page_content\"]:\n        if col in output_df.columns:\n            output_df = output_df.rename(columns={col: replace_dict[col]})\n    return output_df\n</code></pre>"},{"location":"Rag%20Pipeline/result_gen/#results_gen.QueryProcessor.fetch_results","title":"<code>fetch_results()</code>","text":"<p>Fetch results for the query using the QA chain.</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def fetch_results(self):\n    \"\"\"\n    Fetch results for the query using the QA chain.\n    \"\"\"\n    results = self.qa.invoke(\n        input=self.query,\n        config={\n            \"temperature\": self.config[\"temperature\"],\n            \"top-p\": self.config[\"top_p\"],\n        },\n    )\n    if self.config[\"long_context_reorder\"]:\n        results = long_context_reorder(results)\n    id_column = {\"dataset\": \"did\", \"flow\": \"id\", \"data\": \"did\"}[self.type_of_query]\n\n    if self.config[\"reranking\"]:\n        try:\n            print(\"[INFO] Reranking results...\")\n            ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/tmp/\")\n            rerankrequest = RerankRequest(\n                query=self.query,\n                passages=[\n                    {\"id\": result.metadata[id_column], \"text\": result.page_content}\n                    for result in results\n                ],\n            )\n            ranking = ranker.rerank(rerankrequest)\n            ids = [result[\"id\"] for result in ranking]\n            ranked_results = [\n                result for result in results if result.metadata[id_column] in ids\n            ]\n            print(\"[INFO] Reranking complete.\")\n            return ranked_results\n        except Exception as e:\n            print(f\"[ERROR] Reranking failed: {e}\")\n            return results\n    else:\n        return results\n</code></pre>"},{"location":"Rag%20Pipeline/result_gen/#results_gen.QueryProcessor.get_result_from_query","title":"<code>get_result_from_query()</code>","text":"<p>Get the result from the query using the QA chain and return the results in a dataframe that is then sent to the frontend.</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def get_result_from_query(self) -&gt; Tuple[pd.DataFrame, Sequence[Document]]:\n    \"\"\"\n    Get the result from the query using the QA chain and return the results in a dataframe that is then sent to the frontend.\n    \"\"\"\n    if self.type_of_query == \"dataset\":\n        type_of_query = \"data\"\n    elif self.type_of_query == \"flow\":\n        type_of_query = \"flow\"\n    else:\n        raise ValueError(f\"Unsupported type_of_data: {self.type_of_query}\")\n\n    query = self.check_query(self.query)\n    if query == \"\":\n        return pd.DataFrame(), []\n\n    source_documents = self.fetch_results()\n    dict_results, ids_order = self.process_documents(source_documents)\n    output_df = self.create_output_dataframe(dict_results, type_of_query, ids_order)\n\n    return output_df, ids_order\n</code></pre>"},{"location":"Rag%20Pipeline/result_gen/#results_gen.QueryProcessor.make_clickable","title":"<code>make_clickable(val)</code>  <code>staticmethod</code>","text":"<p>Make the URL clickable in the dataframe.</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>@staticmethod\ndef make_clickable(val: str) -&gt; str:\n    \"\"\"\n    Make the URL clickable in the dataframe.\n    \"\"\"\n    return '&lt;a href=\"{}\"&gt;{}&lt;/a&gt;'.format(val, val)\n</code></pre>"},{"location":"Rag%20Pipeline/result_gen/#results_gen.QueryProcessor.process_documents","title":"<code>process_documents(source_documents)</code>  <code>staticmethod</code>","text":"<p>Process the source documents and create a dictionary with the key_name as the key and the name and page content as the values.</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>@staticmethod\ndef process_documents(\n    source_documents: Sequence[Document],\n) -&gt; Tuple[OrderedDict, list]:\n    \"\"\"\n    Process the source documents and create a dictionary with the key_name as the key and the name and page content as the values.\n    \"\"\"\n    dict_results = OrderedDict()\n    for result in source_documents:\n        dict_results[result.metadata[\"did\"]] = {\n            \"name\": result.metadata[\"name\"],\n            \"page_content\": result.page_content,\n        }\n    ids = [result.metadata[\"did\"] for result in source_documents]\n    return dict_results, ids\n</code></pre>"},{"location":"Rag%20Pipeline/result_gen/#results_gen.long_context_reorder","title":"<code>long_context_reorder(results)</code>","text":"<p>Description: Lost in the middle reorder: the less relevant documents will be at the middle of the list and more relevant elements at beginning / end. See: https://arxiv.org/abs//2307.03172</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def long_context_reorder(results):\n    \"\"\"\n    Description: Lost in the middle reorder: the less relevant documents will be at the\n    middle of the list and more relevant elements at beginning / end.\n    See: https://arxiv.org/abs//2307.03172\n\n\n    \"\"\"\n    print(\"[INFO] Reordering results...\")\n    reordering = LongContextReorder()\n    results = reordering.transform_documents(results)\n    print(\"[INFO] Reordering complete.\")\n    return results\n</code></pre>"},{"location":"Rag%20Pipeline/training/","title":"Training","text":"<ul> <li>While we are not creating a new model, we are using the existing model to create embeddings. The name might be misleading but this was chosen as an attempt to keep the naming consistent with other codebases.</li> <li>(Perhaps we might fine tune the model in the future)</li> <li>The training script is present in <code>training.py</code>. Running this script will take care of everything.</li> </ul>"},{"location":"Rag%20Pipeline/training/#what-does-the-training-script-do","title":"What does the training script do?","text":"<ul> <li>Load the config file and set the necessary variables</li> <li>If <code>testing_flag</code> is set to True, the script will use a subset of the data for quick debugging</li> <li>testing_flag is set to True</li> <li>persist_dir is set to ./data/chroma_db_testing</li> <li>test_subset is set to True</li> <li>data_dir is set to ./data/testing_data/</li> <li>If <code>testing_flag</code> is set to False, the script will use the entire dataset</li> <li>For all datasets in the OpenML dataset list:</li> <li>Download the dataset</li> <li>Create the vector dataset with computed embeddings</li> <li>Create a vectordb retriever </li> <li>Run some test queries</li> </ul>"},{"location":"Rag%20Pipeline/vector_store/","title":"Vector Store Utilities","text":"<ul> <li>Code for loading data into the vector store.</li> </ul>"},{"location":"Rag%20Pipeline/vector_store/#what-to-look-for","title":"What to look for","text":"<ul> <li><code>DataLoader</code>: If you want to modify chunking</li> <li><code>DocumentProcessor</code>: If you want to modify how the unique documents are obtained and/or add other methods</li> <li><code>VectorStoreManager</code>: If you want to modify how the documents are embedded and loaded to the vector store</li> </ul>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.DataLoader","title":"<code>DataLoader</code>","text":"<p>Description: Used to chunk data</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>class DataLoader:\n    \"\"\"\n    Description: Used to chunk data\n    \"\"\"\n\n    def __init__(\n        self,\n        metadata_df: pd.DataFrame,\n        page_content_column: str,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 150,\n    ):\n        self.metadata_df = metadata_df\n        self.page_content_column = page_content_column\n        self.chunk_size = chunk_size\n        self.chunk_overlap = (\n            chunk_overlap if self.chunk_size &gt; chunk_overlap else self.chunk_size\n        )\n\n    def load_and_process_data(self) -&gt; list:\n        \"\"\"\n        Description: Recursively chunk data before embedding\n        \"\"\"\n        loader = DataFrameLoader(\n            self.metadata_df, page_content_column=self.page_content_column\n        )\n        documents = loader.load()\n\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap\n        )\n        documents = text_splitter.split_documents(documents)\n\n        return documents\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.DataLoader.load_and_process_data","title":"<code>load_and_process_data()</code>","text":"<p>Description: Recursively chunk data before embedding</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>def load_and_process_data(self) -&gt; list:\n    \"\"\"\n    Description: Recursively chunk data before embedding\n    \"\"\"\n    loader = DataFrameLoader(\n        self.metadata_df, page_content_column=self.page_content_column\n    )\n    documents = loader.load()\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap\n    )\n    documents = text_splitter.split_documents(documents)\n\n    return documents\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.DocumentProcessor","title":"<code>DocumentProcessor</code>","text":"<p>Description: Used to generate unique documents based on text content to prevent duplicates during embedding</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>class DocumentProcessor:\n    \"\"\"\n    Description: Used to generate unique documents based on text content to prevent duplicates during embedding\n    \"\"\"\n\n    @staticmethod\n    def generate_unique_documents(documents: list, db: Chroma) -&gt; tuple:\n        \"\"\"\n        Description: Sometimes the text content of the data is the same, this ensures that does not happen by computing a string matching\n        \"\"\"\n        new_document_ids = set([str(x.metadata[\"did\"]) for x in documents])\n        print(f\"[INFO] Generating unique documents. Total documents: {len(documents)}\")\n        try:\n            old_dids = set([str(x[\"did\"]) for x in db.get()[\"metadatas\"]])\n        except KeyError:\n            old_dids = set([str(x[\"id\"]) for x in db.get()[\"metadatas\"]])\n\n        new_dids = new_document_ids - old_dids\n        documents = [x for x in documents if str(x.metadata[\"did\"]) in new_dids]\n        ids = [\n            str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in documents\n        ]\n\n        unique_ids = list(set(ids))\n        seen_ids = set()\n        unique_docs = [\n            doc\n            for doc, id in zip(documents, ids)\n            if id not in seen_ids and (seen_ids.add(id) or True)\n        ]\n\n        return unique_docs, unique_ids\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.DocumentProcessor.generate_unique_documents","title":"<code>generate_unique_documents(documents, db)</code>  <code>staticmethod</code>","text":"<p>Description: Sometimes the text content of the data is the same, this ensures that does not happen by computing a string matching</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>@staticmethod\ndef generate_unique_documents(documents: list, db: Chroma) -&gt; tuple:\n    \"\"\"\n    Description: Sometimes the text content of the data is the same, this ensures that does not happen by computing a string matching\n    \"\"\"\n    new_document_ids = set([str(x.metadata[\"did\"]) for x in documents])\n    print(f\"[INFO] Generating unique documents. Total documents: {len(documents)}\")\n    try:\n        old_dids = set([str(x[\"did\"]) for x in db.get()[\"metadatas\"]])\n    except KeyError:\n        old_dids = set([str(x[\"id\"]) for x in db.get()[\"metadatas\"]])\n\n    new_dids = new_document_ids - old_dids\n    documents = [x for x in documents if str(x.metadata[\"did\"]) in new_dids]\n    ids = [\n        str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in documents\n    ]\n\n    unique_ids = list(set(ids))\n    seen_ids = set()\n    unique_docs = [\n        doc\n        for doc, id in zip(documents, ids)\n        if id not in seen_ids and (seen_ids.add(id) or True)\n    ]\n\n    return unique_docs, unique_ids\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.VectorStoreManager","title":"<code>VectorStoreManager</code>","text":"<p>Description: Manages the Vector store (chromadb) and takes care of data ingestion, loading the embedding model and embedding the data before adding it to the vector store</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>class VectorStoreManager:\n    \"\"\"\n    Description: Manages the Vector store (chromadb) and takes care of data ingestion, loading the embedding model and embedding the data before adding it to the vector store\n    \"\"\"\n\n    def __init__(self, chroma_client: ClientAPI, config: dict):\n        self.chroma_client = chroma_client\n        self.config = config\n        self.chunk_size = 100\n\n    def chunk_dataframe(self, df, chunk_size):\n        \"\"\"\n        Description: Chunk dataframe for use with chroma metadata saving\n        \"\"\"\n        for i in range(0, df.shape[0], self.chunk_size):\n            yield df.iloc[i : i + self.chunk_size]\n\n    def add_df_chunks_to_db(self, metadata_df):\n        \"\"\"\n        Description: Add chunks from a dataframe for use with chroma metadata saving\n        \"\"\"\n        collec = self.chroma_client.get_or_create_collection(\"metadata\")\n        for chunk in tqdm(\n            self.chunk_dataframe(metadata_df, self.chunk_size),\n            total=(len(metadata_df) // self.chunk_size) + 1,\n        ):\n            ids = chunk[\"did\"].astype(str).tolist()\n            documents = chunk[\"description\"].astype(str).tolist()\n            metadatas = chunk.to_dict(orient=\"records\")\n\n            # Add to ChromaDB collection\n            collec.add(ids=ids, documents=documents, metadatas=metadatas)\n\n    def load_model(self) -&gt; HuggingFaceEmbeddings | None:\n        \"\"\"\n        Description: Load a model from Hugging face for embedding\n        \"\"\"\n        print(\"[INFO] Loading model...\")\n        model_kwargs = {\"device\": self.config[\"device\"], \"trust_remote_code\": True}\n        encode_kwargs = {\"normalize_embeddings\": True}\n        embeddings = HuggingFaceEmbeddings(\n            model_name=self.config[\"embedding_model\"],\n            model_kwargs=model_kwargs,\n            encode_kwargs=encode_kwargs,\n            show_progress=False,\n        )\n        print(\"[INFO] Model loaded.\")\n        return embeddings\n\n    def get_collection_name(self) -&gt; str:\n        \"\"\"\n        Description: Fixes some collection names. (workaround from OpenML API)\n        \"\"\"\n        return {\"dataset\": \"datasets\", \"flow\": \"flows\"}.get(\n            self.config[\"type_of_data\"], \"default\"\n        )\n\n    def load_vector_store(\n        self, embeddings: HuggingFaceEmbeddings, collection_name: str\n    ) -&gt; Chroma:\n        \"\"\"\n        Description: Persist directory. If does not exist, cannot be served\n        \"\"\"\n        if not os.path.exists(self.config[\"persist_dir\"]):\n            raise Exception(\n                \"Persist directory does not exist. Please run the training pipeline first.\"\n            )\n\n        return Chroma(\n            client=self.chroma_client,\n            persist_directory=self.config[\"persist_dir\"],\n            embedding_function=embeddings,\n            collection_name=collection_name,\n        )\n\n    @staticmethod\n    def add_documents_to_db(db, unique_docs, unique_ids, bs=512):\n        \"\"\"\n        Description: Add documents to Chroma DB in batches of bs\n        \"\"\"\n        if len(unique_docs) &lt; bs:\n            db.add_documents(unique_docs, ids=unique_ids)\n        else:\n            for i in range(0, len(unique_docs), bs):\n                db.add_documents(unique_docs[i : i + bs], ids=unique_ids[i : i + bs])\n\n    def create_vector_store(self, metadata_df: pd.DataFrame) -&gt; Chroma:\n        \"\"\"\n        Description: Load embeddings, get chunked data, subset if needed , find unique, and then finally add to ChromaDB\n        \"\"\"\n        embeddings = self.load_model()\n        collection_name = self.get_collection_name()\n\n        db = Chroma(\n            client=self.chroma_client,\n            embedding_function=embeddings,\n            persist_directory=self.config[\"persist_dir\"],\n            collection_name=collection_name,\n        )\n\n        data_loader = DataLoader(\n            metadata_df,\n            page_content_column=\"Combined_information\",\n            chunk_size=self.config[\"chunk_size\"],\n        )\n        documents = data_loader.load_and_process_data()\n\n        if self.config[\"testing_flag\"]:\n            if self.config[\"test_subset\"]:\n                print(\"[INFO] Subsetting the data.\")\n                documents = documents[:500]\n\n        unique_docs, unique_ids = DocumentProcessor.generate_unique_documents(\n            documents, db\n        )\n        print(\n            f\"Number of unique documents: {len(unique_docs)} vs Total documents: {len(documents)}\"\n        )\n\n        if len(unique_docs) == 0:\n            print(\"No new documents to add.\")\n        else:\n            self.add_documents_to_db(db, unique_docs, unique_ids)\n\n        return db\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.VectorStoreManager.add_df_chunks_to_db","title":"<code>add_df_chunks_to_db(metadata_df)</code>","text":"<p>Description: Add chunks from a dataframe for use with chroma metadata saving</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>def add_df_chunks_to_db(self, metadata_df):\n    \"\"\"\n    Description: Add chunks from a dataframe for use with chroma metadata saving\n    \"\"\"\n    collec = self.chroma_client.get_or_create_collection(\"metadata\")\n    for chunk in tqdm(\n        self.chunk_dataframe(metadata_df, self.chunk_size),\n        total=(len(metadata_df) // self.chunk_size) + 1,\n    ):\n        ids = chunk[\"did\"].astype(str).tolist()\n        documents = chunk[\"description\"].astype(str).tolist()\n        metadatas = chunk.to_dict(orient=\"records\")\n\n        # Add to ChromaDB collection\n        collec.add(ids=ids, documents=documents, metadatas=metadatas)\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.VectorStoreManager.add_documents_to_db","title":"<code>add_documents_to_db(db, unique_docs, unique_ids, bs=512)</code>  <code>staticmethod</code>","text":"<p>Description: Add documents to Chroma DB in batches of bs</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>@staticmethod\ndef add_documents_to_db(db, unique_docs, unique_ids, bs=512):\n    \"\"\"\n    Description: Add documents to Chroma DB in batches of bs\n    \"\"\"\n    if len(unique_docs) &lt; bs:\n        db.add_documents(unique_docs, ids=unique_ids)\n    else:\n        for i in range(0, len(unique_docs), bs):\n            db.add_documents(unique_docs[i : i + bs], ids=unique_ids[i : i + bs])\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.VectorStoreManager.chunk_dataframe","title":"<code>chunk_dataframe(df, chunk_size)</code>","text":"<p>Description: Chunk dataframe for use with chroma metadata saving</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>def chunk_dataframe(self, df, chunk_size):\n    \"\"\"\n    Description: Chunk dataframe for use with chroma metadata saving\n    \"\"\"\n    for i in range(0, df.shape[0], self.chunk_size):\n        yield df.iloc[i : i + self.chunk_size]\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.VectorStoreManager.create_vector_store","title":"<code>create_vector_store(metadata_df)</code>","text":"<p>Description: Load embeddings, get chunked data, subset if needed , find unique, and then finally add to ChromaDB</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>def create_vector_store(self, metadata_df: pd.DataFrame) -&gt; Chroma:\n    \"\"\"\n    Description: Load embeddings, get chunked data, subset if needed , find unique, and then finally add to ChromaDB\n    \"\"\"\n    embeddings = self.load_model()\n    collection_name = self.get_collection_name()\n\n    db = Chroma(\n        client=self.chroma_client,\n        embedding_function=embeddings,\n        persist_directory=self.config[\"persist_dir\"],\n        collection_name=collection_name,\n    )\n\n    data_loader = DataLoader(\n        metadata_df,\n        page_content_column=\"Combined_information\",\n        chunk_size=self.config[\"chunk_size\"],\n    )\n    documents = data_loader.load_and_process_data()\n\n    if self.config[\"testing_flag\"]:\n        if self.config[\"test_subset\"]:\n            print(\"[INFO] Subsetting the data.\")\n            documents = documents[:500]\n\n    unique_docs, unique_ids = DocumentProcessor.generate_unique_documents(\n        documents, db\n    )\n    print(\n        f\"Number of unique documents: {len(unique_docs)} vs Total documents: {len(documents)}\"\n    )\n\n    if len(unique_docs) == 0:\n        print(\"No new documents to add.\")\n    else:\n        self.add_documents_to_db(db, unique_docs, unique_ids)\n\n    return db\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.VectorStoreManager.get_collection_name","title":"<code>get_collection_name()</code>","text":"<p>Description: Fixes some collection names. (workaround from OpenML API)</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>def get_collection_name(self) -&gt; str:\n    \"\"\"\n    Description: Fixes some collection names. (workaround from OpenML API)\n    \"\"\"\n    return {\"dataset\": \"datasets\", \"flow\": \"flows\"}.get(\n        self.config[\"type_of_data\"], \"default\"\n    )\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.VectorStoreManager.load_model","title":"<code>load_model()</code>","text":"<p>Description: Load a model from Hugging face for embedding</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>def load_model(self) -&gt; HuggingFaceEmbeddings | None:\n    \"\"\"\n    Description: Load a model from Hugging face for embedding\n    \"\"\"\n    print(\"[INFO] Loading model...\")\n    model_kwargs = {\"device\": self.config[\"device\"], \"trust_remote_code\": True}\n    encode_kwargs = {\"normalize_embeddings\": True}\n    embeddings = HuggingFaceEmbeddings(\n        model_name=self.config[\"embedding_model\"],\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs,\n        show_progress=False,\n    )\n    print(\"[INFO] Model loaded.\")\n    return embeddings\n</code></pre>"},{"location":"Rag%20Pipeline/vector_store/#vector_store_utils.VectorStoreManager.load_vector_store","title":"<code>load_vector_store(embeddings, collection_name)</code>","text":"<p>Description: Persist directory. If does not exist, cannot be served</p> Source code in <code>backend/modules/vector_store_utils.py</code> <pre><code>def load_vector_store(\n    self, embeddings: HuggingFaceEmbeddings, collection_name: str\n) -&gt; Chroma:\n    \"\"\"\n    Description: Persist directory. If does not exist, cannot be served\n    \"\"\"\n    if not os.path.exists(self.config[\"persist_dir\"]):\n        raise Exception(\n            \"Persist directory does not exist. Please run the training pipeline first.\"\n        )\n\n    return Chroma(\n        client=self.chroma_client,\n        persist_directory=self.config[\"persist_dir\"],\n        embedding_function=embeddings,\n        collection_name=collection_name,\n    )\n</code></pre>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/","title":"Developer Tutorials","text":"<ul> <li>Hello there, future OpenML contributor! It is nice meeting you here. This page is a collection of tutorials that will help you get started with contributing to the OpenML RAG pipeline.</li> <li>The tutorials show you how to perform common tasks and should make it a lot easier to get started with contributing to this project.</li> <li>Note that you would have had to setup the project before you begin. If you missed this step, please refer to </li> </ul>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/#how-to-use-them","title":"How to use them","text":"<ul> <li>Once you have setup the project, just navigate to the tutorial you are interested in and open them in your IDE.</li> </ul>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/change%20data%20input/","title":"Change data input","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n</code></pre> <pre><code>from backend.modules.utils import *\nfrom backend.modules.rag_llm import *\nfrom backend.modules.results_gen import *\n</code></pre> <pre>\n<code>/Users/smukherjee/.pyenv/versions/3.10.14/envs/openml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</code>\n</pre> <pre><code>config = load_config_and_device(\"../../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../data/doc_examples/chroma_db/\"\nconfig[\"data_dir\"] = \"../../data/doc_examples/\"\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = False\nconfig[\"testing_flag\"] = True  # set this to false while training, this is for demo\nconfig[\"test_subset\"] = True  # set this to false while training, this is for demo\n\n# load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\nprint(config)\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: mps\n{'rqa_prompt_template': 'This database is a list of metadata. Use the following pieces of context to find the relevant document. Answer only from the context given using the {question} given. If you do not know the answer, say you do not know. {context}', 'llm_prompt_template': 'The following is a set of documents {docs}. Based on these docs, please summarize the content concisely. Also give a list of main concepts found in the documents. Do not add any new information. Helpful Answer: ', 'num_return_documents': 30, 'embedding_model': 'BAAI/bge-large-en-v1.5', 'llm_model': 'llama3', 'num_documents_for_llm': 30, 'data_dir': '../../data/doc_examples/', 'persist_dir': '../../data/doc_examples/chroma_db/', 'testing_flag': True, 'ignore_downloading_data': False, 'test_subset': True, 'data_download_n_jobs': 20, 'training': False, 'temperature': 0.95, 'top_p': 0.95, 'search_type': 'similarity', 'reranking': False, 'long_context_reorder': False, 'structure_query': False, 'use_chroma_for_saving_metadata': False, 'device': 'mps', 'type_of_data': 'dataset'}\n</code>\n</pre> <pre><code>def join_attributes(attribute: object, attr_name: str) -&amp;gt; str:\n    \"\"\"\n    Description: Join the attributes of the OpenML objects into a single string with the format \"key : value\"\n    \"\"\"\n    return (\n        \" ~ \".join(\n            [f\"{k} : {v},\" for k, v in getattr(attribute, attr_name, {}).items()]\n        )\n        if hasattr(attribute, attr_name)\n        else \"\"\n    )\n\n\ndef combine_metadata(\n    self, all_dataset_metadata: pd.DataFrame, all_data_description_df: pd.DataFrame\n) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Description: Combine the descriptions with the metadata table.\n    \"\"\"\n    all_dataset_metadata = pd.merge(\n        all_dataset_metadata, all_data_description_df, on=\"did\", how=\"inner\"\n    )\n    all_dataset_metadata[\"Combined_information\"] = all_dataset_metadata.apply(\n        self.merge_all_columns_to_string, axis=1\n    )\n    return all_dataset_metadata\n</code></pre> <pre><code>OpenMLObjectHandler.join_attributes = join_attributes\nOpenMLObjectHandler.combine_metadata = combine_metadata\n</code></pre> <pre><code># Setup llm chain, initialize the retriever and llm, and setup Retrieval QA\nqa_dataset_handler = QASetup(\n    config=config,\n    data_type=config[\"type_of_data\"],\n    client=client,\n)\n</code></pre> <pre><code>qa_dataset, _ = qa_dataset_handler.setup_vector_db_and_qa()\n</code></pre>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/change%20data%20input/#change-the-way-the-data-is-combined","title":"Change the way the data is combined","text":"<ul> <li>To pass to the RAG, all the metadata is combined into a single string. This is done by concatenating all the metadata fields with a space separator.</li> <li>We can change the way the data in whatever way we want. For example, we can concatenate all the metadata fields with a \"~\" separator.</li> </ul>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/change%20model/","title":"Change model","text":"<pre><code>from __future__ import annotations\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n</code></pre> <pre><code>from backend.modules.utils import load_config_and_device\nfrom backend.modules.rag_llm import QASetup\n</code></pre> <pre><code>config = load_config_and_device(\"../../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../data/doc_examples/chroma_db/\"\nconfig[\"data_dir\"] = \"../../data/doc_examples/\"\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = True\nconfig[\"test_subset\"] = True  # set this to false while training, this is for demo\n# load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\nprint(config)\n</code></pre> <pre><code>config[\"embedding_model\"] = \"BAAI/bge-large-en-v1.5\"\n</code></pre> <ul> <li>Pick a model from Ollama - https://ollama.com/library?sort=popular</li> <li>eg : mistral</li> </ul> <pre><code>config[\"llm_model\"] = \"mistral\"\n</code></pre> <pre><code>qa_dataset_handler = QASetup(\n    config=config,\n    data_type=config[\"type_of_data\"],\n    client=client,\n)\n\nqa_dataset, _ = qa_dataset_handler.setup_vector_db_and_qa()\n</code></pre>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/change%20model/#tutorial-on-changing-models","title":"Tutorial on changing models","text":"<ul> <li>How would you use a different embedding and llm model?</li> </ul>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/change%20model/#initial-config","title":"Initial config","text":""},{"location":"Rag%20Pipeline/Developer%20Tutorials/change%20model/#embedding-model","title":"Embedding model","text":"<ul> <li>Pick a model from HF</li> </ul>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/change%20model/#llm-model","title":"LLM model","text":""},{"location":"Rag%20Pipeline/Developer%20Tutorials/change%20model/#important","title":"IMPORTANT","text":"<ul> <li>Do NOT forget to change the model to the best model in ollama/get_ollama.sh</li> </ul>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/create%20vectordb/","title":"Create vectordb","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n</code></pre> <pre><code>from backend.modules.utils import *\nfrom backend.modules.rag_llm import *\n</code></pre> <pre><code>config = load_config_and_device(\"../../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../data/doc_examples/chroma_db/\"\nconfig[\"data_dir\"] = \"../../data/doc_examples/\"\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = False\nconfig[\"testing_flag\"] = True  # set this to false while training, this is for demo\nconfig[\"test_subset\"] = True  # set this to false while training, this is for demo\n\n# load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\nprint(config)\n</code></pre> <pre><code>qa_dataset_handler = QASetup(\n    config=config,\n    data_type=\"dataset\",\n    client=client,\n)\n</code></pre> <pre><code>qa_dataset, _ = qa_dataset_handler.setup_vector_db_and_qa()\n</code></pre>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/create%20vectordb/#tutorial-on-creating-a-vector-database-with-openml-objects","title":"Tutorial on creating a vector database with openml objects","text":"<ul> <li>How would you use the API to create a vector database with openml objects (datasets, flows etc)</li> </ul>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/load%20vectordb%20and%20get%20results/","title":"Load vectordb and get results","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n</code></pre> <pre><code>from backend.modules.utils import *\nfrom backend.modules.rag_llm import *\nfrom backend.modules.results_gen import *\n</code></pre> <pre>\n<code>/Users/smukherjee/.pyenv/versions/3.10.14/envs/openml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</code>\n</pre> <pre><code>config = load_config_and_device(\"../../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../data/doc_examples/chroma_db/\"\nconfig[\"data_dir\"] = \"../../data/doc_examples/\"\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = False\nconfig[\"testing_flag\"] = True  # set this to false while training, this is for demo\nconfig[\"test_subset\"] = True  # set this to false while training, this is for demo\n# load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\nprint(config)\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: mps\n{'rqa_prompt_template': 'This database is a list of metadata. Use the following pieces of context to find the relevant document. Answer only from the context given using the {question} given. If you do not know the answer, say you do not know. {context}', 'llm_prompt_template': 'The following is a set of documents {docs}. Based on these docs, please summarize the content concisely. Also give a list of main concepts found in the documents. Do not add any new information. Helpful Answer: ', 'num_return_documents': 30, 'embedding_model': 'BAAI/bge-large-en-v1.5', 'llm_model': 'llama3', 'num_documents_for_llm': 30, 'data_dir': '../../data/doc_examples/', 'persist_dir': '../../data/doc_examples/chroma_db/', 'testing_flag': True, 'ignore_downloading_data': False, 'test_subset': True, 'data_download_n_jobs': 20, 'training': False, 'temperature': 0.95, 'top_p': 0.95, 'search_type': 'similarity', 'reranking': False, 'long_context_reorder': False, 'structure_query': False, 'use_chroma_for_saving_metadata': False, 'device': 'mps', 'type_of_data': 'dataset'}\n</code>\n</pre> <pre><code># Setup llm chain, initialize the retriever and llm, and setup Retrieval QA\nqa_dataset_handler = QASetup(\n    config=config,\n    data_type=config[\"type_of_data\"],\n    client=client,\n)\n\nqa_dataset, _ = qa_dataset_handler.setup_vector_db_and_qa()\n</code></pre> <pre>\n<code>[INFO] Loading metadata from file.\n[INFO] Loading model...\n[INFO] Model loaded.\n[INFO] Subsetting the data.\n[INFO] Generating unique documents. Total documents: 500\nNumber of unique documents: 0 vs Total documents: 500\nNo new documents to add.\n</code>\n</pre> <pre><code># get the llm chain and set the cache\nllm_chain_handler = LLMChainCreator(config=config, local=True)\nllm_chain_handler.enable_cache()\nllm_chain = llm_chain_handler.get_llm_chain()\n</code></pre> <pre><code>query = \"give me datasets about mushrooms\"\n</code></pre> <pre><code>res = qa_dataset.invoke(input=query, top_k=5)[:10]\nres\n</code></pre> <pre>\n<code>[Document(metadata={'MajorityClassSize': 4208.0, 'MaxNominalAttDistinctValues': 12.0, 'MinorityClassSize': 3916.0, 'NumberOfClasses': 2.0, 'NumberOfFeatures': 23.0, 'NumberOfInstances': 8124.0, 'NumberOfInstancesWithMissingValues': 2480.0, 'NumberOfMissingValues': 2480.0, 'NumberOfNumericFeatures': 0.0, 'NumberOfSymbolicFeatures': 23.0, 'Unnamed: 0': 19, 'description': \"**Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \\n**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n\\n### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\\n\\n### Attributes Information\\n```\\n1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s \\n2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s \\n3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y \\n4. bruises?: bruises=t,no=f \\n5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s \\n6. gill-attachment: attached=a,descending=d,free=f,notched=n \\n7. gill-spacing: close=c,crowded=w,distant=d \\n8. gill-size: broad=b,narrow=n \\n9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y \\n10. stalk-shape: enlarging=e,tapering=t \\n11. stalk-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=? \\n12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s \\n13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s \\n14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \\n15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \\n16. veil-type: partial=p,universal=u \\n17. veil-color: brown=n,orange=o,white=w,yellow=y \\n18. ring-number: none=n,one=o,two=t \\n19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z \\n20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y \\n21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y \\n22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d\\n```\\n\\n### Relevant papers\\n\\nSchlimmer,J.S. (1987). Concept Acquisition Through Representational Adjustment (Technical Report 87-19). Doctoral disseration, Department of Information and Computer Science, University of California, Irvine. \\n\\nIba,W., Wogulis,J., &amp; Langley,P. (1988). Trading off Simplicity and Coverage in Incremental Concept Learning. In Proceedings of the 5th International Conference on Machine Learning, 73-79. Ann Arbor, Michigan: Morgan Kaufmann. \\n\\nDuch W, Adamczak R, Grabczewski K (1996) Extraction of logical rules from training data using backpropagation networks, in: Proc. of the The 1st Online Workshop on Soft Computing, 19-30.Aug.1996, pp. 25-30, [Web Link] \\n\\nDuch W, Adamczak R, Grabczewski K, Ishikawa M, Ueda H, Extraction of crisp logical rules using constrained backpropagation networks - comparison of two new approaches, in: Proc. of the European Symposium on Artificial Neural Networks (ESANN'97), Bruge, Belgium 16-18.4.1997.\", 'did': 24, 'features': '0 : [0 - cap-shape (nominal)], 1 : [1 - cap-surface (nominal)], 2 : [2 - cap-color (nominal)], 3 : [3 - bruises%3F (nominal)], 4 : [4 - odor (nominal)], 5 : [5 - gill-attachment (nominal)], 6 : [6 - gill-spacing (nominal)], 7 : [7 - gill-size (nominal)], 8 : [8 - gill-color (nominal)], 9 : [9 - stalk-shape (nominal)], 10 : [10 - stalk-root (nominal)], 11 : [11 - stalk-surface-above-ring (nominal)], 12 : [12 - stalk-surface-below-ring (nominal)], 13 : [13 - stalk-color-above-ring (nominal)], 14 : [14 - stalk-color-below-ring (nominal)], 15 : [15 - veil-type (nominal)], 16 : [16 - veil-color (nominal)], 17 : [17 - ring-number (nominal)], 18 : [18 - ring-type (nominal)], 19 : [19 - spore-print-color (nominal)], 20 : [20 - population (nominal)], 21 : [21 - habitat (nominal)], 22 : [22 - class (nominal)],', 'format': 'ARFF', 'name': 'mushroom', 'qualities': 'AutoCorrelation : 0.726332635725717, CfsSubsetEval_DecisionStumpAUC : 0.9910519616800724, CfsSubsetEval_DecisionStumpErrRate : 0.013047759724273756, CfsSubsetEval_DecisionStumpKappa : 0.9738461616958994, CfsSubsetEval_NaiveBayesAUC : 0.9910519616800724, CfsSubsetEval_NaiveBayesErrRate : 0.013047759724273756, CfsSubsetEval_NaiveBayesKappa : 0.9738461616958994, CfsSubsetEval_kNN1NAUC : 0.9910519616800724, CfsSubsetEval_kNN1NErrRate : 0.013047759724273756, CfsSubsetEval_kNN1NKappa : 0.9738461616958994, ClassEntropy : 0.9990678968724604, DecisionStumpAUC : 0.8894935275772204, DecisionStumpErrRate : 0.11324470704086657, DecisionStumpKappa : 0.77457574608175, Dimensionality : 0.002831117676021664, EquivalentNumberOfAtts : 5.0393135801657, J48.00001.AUC : 1.0, J48.00001.ErrRate : 0.0, J48.00001.Kappa : 1.0, J48.0001.AUC : 1.0, J48.0001.ErrRate : 0.0, J48.0001.Kappa : 1.0, J48.001.AUC : 1.0, J48.001.ErrRate : 0.0, J48.001.Kappa : 1.0, MajorityClassPercentage : 51.7971442639094, MajorityClassSize : 4208.0, MaxAttributeEntropy : 3.030432883772633, MaxKurtosisOfNumericAtts : nan, MaxMeansOfNumericAtts : nan, MaxMutualInformation : 0.906074977384, MaxNominalAttDistinctValues : 12.0, MaxSkewnessOfNumericAtts : nan, MaxStdDevOfNumericAtts : nan, MeanAttributeEntropy : 1.4092554739602103, MeanKurtosisOfNumericAtts : nan, MeanMeansOfNumericAtts : nan, MeanMutualInformation : 0.19825475850613955, MeanNoiseToSignalRatio : 6.108305922031972, MeanNominalAttDistinctValues : 5.130434782608695, MeanSkewnessOfNumericAtts : nan, MeanStdDevOfNumericAtts : nan, MinAttributeEntropy : -0.0, MinKurtosisOfNumericAtts : nan, MinMeansOfNumericAtts : nan, MinMutualInformation : 0.0, MinNominalAttDistinctValues : 1.0, MinSkewnessOfNumericAtts : nan, MinStdDevOfNumericAtts : nan, MinorityClassPercentage : 48.20285573609059, MinorityClassSize : 3916.0, NaiveBayesAUC : 0.9976229672941662, NaiveBayesErrRate : 0.04899064500246184, NaiveBayesKappa : 0.9015972799616292, NumberOfBinaryFeatures : 5.0, NumberOfClasses : 2.0, NumberOfFeatures : 23.0, NumberOfInstances : 8124.0, NumberOfInstancesWithMissingValues : 2480.0, NumberOfMissingValues : 2480.0, NumberOfNumericFeatures : 0.0, NumberOfSymbolicFeatures : 23.0, PercentageOfBinaryFeatures : 21.73913043478261, PercentageOfInstancesWithMissingValues : 30.526834071885773, PercentageOfMissingValues : 1.3272536552993814, PercentageOfNumericFeatures : 0.0, PercentageOfSymbolicFeatures : 100.0, Quartile1AttributeEntropy : 0.8286618104993447, Quartile1KurtosisOfNumericAtts : nan, Quartile1MeansOfNumericAtts : nan, Quartile1MutualInformation : 0.034184520425602494, Quartile1SkewnessOfNumericAtts : nan, Quartile1StdDevOfNumericAtts : nan, Quartile2AttributeEntropy : 1.467128011861462, Quartile2KurtosisOfNumericAtts : nan, Quartile2MeansOfNumericAtts : nan, Quartile2MutualInformation : 0.174606545183155, Quartile2SkewnessOfNumericAtts : nan, Quartile2StdDevOfNumericAtts : nan, Quartile3AttributeEntropy : 2.0533554351937426, Quartile3KurtosisOfNumericAtts : nan, Quartile3MeansOfNumericAtts : nan, Quartile3MutualInformation : 0.27510225484918505, Quartile3SkewnessOfNumericAtts : nan, Quartile3StdDevOfNumericAtts : nan, REPTreeDepth1AUC : 0.9999987256143267, REPTreeDepth1ErrRate : 0.00036927621861152144, REPTreeDepth1Kappa : 0.9992605118549308, REPTreeDepth2AUC : 0.9999987256143267, REPTreeDepth2ErrRate : 0.00036927621861152144, REPTreeDepth2Kappa : 0.9992605118549308, REPTreeDepth3AUC : 0.9999987256143267, REPTreeDepth3ErrRate : 0.00036927621861152144, REPTreeDepth3Kappa : 0.9992605118549308, RandomTreeDepth1AUC : 0.9995247148288974, RandomTreeDepth1ErrRate : 0.0004923682914820286, RandomTreeDepth1Kappa : 0.9990140245420991, RandomTreeDepth2AUC : 0.9995247148288974, RandomTreeDepth2ErrRate : 0.0004923682914820286, RandomTreeDepth2Kappa : 0.9990140245420991, RandomTreeDepth3AUC : 0.9995247148288974, RandomTreeDepth3ErrRate : 0.0004923682914820286, RandomTreeDepth3Kappa : 0.9990140245420991, StdvNominalAttDistinctValues : 3.1809710899501766, kNN1NAUC : 1.0, kNN1NErrRate : 0.0, kNN1NKappa : 1.0,', 'status': 'active', 'uploader': 1, 'version': 1}, page_content=\"### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\"),\n Document(metadata={'MajorityClassSize': 4208.0, 'MaxNominalAttDistinctValues': 12.0, 'MinorityClassSize': 3916.0, 'NumberOfClasses': 2.0, 'NumberOfFeatures': 23.0, 'NumberOfInstances': 8124.0, 'NumberOfInstancesWithMissingValues': 2480.0, 'NumberOfMissingValues': 2480.0, 'NumberOfNumericFeatures': 0.0, 'NumberOfSymbolicFeatures': 23.0, 'Unnamed: 0': 19, 'description': \"**Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \\n**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n\\n### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\\n\\n### Attributes Information\\n```\\n1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s \\n2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s \\n3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y \\n4. bruises?: bruises=t,no=f \\n5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s \\n6. gill-attachment: attached=a,descending=d,free=f,notched=n \\n7. gill-spacing: close=c,crowded=w,distant=d \\n8. gill-size: broad=b,narrow=n \\n9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y \\n10. stalk-shape: enlarging=e,tapering=t \\n11. stalk-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=? \\n12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s \\n13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s \\n14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \\n15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \\n16. veil-type: partial=p,universal=u \\n17. veil-color: brown=n,orange=o,white=w,yellow=y \\n18. ring-number: none=n,one=o,two=t \\n19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z \\n20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y \\n21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y \\n22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d\\n```\\n\\n### Relevant papers\\n\\nSchlimmer,J.S. (1987). Concept Acquisition Through Representational Adjustment (Technical Report 87-19). Doctoral disseration, Department of Information and Computer Science, University of California, Irvine. \\n\\nIba,W., Wogulis,J., &amp; Langley,P. (1988). Trading off Simplicity and Coverage in Incremental Concept Learning. In Proceedings of the 5th International Conference on Machine Learning, 73-79. Ann Arbor, Michigan: Morgan Kaufmann. \\n\\nDuch W, Adamczak R, Grabczewski K (1996) Extraction of logical rules from training data using backpropagation networks, in: Proc. of the The 1st Online Workshop on Soft Computing, 19-30.Aug.1996, pp. 25-30, [Web Link] \\n\\nDuch W, Adamczak R, Grabczewski K, Ishikawa M, Ueda H, Extraction of crisp logical rules using constrained backpropagation networks - comparison of two new approaches, in: Proc. of the European Symposium on Artificial Neural Networks (ESANN'97), Bruge, Belgium 16-18.4.1997.\", 'did': 24, 'features': '0 : [0 - cap-shape (nominal)], 1 : [1 - cap-surface (nominal)], 2 : [2 - cap-color (nominal)], 3 : [3 - bruises%3F (nominal)], 4 : [4 - odor (nominal)], 5 : [5 - gill-attachment (nominal)], 6 : [6 - gill-spacing (nominal)], 7 : [7 - gill-size (nominal)], 8 : [8 - gill-color (nominal)], 9 : [9 - stalk-shape (nominal)], 10 : [10 - stalk-root (nominal)], 11 : [11 - stalk-surface-above-ring (nominal)], 12 : [12 - stalk-surface-below-ring (nominal)], 13 : [13 - stalk-color-above-ring (nominal)], 14 : [14 - stalk-color-below-ring (nominal)], 15 : [15 - veil-type (nominal)], 16 : [16 - veil-color (nominal)], 17 : [17 - ring-number (nominal)], 18 : [18 - ring-type (nominal)], 19 : [19 - spore-print-color (nominal)], 20 : [20 - population (nominal)], 21 : [21 - habitat (nominal)], 22 : [22 - class (nominal)],', 'format': 'ARFF', 'name': 'mushroom', 'qualities': 'AutoCorrelation : 0.726332635725717, CfsSubsetEval_DecisionStumpAUC : 0.9910519616800724, CfsSubsetEval_DecisionStumpErrRate : 0.013047759724273756, CfsSubsetEval_DecisionStumpKappa : 0.9738461616958994, CfsSubsetEval_NaiveBayesAUC : 0.9910519616800724, CfsSubsetEval_NaiveBayesErrRate : 0.013047759724273756, CfsSubsetEval_NaiveBayesKappa : 0.9738461616958994, CfsSubsetEval_kNN1NAUC : 0.9910519616800724, CfsSubsetEval_kNN1NErrRate : 0.013047759724273756, CfsSubsetEval_kNN1NKappa : 0.9738461616958994, ClassEntropy : 0.9990678968724604, DecisionStumpAUC : 0.8894935275772204, DecisionStumpErrRate : 0.11324470704086657, DecisionStumpKappa : 0.77457574608175, Dimensionality : 0.002831117676021664, EquivalentNumberOfAtts : 5.0393135801657, J48.00001.AUC : 1.0, J48.00001.ErrRate : 0.0, J48.00001.Kappa : 1.0, J48.0001.AUC : 1.0, J48.0001.ErrRate : 0.0, J48.0001.Kappa : 1.0, J48.001.AUC : 1.0, J48.001.ErrRate : 0.0, J48.001.Kappa : 1.0, MajorityClassPercentage : 51.7971442639094, MajorityClassSize : 4208.0, MaxAttributeEntropy : 3.030432883772633, MaxKurtosisOfNumericAtts : nan, MaxMeansOfNumericAtts : nan, MaxMutualInformation : 0.906074977384, MaxNominalAttDistinctValues : 12.0, MaxSkewnessOfNumericAtts : nan, MaxStdDevOfNumericAtts : nan, MeanAttributeEntropy : 1.4092554739602103, MeanKurtosisOfNumericAtts : nan, MeanMeansOfNumericAtts : nan, MeanMutualInformation : 0.19825475850613955, MeanNoiseToSignalRatio : 6.108305922031972, MeanNominalAttDistinctValues : 5.130434782608695, MeanSkewnessOfNumericAtts : nan, MeanStdDevOfNumericAtts : nan, MinAttributeEntropy : -0.0, MinKurtosisOfNumericAtts : nan, MinMeansOfNumericAtts : nan, MinMutualInformation : 0.0, MinNominalAttDistinctValues : 1.0, MinSkewnessOfNumericAtts : nan, MinStdDevOfNumericAtts : nan, MinorityClassPercentage : 48.20285573609059, MinorityClassSize : 3916.0, NaiveBayesAUC : 0.9976229672941662, NaiveBayesErrRate : 0.04899064500246184, NaiveBayesKappa : 0.9015972799616292, NumberOfBinaryFeatures : 5.0, NumberOfClasses : 2.0, NumberOfFeatures : 23.0, NumberOfInstances : 8124.0, NumberOfInstancesWithMissingValues : 2480.0, NumberOfMissingValues : 2480.0, NumberOfNumericFeatures : 0.0, NumberOfSymbolicFeatures : 23.0, PercentageOfBinaryFeatures : 21.73913043478261, PercentageOfInstancesWithMissingValues : 30.526834071885773, PercentageOfMissingValues : 1.3272536552993814, PercentageOfNumericFeatures : 0.0, PercentageOfSymbolicFeatures : 100.0, Quartile1AttributeEntropy : 0.8286618104993447, Quartile1KurtosisOfNumericAtts : nan, Quartile1MeansOfNumericAtts : nan, Quartile1MutualInformation : 0.034184520425602494, Quartile1SkewnessOfNumericAtts : nan, Quartile1StdDevOfNumericAtts : nan, Quartile2AttributeEntropy : 1.467128011861462, Quartile2KurtosisOfNumericAtts : nan, Quartile2MeansOfNumericAtts : nan, Quartile2MutualInformation : 0.174606545183155, Quartile2SkewnessOfNumericAtts : nan, Quartile2StdDevOfNumericAtts : nan, Quartile3AttributeEntropy : 2.0533554351937426, Quartile3KurtosisOfNumericAtts : nan, Quartile3MeansOfNumericAtts : nan, Quartile3MutualInformation : 0.27510225484918505, Quartile3SkewnessOfNumericAtts : nan, Quartile3StdDevOfNumericAtts : nan, REPTreeDepth1AUC : 0.9999987256143267, REPTreeDepth1ErrRate : 0.00036927621861152144, REPTreeDepth1Kappa : 0.9992605118549308, REPTreeDepth2AUC : 0.9999987256143267, REPTreeDepth2ErrRate : 0.00036927621861152144, REPTreeDepth2Kappa : 0.9992605118549308, REPTreeDepth3AUC : 0.9999987256143267, REPTreeDepth3ErrRate : 0.00036927621861152144, REPTreeDepth3Kappa : 0.9992605118549308, RandomTreeDepth1AUC : 0.9995247148288974, RandomTreeDepth1ErrRate : 0.0004923682914820286, RandomTreeDepth1Kappa : 0.9990140245420991, RandomTreeDepth2AUC : 0.9995247148288974, RandomTreeDepth2ErrRate : 0.0004923682914820286, RandomTreeDepth2Kappa : 0.9990140245420991, RandomTreeDepth3AUC : 0.9995247148288974, RandomTreeDepth3ErrRate : 0.0004923682914820286, RandomTreeDepth3Kappa : 0.9990140245420991, StdvNominalAttDistinctValues : 3.1809710899501766, kNN1NAUC : 1.0, kNN1NErrRate : 0.0, kNN1NKappa : 1.0,', 'status': 'active', 'uploader': 1, 'version': 1}, page_content='did - 24, name - mushroom, version - 1, uploader - 1, status - active, format - ARFF, MajorityClassSize - 4208.0, MaxNominalAttDistinctValues - 12.0, MinorityClassSize - 3916.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 8124.0, NumberOfInstancesWithMissingValues - 2480.0, NumberOfMissingValues - 2480.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - **Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \\n**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n\\n### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.'),\n Document(metadata={'NumberOfClasses': 0.0, 'NumberOfFeatures': 37.0, 'NumberOfInstances': 6435.0, 'NumberOfInstancesWithMissingValues': 0.0, 'NumberOfMissingValues': 0.0, 'NumberOfNumericFeatures': 37.0, 'NumberOfSymbolicFeatures': 0.0, 'Unnamed: 0': 203, 'description': \"**Author**:   \\n**Source**: Unknown - 1993  \\n**Please cite**:   \\n\\nSource:\\nAshwin Srinivasan\\nDepartment of Statistics and Data Modeling\\nUniversity of Strathclyde\\nGlasgow\\nScotland\\nUK\\nross '@' uk.ac.turing\\n\\nThe original Landsat data for this database was generated from data purchased from NASA by the Australian Centre for Remote Sensing, and used for research at: \\nThe Centre for Remote Sensing\\nUniversity of New South Wales\\nKensington, PO Box 1\\nNSW 2033\\nAustralia.\\n\\nThe sample database was generated taking a small section (82 rows and 100 columns) from the original data. The binary values were converted to their present ASCII form by Ashwin Srinivasan. The classification for each pixel was performed on the basis of an actual site visit by Ms. Karen Hall, when working for Professor John A. Richards, at the Centre for Remote Sensing at the University of New South Wales, Australia. Conversion to 3x3 neighbourhoods and splitting into test and training sets was done by Alistair Sutherland.\\n\\nData Set Information:\\nThe database consists of the multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number. The Landsat satellite data is one of the many sources of information available for a scene. The interpretation of a scene by  integrating spatial data of diverse types and resolutions including multispectral and radar data, maps indicating topography, land use etc. is expected to assume significant importance with the onset of an era characterised by integrative approaches to remote sensing (for example, NASA's Earth Observing System commencing this decade). Existing statistical methods are ill-equipped for handling such diverse data types. Note that this is not true for Landsat MSS data considered in isolation (as in this sample database). This data satisfies the important requirements of being numerical and at a single resolution, and standard maximum-likelihood classification performs very well. Consequently, for this data, it should be interesting to compare the performance of other methods against the statistical approach. One frame of Landsat MSS imagery consists of four digital images of the same scene in different spectral bands. Two of these are in the visible region (corresponding approximately to green and red regions of the visible spectrum) and two are in the (near) infra-red. Each pixel is a 8-bit binary word, with 0 corresponding to black and 255 to white. The spatial resolution of a pixel is about 80m x 80m. Each image contains 2340 x 3380 such pixels. The database is a (tiny) sub-area of a scene, consisting of 82 x 100 pixels. Each line of data corresponds to a 3x3 square neighbourhood of pixels completely contained within the 82x100 sub-area. Each line contains the pixel values in the four spectral bands (converted to ASCII) of each of the 9 pixels in the 3x3 neighbourhood and a number indicating the classification label of the central pixel. The number is a code for the following classes:\\n\\nNumber Class\\n1 red soil\\n2 cotton crop\\n3 grey soil\\n4 damp grey soil\\n5 soil with vegetation stubble\\n6 mixture class (all types present)\\n7 very damp grey soil\\nNB. There are no examples with class 6 in this dataset.\\n \\nThe data is given in random order and certain lines of data have been removed so you cannot reconstruct the original image from this dataset. In each line of data the four spectral values for the top-left pixel are given first followed by the four spectral values for the top-middle pixel and then those for the top-right pixel, and so on with the pixels read out in sequence left-to-right and top-to-bottom. Thus, the four spectral values for the central pixel are given by attributes 17,18,19 and 20. If you like you can use only these four attributes, while ignoring the others. This avoids the problem which arises when a 3x3 neighbourhood straddles a boundary.\\n\\nAttribute Information:\\nThe attributes are numerical, in the range 0 to 255.\\n\\nUCI: http://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)\", 'did': 294, 'features': '0 : [0 - attr1 (numeric)], 1 : [1 - attr2 (numeric)], 2 : [2 - attr3 (numeric)], 3 : [3 - attr4 (numeric)], 4 : [4 - attr5 (numeric)], 5 : [5 - attr6 (numeric)], 6 : [6 - attr7 (numeric)], 7 : [7 - attr8 (numeric)], 8 : [8 - attr9 (numeric)], 9 : [9 - attr10 (numeric)], 10 : [10 - attr11 (numeric)], 11 : [11 - attr12 (numeric)], 12 : [12 - attr13 (numeric)], 13 : [13 - attr14 (numeric)], 14 : [14 - attr15 (numeric)], 15 : [15 - attr16 (numeric)], 16 : [16 - attr17 (numeric)], 17 : [17 - attr18 (numeric)], 18 : [18 - attr19 (numeric)], 19 : [19 - attr20 (numeric)], 20 : [20 - attr21 (numeric)], 21 : [21 - attr22 (numeric)], 22 : [22 - attr23 (numeric)], 23 : [23 - attr24 (numeric)], 24 : [24 - attr25 (numeric)], 25 : [25 - attr26 (numeric)], 26 : [26 - attr27 (numeric)], 27 : [27 - attr28 (numeric)], 28 : [28 - attr29 (numeric)], 29 : [29 - attr30 (numeric)], 30 : [30 - attr31 (numeric)], 31 : [31 - attr32 (numeric)], 32 : [32 - attr33 (numeric)], 33 : [33 - attr34 (numeric)], 34 : [34 - attr35 (numeric)], 35 : [35 - attr36 (numeric)], 36 : [36 - class (numeric)],', 'format': 'ARFF', 'name': 'satellite_image', 'qualities': 'AutoCorrelation : 0.5853279452906435, CfsSubsetEval_DecisionStumpAUC : nan, CfsSubsetEval_DecisionStumpErrRate : nan, CfsSubsetEval_DecisionStumpKappa : nan, CfsSubsetEval_NaiveBayesAUC : nan, CfsSubsetEval_NaiveBayesErrRate : nan, CfsSubsetEval_NaiveBayesKappa : nan, CfsSubsetEval_kNN1NAUC : nan, CfsSubsetEval_kNN1NErrRate : nan, CfsSubsetEval_kNN1NKappa : nan, ClassEntropy : nan, DecisionStumpAUC : nan, DecisionStumpErrRate : nan, DecisionStumpKappa : nan, Dimensionality : 0.00574980574980575, EquivalentNumberOfAtts : nan, J48.00001.AUC : nan, J48.00001.ErrRate : nan, J48.00001.Kappa : nan, J48.0001.AUC : nan, J48.0001.ErrRate : nan, J48.0001.Kappa : nan, J48.001.AUC : nan, J48.001.ErrRate : nan, J48.001.Kappa : nan, MajorityClassPercentage : nan, MajorityClassSize : nan, MaxAttributeEntropy : nan, MaxKurtosisOfNumericAtts : 1.2773432544146832, MaxMeansOfNumericAtts : 99.31126651126642, MaxMutualInformation : nan, MaxNominalAttDistinctValues : nan, MaxSkewnessOfNumericAtts : 0.9187090836988436, MaxStdDevOfNumericAtts : 22.90506492772991, MeanAttributeEntropy : nan, MeanKurtosisOfNumericAtts : -0.18345361023395665, MeanMeansOfNumericAtts : 81.3149961149961, MeanMutualInformation : nan, MeanNoiseToSignalRatio : nan, MeanNominalAttDistinctValues : nan, MeanSkewnessOfNumericAtts : 0.04831449741968043, MeanStdDevOfNumericAtts : 17.586070075450067, MinAttributeEntropy : nan, MinKurtosisOfNumericAtts : -1.2441720904806828, MinMeansOfNumericAtts : 3.6686868686868834, MinMutualInformation : nan, MinNominalAttDistinctValues : nan, MinSkewnessOfNumericAtts : -0.6747275074215006, MinStdDevOfNumericAtts : 2.214052121287819, MinorityClassPercentage : nan, MinorityClassSize : nan, NaiveBayesAUC : nan, NaiveBayesErrRate : nan, NaiveBayesKappa : nan, NumberOfBinaryFeatures : 0.0, NumberOfClasses : 0.0, NumberOfFeatures : 37.0, NumberOfInstances : 6435.0, NumberOfInstancesWithMissingValues : 0.0, NumberOfMissingValues : 0.0, NumberOfNumericFeatures : 37.0, NumberOfSymbolicFeatures : 0.0, PercentageOfBinaryFeatures : 0.0, PercentageOfInstancesWithMissingValues : 0.0, PercentageOfMissingValues : 0.0, PercentageOfNumericFeatures : 100.0, PercentageOfSymbolicFeatures : 0.0, Quartile1AttributeEntropy : nan, Quartile1KurtosisOfNumericAtts : -0.8829551820521702, Quartile1MeansOfNumericAtts : 69.34483294483297, Quartile1MutualInformation : nan, Quartile1SkewnessOfNumericAtts : -0.3859749826493584, Quartile1StdDevOfNumericAtts : 13.604282494809674, Quartile2AttributeEntropy : nan, Quartile2KurtosisOfNumericAtts : -0.6732423440004554, Quartile2MeansOfNumericAtts : 82.66060606060603, Quartile2MutualInformation : nan, Quartile2SkewnessOfNumericAtts : 0.02239958092752799, Quartile2StdDevOfNumericAtts : 16.729622667298376, Quartile3AttributeEntropy : nan, Quartile3KurtosisOfNumericAtts : 0.5035049254688353, Quartile3MeansOfNumericAtts : 91.22408702408694, Quartile3MutualInformation : nan, Quartile3SkewnessOfNumericAtts : 0.6162940189640502, Quartile3StdDevOfNumericAtts : 20.936744304390697, REPTreeDepth1AUC : nan, REPTreeDepth1ErrRate : nan, REPTreeDepth1Kappa : nan, REPTreeDepth2AUC : nan, REPTreeDepth2ErrRate : nan, REPTreeDepth2Kappa : nan, REPTreeDepth3AUC : nan, REPTreeDepth3ErrRate : nan, REPTreeDepth3Kappa : nan, RandomTreeDepth1AUC : nan, RandomTreeDepth1ErrRate : nan, RandomTreeDepth1Kappa : nan, RandomTreeDepth2AUC : nan, RandomTreeDepth2ErrRate : nan, RandomTreeDepth2Kappa : nan, RandomTreeDepth3AUC : nan, RandomTreeDepth3ErrRate : nan, RandomTreeDepth3Kappa : nan, StdvNominalAttDistinctValues : nan, kNN1NAUC : nan, kNN1NErrRate : nan, kNN1NKappa : nan,', 'status': 'active', 'uploader': 94, 'version': 1}, page_content='Data Set Information:'),\n Document(metadata={'MajorityClassSize': 518298.0, 'MaxNominalAttDistinctValues': 12.0, 'MinorityClassSize': 481702.0, 'NumberOfClasses': 2.0, 'NumberOfFeatures': 23.0, 'NumberOfInstances': 1000000.0, 'NumberOfInstancesWithMissingValues': 0.0, 'NumberOfMissingValues': 0.0, 'NumberOfNumericFeatures': 0.0, 'NumberOfSymbolicFeatures': 23.0, 'Unnamed: 0': 68, 'did': 120, 'features': '0 : [0 - cap-shape (nominal)], 1 : [1 - cap-surface (nominal)], 2 : [2 - cap-color (nominal)], 3 : [3 - bruises%3F (nominal)], 4 : [4 - odor (nominal)], 5 : [5 - gill-attachment (nominal)], 6 : [6 - gill-spacing (nominal)], 7 : [7 - gill-size (nominal)], 8 : [8 - gill-color (nominal)], 9 : [9 - stalk-shape (nominal)], 10 : [10 - stalk-root (nominal)], 11 : [11 - stalk-surface-above-ring (nominal)], 12 : [12 - stalk-surface-below-ring (nominal)], 13 : [13 - stalk-color-above-ring (nominal)], 14 : [14 - stalk-color-below-ring (nominal)], 15 : [15 - veil-type (nominal)], 16 : [16 - veil-color (nominal)], 17 : [17 - ring-number (nominal)], 18 : [18 - ring-type (nominal)], 19 : [19 - spore-print-color (nominal)], 20 : [20 - population (nominal)], 21 : [21 - habitat (nominal)], 22 : [22 - class (nominal)],', 'format': 'ARFF', 'name': 'BNG(mushroom)', 'qualities': 'AutoCorrelation : 0.5011905011905012, CfsSubsetEval_DecisionStumpAUC : 0.9847860299226502, CfsSubsetEval_DecisionStumpErrRate : 0.021824, CfsSubsetEval_DecisionStumpKappa : 0.9562780842181652, CfsSubsetEval_NaiveBayesAUC : 0.9847860299226502, CfsSubsetEval_NaiveBayesErrRate : 0.021824, CfsSubsetEval_NaiveBayesKappa : 0.9562780842181652, CfsSubsetEval_kNN1NAUC : 0.9847860299226502, CfsSubsetEval_kNN1NErrRate : 0.021824, CfsSubsetEval_kNN1NKappa : 0.9562780842181652, ClassEntropy : 0.9990337071596953, DecisionStumpAUC : 0.8815512935166292, DecisionStumpErrRate : 0.121245, DecisionStumpKappa : 0.7587911383829151, Dimensionality : 2.3e-05, EquivalentNumberOfAtts : 6.097271107545528, J48.00001.AUC : 0.9962742048687271, J48.00001.ErrRate : 0.007847, J48.00001.Kappa : 0.9842850236101645, J48.0001.AUC : 0.9962742048687271, J48.0001.ErrRate : 0.007847, J48.0001.Kappa : 0.9842850236101645, J48.001.AUC : 0.9962742048687271, J48.001.ErrRate : 0.007847, J48.001.Kappa : 0.9842850236101645, MajorityClassPercentage : 51.829800000000006, MajorityClassSize : 518298.0, MaxAttributeEntropy : 3.0845637992777144, MaxKurtosisOfNumericAtts : nan, MaxMeansOfNumericAtts : nan, MaxMutualInformation : 0.84128137803192, MaxNominalAttDistinctValues : 12.0, MaxSkewnessOfNumericAtts : nan, MaxStdDevOfNumericAtts : nan, MeanAttributeEntropy : 1.5385002552906082, MeanKurtosisOfNumericAtts : nan, MeanMeansOfNumericAtts : nan, MeanMutualInformation : 0.16384931710242728, MeanNoiseToSignalRatio : 8.389726380909137, MeanNominalAttDistinctValues : 5.521739130434782, MeanSkewnessOfNumericAtts : nan, MeanStdDevOfNumericAtts : nan, MinAttributeEntropy : 0.0016183542115170931, MinKurtosisOfNumericAtts : nan, MinMeansOfNumericAtts : nan, MinMutualInformation : 1.10978079e-06, MinNominalAttDistinctValues : 2.0, MinSkewnessOfNumericAtts : nan, MinStdDevOfNumericAtts : nan, MinorityClassPercentage : 48.1702, MinorityClassSize : 481702.0, NaiveBayesAUC : 0.989456054908011, NaiveBayesErrRate : 0.072603, NaiveBayesKappa : 0.8540047016650592, NumberOfBinaryFeatures : 5.0, NumberOfClasses : 2.0, NumberOfFeatures : 23.0, NumberOfInstances : 1000000.0, NumberOfInstancesWithMissingValues : 0.0, NumberOfMissingValues : 0.0, NumberOfNumericFeatures : 0.0, NumberOfSymbolicFeatures : 23.0, PercentageOfBinaryFeatures : 21.73913043478261, PercentageOfInstancesWithMissingValues : 0.0, PercentageOfMissingValues : 0.0, PercentageOfNumericFeatures : 0.0, PercentageOfSymbolicFeatures : 100.0, Quartile1AttributeEntropy : 0.8684476271925594, Quartile1KurtosisOfNumericAtts : nan, Quartile1MeansOfNumericAtts : nan, Quartile1MutualInformation : 0.0261470876211225, Quartile1SkewnessOfNumericAtts : nan, Quartile1StdDevOfNumericAtts : nan, Quartile2AttributeEntropy : 1.5540739508863595, Quartile2KurtosisOfNumericAtts : nan, Quartile2MeansOfNumericAtts : nan, Quartile2MutualInformation : 0.13087075005855, Quartile2SkewnessOfNumericAtts : nan, Quartile2StdDevOfNumericAtts : nan, Quartile3AttributeEntropy : 2.281038681528015, Quartile3KurtosisOfNumericAtts : nan, Quartile3MeansOfNumericAtts : nan, Quartile3MutualInformation : 0.2240629781340025, Quartile3SkewnessOfNumericAtts : nan, Quartile3StdDevOfNumericAtts : nan, REPTreeDepth1AUC : 0.9971920115811678, REPTreeDepth1ErrRate : 0.01052, REPTreeDepth1Kappa : 0.9789309934238616, REPTreeDepth2AUC : 0.9971920115811678, REPTreeDepth2ErrRate : 0.01052, REPTreeDepth2Kappa : 0.9789309934238616, REPTreeDepth3AUC : 0.9971920115811678, REPTreeDepth3ErrRate : 0.01052, REPTreeDepth3Kappa : 0.9789309934238616, RandomTreeDepth1AUC : 0.9815888004820784, RandomTreeDepth1ErrRate : 0.024243, RandomTreeDepth1Kappa : 0.9514421122524949, RandomTreeDepth2AUC : 0.9815888004820784, RandomTreeDepth2ErrRate : 0.024243, RandomTreeDepth2Kappa : 0.9514421122524949, RandomTreeDepth3AUC : 0.9815888004820784, RandomTreeDepth3ErrRate : 0.024243, RandomTreeDepth3Kappa : 0.9514421122524949, StdvNominalAttDistinctValues : 3.0580677978302706, kNN1NAUC : 0.9989058041456409, kNN1NErrRate : 0.011358, kNN1NKappa : 0.977249584712958,', 'status': 'active', 'uploader': 1, 'version': 1}, page_content='did - 120, name - BNG(mushroom), version - 1, uploader - 1, status - active, format - ARFF, MajorityClassSize - 518298.0, MaxNominalAttDistinctValues - 12.0, MinorityClassSize - 481702.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 1000000.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - None, qualities - AutoCorrelation : 0.5011905011905012, CfsSubsetEval_DecisionStumpAUC : 0.9847860299226502, CfsSubsetEval_DecisionStumpErrRate : 0.021824, CfsSubsetEval_DecisionStumpKappa : 0.9562780842181652, CfsSubsetEval_NaiveBayesAUC : 0.9847860299226502, CfsSubsetEval_NaiveBayesErrRate : 0.021824, CfsSubsetEval_NaiveBayesKappa : 0.9562780842181652, CfsSubsetEval_kNN1NAUC : 0.9847860299226502, CfsSubsetEval_kNN1NErrRate : 0.021824, CfsSubsetEval_kNN1NKappa : 0.9562780842181652, ClassEntropy : 0.9990337071596953, DecisionStumpAUC : 0.8815512935166292, DecisionStumpErrRate :'),\n Document(metadata={'MajorityClassSize': 518298.0, 'MaxNominalAttDistinctValues': 12.0, 'MinorityClassSize': 481702.0, 'NumberOfClasses': 2.0, 'NumberOfFeatures': 23.0, 'NumberOfInstances': 1000000.0, 'NumberOfInstancesWithMissingValues': 0.0, 'NumberOfMissingValues': 0.0, 'NumberOfNumericFeatures': 0.0, 'NumberOfSymbolicFeatures': 23.0, 'Unnamed: 0': 68, 'did': 120, 'features': '0 : [0 - cap-shape (nominal)], 1 : [1 - cap-surface (nominal)], 2 : [2 - cap-color (nominal)], 3 : [3 - bruises%3F (nominal)], 4 : [4 - odor (nominal)], 5 : [5 - gill-attachment (nominal)], 6 : [6 - gill-spacing (nominal)], 7 : [7 - gill-size (nominal)], 8 : [8 - gill-color (nominal)], 9 : [9 - stalk-shape (nominal)], 10 : [10 - stalk-root (nominal)], 11 : [11 - stalk-surface-above-ring (nominal)], 12 : [12 - stalk-surface-below-ring (nominal)], 13 : [13 - stalk-color-above-ring (nominal)], 14 : [14 - stalk-color-below-ring (nominal)], 15 : [15 - veil-type (nominal)], 16 : [16 - veil-color (nominal)], 17 : [17 - ring-number (nominal)], 18 : [18 - ring-type (nominal)], 19 : [19 - spore-print-color (nominal)], 20 : [20 - population (nominal)], 21 : [21 - habitat (nominal)], 22 : [22 - class (nominal)],', 'format': 'ARFF', 'name': 'BNG(mushroom)', 'qualities': 'AutoCorrelation : 0.5011905011905012, CfsSubsetEval_DecisionStumpAUC : 0.9847860299226502, CfsSubsetEval_DecisionStumpErrRate : 0.021824, CfsSubsetEval_DecisionStumpKappa : 0.9562780842181652, CfsSubsetEval_NaiveBayesAUC : 0.9847860299226502, CfsSubsetEval_NaiveBayesErrRate : 0.021824, CfsSubsetEval_NaiveBayesKappa : 0.9562780842181652, CfsSubsetEval_kNN1NAUC : 0.9847860299226502, CfsSubsetEval_kNN1NErrRate : 0.021824, CfsSubsetEval_kNN1NKappa : 0.9562780842181652, ClassEntropy : 0.9990337071596953, DecisionStumpAUC : 0.8815512935166292, DecisionStumpErrRate : 0.121245, DecisionStumpKappa : 0.7587911383829151, Dimensionality : 2.3e-05, EquivalentNumberOfAtts : 6.097271107545528, J48.00001.AUC : 0.9962742048687271, J48.00001.ErrRate : 0.007847, J48.00001.Kappa : 0.9842850236101645, J48.0001.AUC : 0.9962742048687271, J48.0001.ErrRate : 0.007847, J48.0001.Kappa : 0.9842850236101645, J48.001.AUC : 0.9962742048687271, J48.001.ErrRate : 0.007847, J48.001.Kappa : 0.9842850236101645, MajorityClassPercentage : 51.829800000000006, MajorityClassSize : 518298.0, MaxAttributeEntropy : 3.0845637992777144, MaxKurtosisOfNumericAtts : nan, MaxMeansOfNumericAtts : nan, MaxMutualInformation : 0.84128137803192, MaxNominalAttDistinctValues : 12.0, MaxSkewnessOfNumericAtts : nan, MaxStdDevOfNumericAtts : nan, MeanAttributeEntropy : 1.5385002552906082, MeanKurtosisOfNumericAtts : nan, MeanMeansOfNumericAtts : nan, MeanMutualInformation : 0.16384931710242728, MeanNoiseToSignalRatio : 8.389726380909137, MeanNominalAttDistinctValues : 5.521739130434782, MeanSkewnessOfNumericAtts : nan, MeanStdDevOfNumericAtts : nan, MinAttributeEntropy : 0.0016183542115170931, MinKurtosisOfNumericAtts : nan, MinMeansOfNumericAtts : nan, MinMutualInformation : 1.10978079e-06, MinNominalAttDistinctValues : 2.0, MinSkewnessOfNumericAtts : nan, MinStdDevOfNumericAtts : nan, MinorityClassPercentage : 48.1702, MinorityClassSize : 481702.0, NaiveBayesAUC : 0.989456054908011, NaiveBayesErrRate : 0.072603, NaiveBayesKappa : 0.8540047016650592, NumberOfBinaryFeatures : 5.0, NumberOfClasses : 2.0, NumberOfFeatures : 23.0, NumberOfInstances : 1000000.0, NumberOfInstancesWithMissingValues : 0.0, NumberOfMissingValues : 0.0, NumberOfNumericFeatures : 0.0, NumberOfSymbolicFeatures : 23.0, PercentageOfBinaryFeatures : 21.73913043478261, PercentageOfInstancesWithMissingValues : 0.0, PercentageOfMissingValues : 0.0, PercentageOfNumericFeatures : 0.0, PercentageOfSymbolicFeatures : 100.0, Quartile1AttributeEntropy : 0.8684476271925594, Quartile1KurtosisOfNumericAtts : nan, Quartile1MeansOfNumericAtts : nan, Quartile1MutualInformation : 0.0261470876211225, Quartile1SkewnessOfNumericAtts : nan, Quartile1StdDevOfNumericAtts : nan, Quartile2AttributeEntropy : 1.5540739508863595, Quartile2KurtosisOfNumericAtts : nan, Quartile2MeansOfNumericAtts : nan, Quartile2MutualInformation : 0.13087075005855, Quartile2SkewnessOfNumericAtts : nan, Quartile2StdDevOfNumericAtts : nan, Quartile3AttributeEntropy : 2.281038681528015, Quartile3KurtosisOfNumericAtts : nan, Quartile3MeansOfNumericAtts : nan, Quartile3MutualInformation : 0.2240629781340025, Quartile3SkewnessOfNumericAtts : nan, Quartile3StdDevOfNumericAtts : nan, REPTreeDepth1AUC : 0.9971920115811678, REPTreeDepth1ErrRate : 0.01052, REPTreeDepth1Kappa : 0.9789309934238616, REPTreeDepth2AUC : 0.9971920115811678, REPTreeDepth2ErrRate : 0.01052, REPTreeDepth2Kappa : 0.9789309934238616, REPTreeDepth3AUC : 0.9971920115811678, REPTreeDepth3ErrRate : 0.01052, REPTreeDepth3Kappa : 0.9789309934238616, RandomTreeDepth1AUC : 0.9815888004820784, RandomTreeDepth1ErrRate : 0.024243, RandomTreeDepth1Kappa : 0.9514421122524949, RandomTreeDepth2AUC : 0.9815888004820784, RandomTreeDepth2ErrRate : 0.024243, RandomTreeDepth2Kappa : 0.9514421122524949, RandomTreeDepth3AUC : 0.9815888004820784, RandomTreeDepth3ErrRate : 0.024243, RandomTreeDepth3Kappa : 0.9514421122524949, StdvNominalAttDistinctValues : 3.0580677978302706, kNN1NAUC : 0.9989058041456409, kNN1NErrRate : 0.011358, kNN1NKappa : 0.977249584712958,', 'status': 'active', 'uploader': 1, 'version': 1}, page_content='RandomTreeDepth3ErrRate : 0.024243, RandomTreeDepth3Kappa : 0.9514421122524949, StdvNominalAttDistinctValues : 3.0580677978302706, kNN1NAUC : 0.9989058041456409, kNN1NErrRate : 0.011358, kNN1NKappa : 0.977249584712958,, features - 0 : [0 - cap-shape (nominal)], 1 : [1 - cap-surface (nominal)], 2 : [2 - cap-color (nominal)], 3 : [3 - bruises%3F (nominal)], 4 : [4 - odor (nominal)], 5 : [5 - gill-attachment (nominal)], 6 : [6 - gill-spacing (nominal)], 7 : [7 - gill-size (nominal)], 8 : [8 - gill-color (nominal)], 9 : [9 - stalk-shape (nominal)], 10 : [10 - stalk-root (nominal)], 11 : [11 - stalk-surface-above-ring (nominal)], 12 : [12 - stalk-surface-below-ring (nominal)], 13 : [13 - stalk-color-above-ring (nominal)], 14 : [14 - stalk-color-below-ring (nominal)], 15 : [15 - veil-type (nominal)], 16 : [16 - veil-color (nominal)], 17 : [17 - ring-number (nominal)], 18 : [18 - ring-type (nominal)], 19 : [19 - spore-print-color (nominal)], 20 : [20 - population (nominal)], 21 : [21 -'),\n Document(metadata={'MajorityClassSize': 92.0, 'MaxNominalAttDistinctValues': 19.0, 'MinorityClassSize': 8.0, 'NumberOfClasses': 19.0, 'NumberOfFeatures': 36.0, 'NumberOfInstances': 683.0, 'NumberOfInstancesWithMissingValues': 121.0, 'NumberOfMissingValues': 2337.0, 'NumberOfNumericFeatures': 0.0, 'NumberOfSymbolicFeatures': 36.0, 'Unnamed: 0': 36, 'description': '**Author**: R.S. Michalski and R.L. Chilausky (Donors: Ming Tan &amp; Jeff Schlimmer)  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Soybean+(Large)) - 1988  \\n**Please cite**: R.S. Michalski and R.L. Chilausky \"Learning by Being Told and Learning from Examples: An Experimental Comparison of the Two Methods of Knowledge Acquisition in the Context of Developing an Expert System for Soybean Disease Diagnosis\", International Journal of Policy Analysis and Information Systems, Vol. 4, No. 2, 1980.  \\n\\n**Large Soybean Database**  \\nThis is the large soybean database from the UCI repository, with its training and test database combined into a single file. \\n\\nThere are 19 classes, only the first 15 of which have been used in prior work. The folklore seems to be that the last four classes are unjustified by the data since they have so few examples. There are 35 categorical attributes, some nominal and some ordered. The value \\'dna\\' means does not apply. The values for attributes are encoded numerically, with the first value encoded as \"0,\\'\\' the second as \"1,\\'\\' and so forth. An unknown value is encoded as \"?\\'\\'.\\n\\n### Attribute Information\\n\\n1. date: april,may,june,july,august,september,october,?. \\n2. plant-stand: normal,lt-normal,?. \\n3. precip: lt-norm,norm,gt-norm,?. \\n4. temp: lt-norm,norm,gt-norm,?. \\n5. hail: yes,no,?. \\n6. crop-hist: diff-lst-year,same-lst-yr,same-lst-two-yrs, \\nsame-lst-sev-yrs,?. \\n7. area-damaged: scattered,low-areas,upper-areas,whole-field,?. \\n8. severity: minor,pot-severe,severe,?. \\n9. seed-tmt: none,fungicide,other,?. \\n10. germination: 90-100%,80-89%,lt-80%,?. \\n11. plant-growth: norm,abnorm,?. \\n12. leaves: norm,abnorm. \\n13. leafspots-halo: absent,yellow-halos,no-yellow-halos,?. \\n14. leafspots-marg: w-s-marg,no-w-s-marg,dna,?. \\n15. leafspot-size: lt-1/8,gt-1/8,dna,?. \\n16. leaf-shread: absent,present,?. \\n17. leaf-malf: absent,present,?. \\n18. leaf-mild: absent,upper-surf,lower-surf,?. \\n19. stem: norm,abnorm,?. \\n20. lodging: yes,no,?. \\n21. stem-cankers: absent,below-soil,above-soil,above-sec-nde,?. \\n22. canker-lesion: dna,brown,dk-brown-blk,tan,?. \\n23. fruiting-bodies: absent,present,?. \\n24. external decay: absent,firm-and-dry,watery,?. \\n25. mycelium: absent,present,?. \\n26. int-discolor: none,brown,black,?. \\n27. sclerotia: absent,present,?. \\n28. fruit-pods: norm,diseased,few-present,dna,?. \\n29. fruit spots: absent,colored,brown-w/blk-specks,distort,dna,?. \\n30. seed: norm,abnorm,?. \\n31. mold-growth: absent,present,?. \\n32. seed-discolor: absent,present,?. \\n33. seed-size: norm,lt-norm,?. \\n34. shriveling: absent,present,?. \\n35. roots: norm,rotted,galls-cysts,?.\\n\\n### Classes \\n\\n-- 19 Classes = {diaporthe-stem-canker, charcoal-rot, rhizoctonia-root-rot, phytophthora-rot, brown-stem-rot, powdery-mildew, downy-mildew, brown-spot, bacterial-blight, bacterial-pustule, purple-seed-stain, anthracnose, phyllosticta-leaf-spot, alternarialeaf-spot, frog-eye-leaf-spot, diaporthe-pod-&amp;-stem-blight, cyst-nematode, 2-4-d-injury, herbicide-injury} \\n\\n### Revelant papers\\n\\nTan, M., &amp; Eshelman, L. (1988). Using weighted networks to represent classification knowledge in noisy domains. Proceedings of the Fifth International Conference on Machine Learning (pp. 121-134). Ann Arbor, Michigan: Morgan Kaufmann. \\n\\nFisher,D.H. &amp; Schlimmer,J.C. (1988). Concept Simplification and Predictive Accuracy. Proceedings of the Fifth International Conference on Machine Learning (pp. 22-28). Ann Arbor, Michigan: Morgan Kaufmann.', 'did': 42, 'features': '0 : [0 - date (nominal)], 1 : [1 - plant-stand (nominal)], 2 : [2 - precip (nominal)], 3 : [3 - temp (nominal)], 4 : [4 - hail (nominal)], 5 : [5 - crop-hist (nominal)], 6 : [6 - area-damaged (nominal)], 7 : [7 - severity (nominal)], 8 : [8 - seed-tmt (nominal)], 9 : [9 - germination (nominal)], 10 : [10 - plant-growth (nominal)], 11 : [11 - leaves (nominal)], 12 : [12 - leafspots-halo (nominal)], 13 : [13 - leafspots-marg (nominal)], 14 : [14 - leafspot-size (nominal)], 15 : [15 - leaf-shread (nominal)], 16 : [16 - leaf-malf (nominal)], 17 : [17 - leaf-mild (nominal)], 18 : [18 - stem (nominal)], 19 : [19 - lodging (nominal)], 20 : [20 - stem-cankers (nominal)], 21 : [21 - canker-lesion (nominal)], 22 : [22 - fruiting-bodies (nominal)], 23 : [23 - external-decay (nominal)], 24 : [24 - mycelium (nominal)], 25 : [25 - int-discolor (nominal)], 26 : [26 - sclerotia (nominal)], 27 : [27 - fruit-pods (nominal)], 28 : [28 - fruit-spots (nominal)], 29 : [29 - seed (nominal)], 30 : [30 - mold-growth (nominal)], 31 : [31 - seed-discolor (nominal)], 32 : [32 - seed-size (nominal)], 33 : [33 - shriveling (nominal)], 34 : [34 - roots (nominal)], 35 : [35 - class (nominal)],', 'format': 'ARFF', 'name': 'soybean', 'qualities': 'AutoCorrelation : 0.9457478005865103, CfsSubsetEval_DecisionStumpAUC : 0.9620422408823379, CfsSubsetEval_DecisionStumpErrRate : 0.13323572474377746, CfsSubsetEval_DecisionStumpKappa : 0.8534752853145238, CfsSubsetEval_NaiveBayesAUC : 0.9620422408823379, CfsSubsetEval_NaiveBayesErrRate : 0.13323572474377746, CfsSubsetEval_NaiveBayesKappa : 0.8534752853145238, CfsSubsetEval_kNN1NAUC : 0.9620422408823379, CfsSubsetEval_kNN1NErrRate : 0.13323572474377746, CfsSubsetEval_kNN1NKappa : 0.8534752853145238, ClassEntropy : 3.83550798457672, DecisionStumpAUC : 0.8099631489104341, DecisionStumpErrRate : 0.7203513909224012, DecisionStumpKappa : 0.19424522533539545, Dimensionality : 0.0527086383601757, EquivalentNumberOfAtts : 7.508591767241043, J48.00001.AUC : 0.9739047068470593, J48.00001.ErrRate : 0.12152269399707175, J48.00001.Kappa : 0.8663370421980624, J48.0001.AUC : 0.9739047068470593, J48.0001.ErrRate : 0.12152269399707175, J48.0001.Kappa : 0.8663370421980624, J48.001.AUC : 0.9739047068470593, J48.001.ErrRate : 0.12152269399707175, J48.001.Kappa : 0.8663370421980624, MajorityClassPercentage : 13.469985358711567, MajorityClassSize : 92.0, MaxAttributeEntropy : 2.6849389644492594, MaxKurtosisOfNumericAtts : nan, MaxMeansOfNumericAtts : nan, MaxMutualInformation : 1.28692474762189, MaxNominalAttDistinctValues : 19.0, MaxSkewnessOfNumericAtts : nan, MaxStdDevOfNumericAtts : nan, MeanAttributeEntropy : 0.9655890619117928, MeanKurtosisOfNumericAtts : nan, MeanMeansOfNumericAtts : nan, MeanMutualInformation : 0.5108158897798274, MeanNoiseToSignalRatio : 0.8902878340922058, MeanNominalAttDistinctValues : 3.2777777777777777, MeanSkewnessOfNumericAtts : nan, MeanStdDevOfNumericAtts : nan, MinAttributeEntropy : 0.07262476248540556, MinKurtosisOfNumericAtts : nan, MinMeansOfNumericAtts : nan, MinMutualInformation : 0.0468182939867, MinNominalAttDistinctValues : 2.0, MinSkewnessOfNumericAtts : nan, MinStdDevOfNumericAtts : nan, MinorityClassPercentage : 1.171303074670571, MinorityClassSize : 8.0, NaiveBayesAUC : 0.9921587580230303, NaiveBayesErrRate : 0.08931185944363104, NaiveBayesKappa : 0.9019654903843212, NumberOfBinaryFeatures : 16.0, NumberOfClasses : 19.0, NumberOfFeatures : 36.0, NumberOfInstances : 683.0, NumberOfInstancesWithMissingValues : 121.0, NumberOfMissingValues : 2337.0, NumberOfNumericFeatures : 0.0, NumberOfSymbolicFeatures : 36.0, PercentageOfBinaryFeatures : 44.44444444444444, PercentageOfInstancesWithMissingValues : 17.71595900439239, PercentageOfMissingValues : 9.504636408003904, PercentageOfNumericFeatures : 0.0, PercentageOfSymbolicFeatures : 100.0, Quartile1AttributeEntropy : 0.4629328593168401, Quartile1KurtosisOfNumericAtts : nan, Quartile1MeansOfNumericAtts : nan, Quartile1MutualInformation : 0.26369905545327, Quartile1SkewnessOfNumericAtts : nan, Quartile1StdDevOfNumericAtts : nan, Quartile2AttributeEntropy : 0.9158362664344971, Quartile2KurtosisOfNumericAtts : nan, Quartile2MeansOfNumericAtts : nan, Quartile2MutualInformation : 0.45996721558355, Quartile2SkewnessOfNumericAtts : nan, Quartile2StdDevOfNumericAtts : nan, Quartile3AttributeEntropy : 1.408326420019514, Quartile3KurtosisOfNumericAtts : nan, Quartile3MeansOfNumericAtts : nan, Quartile3MutualInformation : 0.71879499353135, Quartile3SkewnessOfNumericAtts : nan, Quartile3StdDevOfNumericAtts : nan, REPTreeDepth1AUC : 0.9436075624852911, REPTreeDepth1ErrRate : 0.26500732064421667, REPTreeDepth1Kappa : 0.7052208643815201, REPTreeDepth2AUC : 0.9436075624852911, REPTreeDepth2ErrRate : 0.26500732064421667, REPTreeDepth2Kappa : 0.7052208643815201, REPTreeDepth3AUC : 0.9436075624852911, REPTreeDepth3ErrRate : 0.26500732064421667, REPTreeDepth3Kappa : 0.7052208643815201, RandomTreeDepth1AUC : 0.9035959879652148, RandomTreeDepth1ErrRate : 0.18594436310395315, RandomTreeDepth1Kappa : 0.7960191985250715, RandomTreeDepth2AUC : 0.9035959879652148, RandomTreeDepth2ErrRate : 0.18594436310395315, RandomTreeDepth2Kappa : 0.7960191985250715, RandomTreeDepth3AUC : 0.9035959879652148, RandomTreeDepth3ErrRate : 0.18594436310395315, RandomTreeDepth3Kappa : 0.7960191985250715, StdvNominalAttDistinctValues : 2.884551077834282, kNN1NAUC : 0.9616161058225481, kNN1NErrRate : 0.1171303074670571, kNN1NKappa : 0.871344781387376,', 'status': 'active', 'uploader': 1, 'version': 1}, page_content='### Classes \\n\\n-- 19 Classes = {diaporthe-stem-canker, charcoal-rot, rhizoctonia-root-rot, phytophthora-rot, brown-stem-rot, powdery-mildew, downy-mildew, brown-spot, bacterial-blight, bacterial-pustule, purple-seed-stain, anthracnose, phyllosticta-leaf-spot, alternarialeaf-spot, frog-eye-leaf-spot, diaporthe-pod-&amp;-stem-blight, cyst-nematode, 2-4-d-injury, herbicide-injury} \\n\\n### Revelant papers\\n\\nTan, M., &amp; Eshelman, L. (1988). Using weighted networks to represent classification knowledge in noisy domains. Proceedings of the Fifth International Conference on Machine Learning (pp. 121-134). Ann Arbor, Michigan: Morgan Kaufmann.'),\n Document(metadata={'MajorityClassSize': 214.0, 'MaxNominalAttDistinctValues': 27.0, 'MinorityClassSize': 105.0, 'NumberOfClasses': 5.0, 'NumberOfFeatures': 20.0, 'NumberOfInstances': 736.0, 'NumberOfInstancesWithMissingValues': 95.0, 'NumberOfMissingValues': 448.0, 'NumberOfNumericFeatures': 14.0, 'NumberOfSymbolicFeatures': 6.0, 'Unnamed: 0': 123, 'description': \"**Author**: Bruce Bulloch    \\n**Source**: [WEKA Dataset Collection](http://www.cs.waikato.ac.nz/ml/weka/datasets.html) - part of the agridatasets archive. [This is the true source](http://tunedit.org/repo/Data/Agricultural/eucalyptus.arff)  \\n**Please cite**: None  \\n\\n**Eucalyptus Soil Conservation**  \\nThe objective was to determine which seedlots in a species are best for soil conservation in seasonally dry hill country. Determination is found by measurement of height, diameter by height, survival, and other contributing factors. \\n \\nIt is important to note that eucalypt trial methods changed over time; earlier trials included mostly 15 - 30cm tall seedling grown in peat plots and the later trials have included mostly three replications of eight trees grown. This change may contribute to less significant results.\\n\\nExperimental data recording procedures which require noting include:\\n - instances with no data recorded due to experimental recording procedures\\n   require that the absence of a species from one replicate at a site was\\n   treated as a missing value, but if absent from two or more replicates at a\\n   site the species was excluded from the site's analyses.\\n - missing data for survival, vigour, insect resistance, stem form, crown form\\n   and utility especially for the data recorded at the Morea Station; this \\n   could indicate the death of species in these areas or a lack in collection\\n   of data.  \\n\\n### Attribute Information  \\n \\n  1.  Abbrev - site abbreviation - enumerated\\n  2.  Rep - site rep - integer\\n  3.  Locality - site locality in the North Island - enumerated\\n  4.  Map_Ref - map location in the North Island - enumerated\\n  5.  Latitude - latitude approximation - enumerated\\n  6.  Altitude - altitude approximation - integer\\n  7.  Rainfall - rainfall (mm pa) - integer\\n  8.  Frosts - frosts (deg. c) - integer\\n  9.  Year - year of planting - integer\\n  10. Sp - species code - enumerated\\n  11. PMCno - seedlot number - integer\\n  12. DBH - best diameter base height (cm) - real\\n  13. Ht - height (m) - real\\n  14. Surv - survival - integer\\n  15. Vig - vigour - real\\n  16. Ins_res - insect resistance - real\\n  17. Stem_Fm - stem form - real\\n  18. Crown_Fm - crown form - real\\n  19. Brnch_Fm - branch form - real\\n  Class:\\n  20. Utility - utility rating - enumerated\\n\\n### Relevant papers\\n\\nBulluch B. T., (1992) Eucalyptus Species Selection for Soil Conservation in Seasonally Dry Hill Country - Twelfth Year Assessment  New Zealand Journal of Forestry Science 21(1): 10 - 31 (1991)  \\n\\nKirsten Thomson and Robert J. McQueen (1996) Machine Learning Applied to Fourteen Agricultural Datasets. University of Waikato Research Report  \\nhttps://www.cs.waikato.ac.nz/ml/publications/1996/Thomson-McQueen-96.pdf + the original publication:\", 'did': 188, 'features': '0 : [0 - Abbrev (nominal)], 1 : [1 - Rep (numeric)], 2 : [2 - Locality (nominal)], 3 : [3 - Map_Ref (nominal)], 4 : [4 - Latitude (nominal)], 5 : [5 - Altitude (numeric)], 6 : [6 - Rainfall (numeric)], 7 : [7 - Frosts (numeric)], 8 : [8 - Year (numeric)], 9 : [9 - Sp (nominal)], 10 : [10 - PMCno (numeric)], 11 : [11 - DBH (numeric)], 12 : [12 - Ht (numeric)], 13 : [13 - Surv (numeric)], 14 : [14 - Vig (numeric)], 15 : [15 - Ins_res (numeric)], 16 : [16 - Stem_Fm (numeric)], 17 : [17 - Crown_Fm (numeric)], 18 : [18 - Brnch_Fm (numeric)], 19 : [19 - Utility (nominal)],', 'format': 'ARFF', 'name': 'eucalyptus', 'qualities': 'AutoCorrelation : 0.39319727891156464, CfsSubsetEval_DecisionStumpAUC : 0.8239493966657213, CfsSubsetEval_DecisionStumpErrRate : 0.41847826086956524, CfsSubsetEval_DecisionStumpKappa : 0.4637307109078737, CfsSubsetEval_NaiveBayesAUC : 0.8239493966657213, CfsSubsetEval_NaiveBayesErrRate : 0.41847826086956524, CfsSubsetEval_NaiveBayesKappa : 0.4637307109078737, CfsSubsetEval_kNN1NAUC : 0.8239493966657213, CfsSubsetEval_kNN1NErrRate : 0.41847826086956524, CfsSubsetEval_kNN1NKappa : 0.4637307109078737, ClassEntropy : 2.262083620428274, DecisionStumpAUC : 0.7519401667350958, DecisionStumpErrRate : 0.5054347826086957, DecisionStumpKappa : 0.30247986100142155, Dimensionality : 0.02717391304347826, EquivalentNumberOfAtts : 5.9334684401020565, J48.00001.AUC : 0.8184137151683228, J48.00001.ErrRate : 0.3967391304347826, J48.00001.Kappa : 0.49336985707179887, J48.0001.AUC : 0.8184137151683228, J48.0001.ErrRate : 0.3967391304347826, J48.0001.Kappa : 0.49336985707179887, J48.001.AUC : 0.8184137151683228, J48.001.ErrRate : 0.3967391304347826, J48.001.Kappa : 0.49336985707179887, MajorityClassPercentage : 29.076086956521742, MajorityClassSize : 214.0, MaxAttributeEntropy : 4.2373637557635595, MaxKurtosisOfNumericAtts : 734.9416211795777, MaxMeansOfNumericAtts : 2054.7393689986247, MaxMutualInformation : 0.42753276429854, MaxNominalAttDistinctValues : 27.0, MaxSkewnessOfNumericAtts : 27.109270846229688, MaxStdDevOfNumericAtts : 1551.7798185802085, MeanAttributeEntropy : 3.4626363060529055, MeanKurtosisOfNumericAtts : 62.86596625813314, MeanMeansOfNumericAtts : 390.0868288072735, MeanMutualInformation : 0.381241367214446, MeanNoiseToSignalRatio : 8.082530396301912, MeanNominalAttDistinctValues : 13.666666666666666, MeanSkewnessOfNumericAtts : 2.551453016115177, MeanStdDevOfNumericAtts : 172.61081562461396, MinAttributeEntropy : 2.5810641739409617, MinKurtosisOfNumericAtts : -1.887802596870339, MinMeansOfNumericAtts : -2.5842391304347836, MinMutualInformation : 0.24650313929826, MinNominalAttDistinctValues : 5.0, MinSkewnessOfNumericAtts : -0.6970908724266737, MinStdDevOfNumericAtts : 0.49318784476285216, MinorityClassPercentage : 14.266304347826086, MinorityClassSize : 105.0, NaiveBayesAUC : 0.8520788174118736, NaiveBayesErrRate : 0.45108695652173914, NaiveBayesKappa : 0.42741183362624485, NumberOfBinaryFeatures : 0.0, NumberOfClasses : 5.0, NumberOfFeatures : 20.0, NumberOfInstances : 736.0, NumberOfInstancesWithMissingValues : 95.0, NumberOfMissingValues : 448.0, NumberOfNumericFeatures : 14.0, NumberOfSymbolicFeatures : 6.0, PercentageOfBinaryFeatures : 0.0, PercentageOfInstancesWithMissingValues : 12.907608695652172, PercentageOfMissingValues : 3.0434782608695654, PercentageOfNumericFeatures : 70.0, PercentageOfSymbolicFeatures : 30.0, Quartile1AttributeEntropy : 2.908861461974274, Quartile1KurtosisOfNumericAtts : -0.4961422376730956, Quartile1MeansOfNumericAtts : 2.882908545727137, Quartile1MutualInformation : 0.323312362530555, Quartile1SkewnessOfNumericAtts : -0.3960800165047112, Quartile1StdDevOfNumericAtts : 0.778502789181291, Quartile2AttributeEntropy : 3.4759821137655975, Quartile2KurtosisOfNumericAtts : 0.4289115082721384, Quartile2MeansOfNumericAtts : 6.249602617058818, Quartile2MutualInformation : 0.40765214345173, Quartile2SkewnessOfNumericAtts : 0.11119478923130877, Quartile2StdDevOfNumericAtts : 1.3465996573398586, Quartile3AttributeEntropy : 4.009738246275191, Quartile3KurtosisOfNumericAtts : 1.3641239688052669, Quartile3MeansOfNumericAtts : 403.0027173913041, Quartile3MutualInformation : 0.425964983779695, Quartile3SkewnessOfNumericAtts : 0.9548948008878528, Quartile3StdDevOfNumericAtts : 80.61760056258042, REPTreeDepth1AUC : 0.7171370640805235, REPTreeDepth1ErrRate : 0.5557065217391305, REPTreeDepth1Kappa : 0.2672017371533179, REPTreeDepth2AUC : 0.7171370640805235, REPTreeDepth2ErrRate : 0.5557065217391305, REPTreeDepth2Kappa : 0.2672017371533179, REPTreeDepth3AUC : 0.7171370640805235, REPTreeDepth3ErrRate : 0.5557065217391305, REPTreeDepth3Kappa : 0.2672017371533179, RandomTreeDepth1AUC : 0.7219508813313532, RandomTreeDepth1ErrRate : 0.47690217391304346, RandomTreeDepth1Kappa : 0.3915134670419616, RandomTreeDepth2AUC : 0.7219508813313532, RandomTreeDepth2ErrRate : 0.47690217391304346, RandomTreeDepth2Kappa : 0.3915134670419616, RandomTreeDepth3AUC : 0.7219508813313532, RandomTreeDepth3ErrRate : 0.47690217391304346, RandomTreeDepth3Kappa : 0.3915134670419616, StdvNominalAttDistinctValues : 7.659416862050705, kNN1NAUC : 0.7018152602695222, kNN1NErrRate : 0.46603260869565216, kNN1NKappa : 0.40228622299671357,', 'status': 'active', 'uploader': 1, 'version': 1}, page_content='Kirsten Thomson and Robert J. McQueen (1996) Machine Learning Applied to Fourteen Agricultural Datasets. University of Waikato Research Report'),\n Document(metadata={'MajorityClassSize': 92.0, 'MaxNominalAttDistinctValues': 19.0, 'MinorityClassSize': 8.0, 'NumberOfClasses': 19.0, 'NumberOfFeatures': 36.0, 'NumberOfInstances': 683.0, 'NumberOfInstancesWithMissingValues': 121.0, 'NumberOfMissingValues': 2337.0, 'NumberOfNumericFeatures': 0.0, 'NumberOfSymbolicFeatures': 36.0, 'Unnamed: 0': 36, 'description': '**Author**: R.S. Michalski and R.L. Chilausky (Donors: Ming Tan &amp; Jeff Schlimmer)  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Soybean+(Large)) - 1988  \\n**Please cite**: R.S. Michalski and R.L. Chilausky \"Learning by Being Told and Learning from Examples: An Experimental Comparison of the Two Methods of Knowledge Acquisition in the Context of Developing an Expert System for Soybean Disease Diagnosis\", International Journal of Policy Analysis and Information Systems, Vol. 4, No. 2, 1980.  \\n\\n**Large Soybean Database**  \\nThis is the large soybean database from the UCI repository, with its training and test database combined into a single file. \\n\\nThere are 19 classes, only the first 15 of which have been used in prior work. The folklore seems to be that the last four classes are unjustified by the data since they have so few examples. There are 35 categorical attributes, some nominal and some ordered. The value \\'dna\\' means does not apply. The values for attributes are encoded numerically, with the first value encoded as \"0,\\'\\' the second as \"1,\\'\\' and so forth. An unknown value is encoded as \"?\\'\\'.\\n\\n### Attribute Information\\n\\n1. date: april,may,june,july,august,september,october,?. \\n2. plant-stand: normal,lt-normal,?. \\n3. precip: lt-norm,norm,gt-norm,?. \\n4. temp: lt-norm,norm,gt-norm,?. \\n5. hail: yes,no,?. \\n6. crop-hist: diff-lst-year,same-lst-yr,same-lst-two-yrs, \\nsame-lst-sev-yrs,?. \\n7. area-damaged: scattered,low-areas,upper-areas,whole-field,?. \\n8. severity: minor,pot-severe,severe,?. \\n9. seed-tmt: none,fungicide,other,?. \\n10. germination: 90-100%,80-89%,lt-80%,?. \\n11. plant-growth: norm,abnorm,?. \\n12. leaves: norm,abnorm. \\n13. leafspots-halo: absent,yellow-halos,no-yellow-halos,?. \\n14. leafspots-marg: w-s-marg,no-w-s-marg,dna,?. \\n15. leafspot-size: lt-1/8,gt-1/8,dna,?. \\n16. leaf-shread: absent,present,?. \\n17. leaf-malf: absent,present,?. \\n18. leaf-mild: absent,upper-surf,lower-surf,?. \\n19. stem: norm,abnorm,?. \\n20. lodging: yes,no,?. \\n21. stem-cankers: absent,below-soil,above-soil,above-sec-nde,?. \\n22. canker-lesion: dna,brown,dk-brown-blk,tan,?. \\n23. fruiting-bodies: absent,present,?. \\n24. external decay: absent,firm-and-dry,watery,?. \\n25. mycelium: absent,present,?. \\n26. int-discolor: none,brown,black,?. \\n27. sclerotia: absent,present,?. \\n28. fruit-pods: norm,diseased,few-present,dna,?. \\n29. fruit spots: absent,colored,brown-w/blk-specks,distort,dna,?. \\n30. seed: norm,abnorm,?. \\n31. mold-growth: absent,present,?. \\n32. seed-discolor: absent,present,?. \\n33. seed-size: norm,lt-norm,?. \\n34. shriveling: absent,present,?. \\n35. roots: norm,rotted,galls-cysts,?.\\n\\n### Classes \\n\\n-- 19 Classes = {diaporthe-stem-canker, charcoal-rot, rhizoctonia-root-rot, phytophthora-rot, brown-stem-rot, powdery-mildew, downy-mildew, brown-spot, bacterial-blight, bacterial-pustule, purple-seed-stain, anthracnose, phyllosticta-leaf-spot, alternarialeaf-spot, frog-eye-leaf-spot, diaporthe-pod-&amp;-stem-blight, cyst-nematode, 2-4-d-injury, herbicide-injury} \\n\\n### Revelant papers\\n\\nTan, M., &amp; Eshelman, L. (1988). Using weighted networks to represent classification knowledge in noisy domains. Proceedings of the Fifth International Conference on Machine Learning (pp. 121-134). Ann Arbor, Michigan: Morgan Kaufmann. \\n\\nFisher,D.H. &amp; Schlimmer,J.C. (1988). Concept Simplification and Predictive Accuracy. Proceedings of the Fifth International Conference on Machine Learning (pp. 22-28). Ann Arbor, Michigan: Morgan Kaufmann.', 'did': 42, 'features': '0 : [0 - date (nominal)], 1 : [1 - plant-stand (nominal)], 2 : [2 - precip (nominal)], 3 : [3 - temp (nominal)], 4 : [4 - hail (nominal)], 5 : [5 - crop-hist (nominal)], 6 : [6 - area-damaged (nominal)], 7 : [7 - severity (nominal)], 8 : [8 - seed-tmt (nominal)], 9 : [9 - germination (nominal)], 10 : [10 - plant-growth (nominal)], 11 : [11 - leaves (nominal)], 12 : [12 - leafspots-halo (nominal)], 13 : [13 - leafspots-marg (nominal)], 14 : [14 - leafspot-size (nominal)], 15 : [15 - leaf-shread (nominal)], 16 : [16 - leaf-malf (nominal)], 17 : [17 - leaf-mild (nominal)], 18 : [18 - stem (nominal)], 19 : [19 - lodging (nominal)], 20 : [20 - stem-cankers (nominal)], 21 : [21 - canker-lesion (nominal)], 22 : [22 - fruiting-bodies (nominal)], 23 : [23 - external-decay (nominal)], 24 : [24 - mycelium (nominal)], 25 : [25 - int-discolor (nominal)], 26 : [26 - sclerotia (nominal)], 27 : [27 - fruit-pods (nominal)], 28 : [28 - fruit-spots (nominal)], 29 : [29 - seed (nominal)], 30 : [30 - mold-growth (nominal)], 31 : [31 - seed-discolor (nominal)], 32 : [32 - seed-size (nominal)], 33 : [33 - shriveling (nominal)], 34 : [34 - roots (nominal)], 35 : [35 - class (nominal)],', 'format': 'ARFF', 'name': 'soybean', 'qualities': 'AutoCorrelation : 0.9457478005865103, CfsSubsetEval_DecisionStumpAUC : 0.9620422408823379, CfsSubsetEval_DecisionStumpErrRate : 0.13323572474377746, CfsSubsetEval_DecisionStumpKappa : 0.8534752853145238, CfsSubsetEval_NaiveBayesAUC : 0.9620422408823379, CfsSubsetEval_NaiveBayesErrRate : 0.13323572474377746, CfsSubsetEval_NaiveBayesKappa : 0.8534752853145238, CfsSubsetEval_kNN1NAUC : 0.9620422408823379, CfsSubsetEval_kNN1NErrRate : 0.13323572474377746, CfsSubsetEval_kNN1NKappa : 0.8534752853145238, ClassEntropy : 3.83550798457672, DecisionStumpAUC : 0.8099631489104341, DecisionStumpErrRate : 0.7203513909224012, DecisionStumpKappa : 0.19424522533539545, Dimensionality : 0.0527086383601757, EquivalentNumberOfAtts : 7.508591767241043, J48.00001.AUC : 0.9739047068470593, J48.00001.ErrRate : 0.12152269399707175, J48.00001.Kappa : 0.8663370421980624, J48.0001.AUC : 0.9739047068470593, J48.0001.ErrRate : 0.12152269399707175, J48.0001.Kappa : 0.8663370421980624, J48.001.AUC : 0.9739047068470593, J48.001.ErrRate : 0.12152269399707175, J48.001.Kappa : 0.8663370421980624, MajorityClassPercentage : 13.469985358711567, MajorityClassSize : 92.0, MaxAttributeEntropy : 2.6849389644492594, MaxKurtosisOfNumericAtts : nan, MaxMeansOfNumericAtts : nan, MaxMutualInformation : 1.28692474762189, MaxNominalAttDistinctValues : 19.0, MaxSkewnessOfNumericAtts : nan, MaxStdDevOfNumericAtts : nan, MeanAttributeEntropy : 0.9655890619117928, MeanKurtosisOfNumericAtts : nan, MeanMeansOfNumericAtts : nan, MeanMutualInformation : 0.5108158897798274, MeanNoiseToSignalRatio : 0.8902878340922058, MeanNominalAttDistinctValues : 3.2777777777777777, MeanSkewnessOfNumericAtts : nan, MeanStdDevOfNumericAtts : nan, MinAttributeEntropy : 0.07262476248540556, MinKurtosisOfNumericAtts : nan, MinMeansOfNumericAtts : nan, MinMutualInformation : 0.0468182939867, MinNominalAttDistinctValues : 2.0, MinSkewnessOfNumericAtts : nan, MinStdDevOfNumericAtts : nan, MinorityClassPercentage : 1.171303074670571, MinorityClassSize : 8.0, NaiveBayesAUC : 0.9921587580230303, NaiveBayesErrRate : 0.08931185944363104, NaiveBayesKappa : 0.9019654903843212, NumberOfBinaryFeatures : 16.0, NumberOfClasses : 19.0, NumberOfFeatures : 36.0, NumberOfInstances : 683.0, NumberOfInstancesWithMissingValues : 121.0, NumberOfMissingValues : 2337.0, NumberOfNumericFeatures : 0.0, NumberOfSymbolicFeatures : 36.0, PercentageOfBinaryFeatures : 44.44444444444444, PercentageOfInstancesWithMissingValues : 17.71595900439239, PercentageOfMissingValues : 9.504636408003904, PercentageOfNumericFeatures : 0.0, PercentageOfSymbolicFeatures : 100.0, Quartile1AttributeEntropy : 0.4629328593168401, Quartile1KurtosisOfNumericAtts : nan, Quartile1MeansOfNumericAtts : nan, Quartile1MutualInformation : 0.26369905545327, Quartile1SkewnessOfNumericAtts : nan, Quartile1StdDevOfNumericAtts : nan, Quartile2AttributeEntropy : 0.9158362664344971, Quartile2KurtosisOfNumericAtts : nan, Quartile2MeansOfNumericAtts : nan, Quartile2MutualInformation : 0.45996721558355, Quartile2SkewnessOfNumericAtts : nan, Quartile2StdDevOfNumericAtts : nan, Quartile3AttributeEntropy : 1.408326420019514, Quartile3KurtosisOfNumericAtts : nan, Quartile3MeansOfNumericAtts : nan, Quartile3MutualInformation : 0.71879499353135, Quartile3SkewnessOfNumericAtts : nan, Quartile3StdDevOfNumericAtts : nan, REPTreeDepth1AUC : 0.9436075624852911, REPTreeDepth1ErrRate : 0.26500732064421667, REPTreeDepth1Kappa : 0.7052208643815201, REPTreeDepth2AUC : 0.9436075624852911, REPTreeDepth2ErrRate : 0.26500732064421667, REPTreeDepth2Kappa : 0.7052208643815201, REPTreeDepth3AUC : 0.9436075624852911, REPTreeDepth3ErrRate : 0.26500732064421667, REPTreeDepth3Kappa : 0.7052208643815201, RandomTreeDepth1AUC : 0.9035959879652148, RandomTreeDepth1ErrRate : 0.18594436310395315, RandomTreeDepth1Kappa : 0.7960191985250715, RandomTreeDepth2AUC : 0.9035959879652148, RandomTreeDepth2ErrRate : 0.18594436310395315, RandomTreeDepth2Kappa : 0.7960191985250715, RandomTreeDepth3AUC : 0.9035959879652148, RandomTreeDepth3ErrRate : 0.18594436310395315, RandomTreeDepth3Kappa : 0.7960191985250715, StdvNominalAttDistinctValues : 2.884551077834282, kNN1NAUC : 0.9616161058225481, kNN1NErrRate : 0.1171303074670571, kNN1NKappa : 0.871344781387376,', 'status': 'active', 'uploader': 1, 'version': 1}, page_content='RandomTreeDepth3Kappa : 0.7960191985250715, StdvNominalAttDistinctValues : 2.884551077834282, kNN1NAUC : 0.9616161058225481, kNN1NErrRate : 0.1171303074670571, kNN1NKappa : 0.871344781387376,, features - 0 : [0 - date (nominal)], 1 : [1 - plant-stand (nominal)], 2 : [2 - precip (nominal)], 3 : [3 - temp (nominal)], 4 : [4 - hail (nominal)], 5 : [5 - crop-hist (nominal)], 6 : [6 - area-damaged (nominal)], 7 : [7 - severity (nominal)], 8 : [8 - seed-tmt (nominal)], 9 : [9 - germination (nominal)], 10 : [10 - plant-growth (nominal)], 11 : [11 - leaves (nominal)], 12 : [12 - leafspots-halo (nominal)], 13 : [13 - leafspots-marg (nominal)], 14 : [14 - leafspot-size (nominal)], 15 : [15 - leaf-shread (nominal)], 16 : [16 - leaf-malf (nominal)], 17 : [17 - leaf-mild (nominal)], 18 : [18 - stem (nominal)], 19 : [19 - lodging (nominal)], 20 : [20 - stem-cankers (nominal)], 21 : [21 - canker-lesion (nominal)], 22 : [22 - fruiting-bodies (nominal)], 23 : [23 - external-decay (nominal)], 24 : [24'),\n Document(metadata={'MajorityClassSize': 71.0, 'MaxNominalAttDistinctValues': 3.0, 'MinorityClassSize': 48.0, 'NumberOfClasses': 3.0, 'NumberOfFeatures': 14.0, 'NumberOfInstances': 178.0, 'NumberOfInstancesWithMissingValues': 0.0, 'NumberOfMissingValues': 0.0, 'NumberOfNumericFeatures': 13.0, 'NumberOfSymbolicFeatures': 1.0, 'Unnamed: 0': 122, 'description': '**Author**:   \\n**Source**: Unknown -   \\n**Please cite**:   \\n\\n1. Title of Database: Wine recognition data\\n \\tUpdated Sept 21, 1998 by C.Blake : Added attribute information\\n \\n 2. Sources:\\n    (a) Forina, M. et al, PARVUS - An Extendible Package for Data\\n        Exploration, Classification and Correlation. Institute of Pharmaceutical\\n        and Food Analysis and Technologies, Via Brigata Salerno, \\n        16147 Genoa, Italy.\\n \\n    (b) Stefan Aeberhard, email: stefan@coral.cs.jcu.edu.au\\n    (c) July 1991\\n 3. Past Usage:\\n \\n    (1)\\n    S. Aeberhard, D. Coomans and O. de Vel,\\n    Comparison of Classifiers in High Dimensional Settings,\\n    Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\\n    Mathematics and Statistics, James Cook University of North Queensland.\\n    (Also submitted to Technometrics).\\n \\n    The data was used with many others for comparing various \\n    classifiers. The classes are separable, though only RDA \\n    has achieved 100% correct classification.\\n    (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\\n    (All results using the leave-one-out technique)\\n \\n    In a classification context, this is a well posed problem \\n    with \"well behaved\" class structures. A good data set \\n    for first testing of a new classifier, but not very \\n    challenging.\\n \\n    (2) \\n    S. Aeberhard, D. Coomans and O. de Vel,\\n    \"THE CLASSIFICATION PERFORMANCE OF RDA\"\\n    Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\\n    Mathematics and Statistics, James Cook University of North Queensland.\\n    (Also submitted to Journal of Chemometrics).\\n \\n    Here, the data was used to illustrate the superior performance of\\n    the use of a new appreciation function with RDA. \\n \\n 4. Relevant Information:\\n \\n    -- These data are the results of a chemical analysis of\\n       wines grown in the same region in Italy but derived from three\\n       different cultivars.\\n       The analysis determined the quantities of 13 constituents\\n       found in each of the three types of wines. \\n \\n    -- I think that the initial data set had around 30 variables, but \\n       for some reason I only have the 13 dimensional version. \\n       I had a list of what the 30 or so variables were, but a.) \\n       I lost it, and b.), I would not know which 13 variables\\n       are included in the set.\\n \\n    -- The attributes are (dontated by Riccardo Leardi, \\n \\triclea@anchem.unige.it )\\n  \\t1) Alcohol\\n  \\t2) Malic acid\\n  \\t3) Ash\\n \\t4) Alcalinity of ash  \\n  \\t5) Magnesium\\n \\t6) Total phenols\\n  \\t7) Flavanoids\\n  \\t8) Nonflavanoid phenols\\n  \\t9) Proanthocyanins\\n \\t10)Color intensity\\n  \\t11)Hue\\n  \\t12)OD280/OD315 of diluted wines\\n  \\t13)Proline            \\n \\n 5. Number of Instances\\n \\n       \\tclass 1 59\\n \\tclass 2 71\\n \\tclass 3 48\\n \\n 6. Number of Attributes \\n \\t\\n \\t13\\n \\n 7. For Each Attribute:\\n \\n \\tAll attributes are continuous\\n \\t\\n \\tNo statistics available, but suggest to standardise\\n \\tvariables for certain uses (e.g. for us with classifiers\\n \\twhich are NOT scale invariant)\\n \\n \\tNOTE: 1st attribute is class identifier (1-3)\\n \\n 8. Missing Attribute Values:\\n \\n \\tNone\\n \\n 9. Class Distribution: number of instances per class\\n \\n       \\tclass 1 59\\n \\tclass 2 71\\n \\tclass 3 48\\n\\n Information about the dataset\\n CLASSTYPE: nominal\\n CLASSINDEX: first', 'did': 187, 'features': '0 : [0 - class (nominal)], 1 : [1 - Alcohol (numeric)], 2 : [2 - Malic_acid (numeric)], 3 : [3 - Ash (numeric)], 4 : [4 - Alcalinity_of_ash (numeric)], 5 : [5 - Magnesium (numeric)], 6 : [6 - Total_phenols (numeric)], 7 : [7 - Flavanoids (numeric)], 8 : [8 - Nonflavanoid_phenols (numeric)], 9 : [9 - Proanthocyanins (numeric)], 10 : [10 - Color_intensity (numeric)], 11 : [11 - Hue (numeric)], 12 : [12 - OD280%2FOD315_of_diluted_wines (numeric)], 13 : [13 - Proline (numeric)],', 'format': 'ARFF', 'name': 'wine', 'qualities': 'AutoCorrelation : 0.9887005649717514, CfsSubsetEval_DecisionStumpAUC : 0.934807485785613, CfsSubsetEval_DecisionStumpErrRate : 0.0898876404494382, CfsSubsetEval_DecisionStumpKappa : 0.8636080647478569, CfsSubsetEval_NaiveBayesAUC : 0.934807485785613, CfsSubsetEval_NaiveBayesErrRate : 0.0898876404494382, CfsSubsetEval_NaiveBayesKappa : 0.8636080647478569, CfsSubsetEval_kNN1NAUC : 0.934807485785613, CfsSubsetEval_kNN1NErrRate : 0.0898876404494382, CfsSubsetEval_kNN1NKappa : 0.8636080647478569, ClassEntropy : 1.5668222768551812, DecisionStumpAUC : 0.7973435168459908, DecisionStumpErrRate : 0.37640449438202245, DecisionStumpKappa : 0.4058981767460396, Dimensionality : 0.07865168539325842, EquivalentNumberOfAtts : nan, J48.00001.AUC : 0.934807485785613, J48.00001.ErrRate : 0.0898876404494382, J48.00001.Kappa : 0.8636080647478569, J48.0001.AUC : 0.934807485785613, J48.0001.ErrRate : 0.0898876404494382, J48.0001.Kappa : 0.8636080647478569, J48.001.AUC : 0.934807485785613, J48.001.ErrRate : 0.0898876404494382, J48.001.Kappa : 0.8636080647478569, MajorityClassPercentage : 39.8876404494382, MajorityClassSize : 71.0, MaxAttributeEntropy : nan, MaxKurtosisOfNumericAtts : 2.1049913235905877, MaxMeansOfNumericAtts : 746.8932584269661, MaxMutualInformation : nan, MaxNominalAttDistinctValues : 3.0, MaxSkewnessOfNumericAtts : 1.0981910547551612, MaxStdDevOfNumericAtts : 314.90747427684903, MeanAttributeEntropy : nan, MeanKurtosisOfNumericAtts : 0.006742802303924433, MeanMeansOfNumericAtts : 69.13366292091614, MeanMutualInformation : nan, MeanNoiseToSignalRatio : nan, MeanNominalAttDistinctValues : 3.0, MeanSkewnessOfNumericAtts : 0.3501684984202117, MeanStdDevOfNumericAtts : 26.17778523132608, MinAttributeEntropy : nan, MinKurtosisOfNumericAtts : -1.0864345274098706, MinMeansOfNumericAtts : 0.3618539325842697, MinMutualInformation : nan, MinNominalAttDistinctValues : 3.0, MinSkewnessOfNumericAtts : -0.30728549895848073, MinStdDevOfNumericAtts : 0.12445334029667939, MinorityClassPercentage : 26.96629213483146, MinorityClassSize : 48.0, NaiveBayesAUC : 0.983140867878747, NaiveBayesErrRate : 0.0449438202247191, NaiveBayesKappa : 0.9319148936170213, NumberOfBinaryFeatures : 0.0, NumberOfClasses : 3.0, NumberOfFeatures : 14.0, NumberOfInstances : 178.0, NumberOfInstancesWithMissingValues : 0.0, NumberOfMissingValues : 0.0, NumberOfNumericFeatures : 13.0, NumberOfSymbolicFeatures : 1.0, PercentageOfBinaryFeatures : 0.0, PercentageOfInstancesWithMissingValues : 0.0, PercentageOfMissingValues : 0.0, PercentageOfNumericFeatures : 92.85714285714286, PercentageOfSymbolicFeatures : 7.142857142857142, Quartile1AttributeEntropy : nan, Quartile1KurtosisOfNumericAtts : -0.8440630459414751, Quartile1MeansOfNumericAtts : 1.8100842696629211, Quartile1MutualInformation : nan, Quartile1SkewnessOfNumericAtts : -0.0151955294387136, Quartile1StdDevOfNumericAtts : 0.4233514358677881, Quartile2AttributeEntropy : nan, Quartile2KurtosisOfNumericAtts : -0.24840310614613204, Quartile2MeansOfNumericAtts : 2.366516853932584, Quartile2MutualInformation : nan, Quartile2SkewnessOfNumericAtts : 0.2130468864264532, Quartile2StdDevOfNumericAtts : 0.8118265380058574, Quartile3AttributeEntropy : nan, Quartile3KurtosisOfNumericAtts : 0.5212950315345126, Quartile3MeansOfNumericAtts : 16.247780898876407, Quartile3MutualInformation : nan, Quartile3SkewnessOfNumericAtts : 0.8182032861734947, Quartile3StdDevOfNumericAtts : 2.828924819497959, REPTreeDepth1AUC : 0.9038527890255288, REPTreeDepth1ErrRate : 0.15168539325842698, REPTreeDepth1Kappa : 0.7710010959165197, REPTreeDepth2AUC : 0.9038527890255288, REPTreeDepth2ErrRate : 0.15168539325842698, REPTreeDepth2Kappa : 0.7710010959165197, REPTreeDepth3AUC : 0.9038527890255288, REPTreeDepth3ErrRate : 0.15168539325842698, REPTreeDepth3Kappa : 0.7710010959165197, RandomTreeDepth1AUC : 0.9363963414265778, RandomTreeDepth1ErrRate : 0.08426966292134831, RandomTreeDepth1Kappa : 0.872212118311477, RandomTreeDepth2AUC : 0.9363963414265778, RandomTreeDepth2ErrRate : 0.08426966292134831, RandomTreeDepth2Kappa : 0.872212118311477, RandomTreeDepth3AUC : 0.9363963414265778, RandomTreeDepth3ErrRate : 0.08426966292134831, RandomTreeDepth3Kappa : 0.872212118311477, StdvNominalAttDistinctValues : 0.0, kNN1NAUC : 0.9552036199095022, kNN1NErrRate : 0.06179775280898876, kNN1NKappa : 0.9069126176666349,', 'status': 'active', 'uploader': 1, 'version': 1}, page_content='the use of a new appreciation function with RDA. \\n \\n 4. Relevant Information:\\n \\n    -- These data are the results of a chemical analysis of\\n       wines grown in the same region in Italy but derived from three\\n       different cultivars.\\n       The analysis determined the quantities of 13 constituents\\n       found in each of the three types of wines. \\n \\n    -- I think that the initial data set had around 30 variables, but \\n       for some reason I only have the 13 dimensional version. \\n       I had a list of what the 30 or so variables were, but a.) \\n       I lost it, and b.), I would not know which 13 variables\\n       are included in the set.\\n \\n    -- The attributes are (dontated by Riccardo Leardi, \\n \\triclea@anchem.unige.it )\\n  \\t1) Alcohol\\n  \\t2) Malic acid\\n  \\t3) Ash\\n \\t4) Alcalinity of ash  \\n  \\t5) Magnesium\\n \\t6) Total phenols\\n  \\t7) Flavanoids\\n  \\t8) Nonflavanoid phenols\\n  \\t9) Proanthocyanins\\n \\t10)Color intensity\\n  \\t11)Hue\\n  \\t12)OD280/OD315 of diluted wines'),\n Document(metadata={'MaxNominalAttDistinctValues': 3.0, 'NumberOfClasses': 0.0, 'NumberOfFeatures': 5.0, 'NumberOfInstances': 125.0, 'NumberOfInstancesWithMissingValues': 0.0, 'NumberOfMissingValues': 0.0, 'NumberOfNumericFeatures': 3.0, 'NumberOfSymbolicFeatures': 2.0, 'Unnamed: 0': 134, 'description': '**Author**:   \\n**Source**: Unknown -   \\n**Please cite**:   \\n\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\\n Identifier attribute deleted.\\n\\n !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\\n NAME:  Sexual activity and the lifespan of male fruitflies\\n TYPE:  Designed (almost factorial) experiment\\n SIZE:  125 observations, 5 variables\\n \\n DESCRIPTIVE ABSTRACT:\\n A cost of increased reproduction in terms of reduced longevity has been\\n shown for female fruitflies, but not for males.  The flies used were an\\n outbred stock.  Sexual activity was manipulated by supplying individual\\n males with one or eight receptive virgin females per day.  The\\n longevity of these males was compared with that of two control types.\\n The first control consisted of two sets of individual males kept with\\n one or eight newly inseminated females.  Newly inseminated females will\\n not usually remate for at least two days, and thus served as a control\\n for any effect of competition with the male for food or space.  The\\n second control was a set of individual males kept with no females.\\n There were 25 males in each of the five groups, which were treated\\n identically in number of anaesthetizations (using CO2) and provision of\\n fresh food medium.\\n \\n SOURCE:\\n Figure 2 in the article \"Sexual Activity and the Lifespan of Male\\n Fruitflies\" by Linda Partridge and Marion Farquhar.  _Nature_, 294,\\n 580-581, 1981.\\n \\n VARIABLE DESCRIPTIONS:\\n Columns  Variable    Description\\n -------  --------    -----------\\n  1- 2    ID          Serial No. (1-25) within each group of 25\\n                      (the order in which data points were abstracted)\\n \\n  4       PARTNERS    Number of companions (0, 1 or 8)\\n \\n  6       TYPE        Type of companion\\n                        0: newly pregnant female\\n                        1: virgin female\\n                        9: not applicable (when PARTNERS=0)\\n \\n  8- 9    LONGEVITY   Lifespan, in days\\n \\n 11-14    THORAX      Length of thorax, in mm (x.xx)\\n \\n 16-17    SLEEP       Percentage of each day spent sleeping\\n \\n \\n SPECIAL NOTES:\\n `Compliance\\' of the males in the two experimental groups was documented\\n as follows:  On two days per week throughout the life of each\\n experimental male, the females that had been supplied as virgins to\\n that male were kept and examined for fertile eggs.  The insemination\\n rate declined from approximately 7 females/day at age one week to just\\n under 2/day at age eight weeks in the males supplied with eight virgin\\n females per day, and from just under 1/day at age one week to\\n approximately 0.6/day at age eight weeks in the males supplied with one\\n virgin female per day.  These `compliance\\' data were not supplied for\\n individual males, but the authors say that \"There were no significant\\n differences between the individual males within each experimental\\n group.\"\\n \\n STORY BEHIND THE DATA:\\n James Hanley found this dataset in _Nature_ and was attracted by the\\n way the raw data were presented in classical analysis of covariance\\n style in Figure 2.  He read the data points from the graphs and brought\\n them to the attention of a colleague with whom he was teaching the\\n applied statistics course.  Dr. Liddell thought that with only three\\n explanatory variables (THORAX, plus PARTNERS and TYPE to describe the\\n five groups), it would not be challenging enough as a data-analysis\\n project.  He suggested adding another variable.  James Hanley added\\n SLEEP, a variable not mentioned in the published article.  Teachers can\\n contact us about the construction of this variable.  (We prefer to\\n divulge the details at the end of the data-analysis project.)\\n \\n Further discussion of the background and pedagogical use of this\\n dataset can be found in Hanley (1983) and in Hanley and Shapiro\\n (1994).  To obtain the Hanley and Shapiro article, send the one-line\\n e-mail message:\\n send jse/v2n1/datasets.hanley\\n to the address archive@jse.stat.ncsu.edu\\n \\n PEDAGOGICAL NOTES:\\n This has been the most successful and the most memorable dataset we\\n have used in an \"applications of statistics\" course, which we have\\n taught for ten years.  The most common analysis techniques have been\\n analysis of variance, classical analysis of covariance, and multiple\\n regression.  Because the variable THORAX is so strong (it explains\\n about 1/3 of the variance in LONGEVITY), it is important to consider it\\n to increase the precision of between-group contrasts.  When students\\n first check and find that the distributions of thorax length, and in\\n particular, the mean thorax length, are very similar in the different\\n groups, many of them are willing to say (in epidemiological\\n terminology) that THORAX is not a confounding variable, and that it can\\n be omitted from the analysis.\\n \\n There is usually lively discussion about the primary contrast.  The\\n five groups and their special structure allow opportunities for\\n students to understand and verbalize what we mean by the term\\n \"statistical interaction.\"\\n \\n There is also much debate as to whether one should take the SLEEP\\n variable into account.  Some students say that it is an `intermediate\\'\\n variable.  Some students formally test the mean level of SLEEP across\\n groups, find one pair where there is a statistically significant\\n difference, and want to treat it as a confounding variable.  A few\\n students muse about how it was measured.\\n \\n There is heteroscedasticity in the LONGEVITY variable.\\n \\n One very observant student (now a professor) argued that THORAX cannot\\n be used as a predictor or explanatory variable for the LONGEVITY\\n outcome since fruitflies who die young may not be fully grown, i.e., it\\n is also an intermediate variable.  One Ph.D. student who had studied\\n entomology assured us that fruitflies do not grow longer after birth;\\n therefore, the THORAX length is not time-dependent!\\n \\n Curiously, the dataset has seldom been analyzed using techniques from\\n survival analysis.  The fact that there are no censored observations is\\n not really an excuse, and one could easily devise a way to introduce\\n censoring of LONGEVITY.\\n \\n REFERENCES:\\n Hanley, J. A. (1983), \"Appropriate Uses of Multivariate Analysis,\"\\n _Annual Review of Public Health_, 4, 155-180.\\n \\n Hanley, J. A., and Shapiro, S. H. (1994), \"Sexual Activity and the\\n Lifespan of Male Fruitflies:  A Dataset That Gets Attention,\" _Journal\\n of Statistics Education_, Volume 2, Number 1.\\n \\n SUBMITTED BY:\\n James A. Hanley and Stanley H. Shapiro\\n Department of Epidemiology and Biostatistics\\n McGill University\\n 1020 Pine Avenue West\\n Montreal, Quebec, H3A 1A2\\n Canada\\n tel: +1 (514) 398-6270 (JH) \\n      +1 (514) 398-6272 (SS)\\n fax: +1 (514) 398-4503\\n INJH@musicb.mcgill.ca, StanS@epid.lan.mcgill.ca', 'did': 199, 'features': '0 : [0 - PARTNERS (nominal)], 1 : [1 - TYPE (nominal)], 2 : [2 - THORAX (numeric)], 3 : [3 - SLEEP (numeric)], 4 : [4 - class (numeric)],', 'format': 'ARFF', 'name': 'fruitfly', 'qualities': 'AutoCorrelation : -16.653225806451612, CfsSubsetEval_DecisionStumpAUC : nan, CfsSubsetEval_DecisionStumpErrRate : nan, CfsSubsetEval_DecisionStumpKappa : nan, CfsSubsetEval_NaiveBayesAUC : nan, CfsSubsetEval_NaiveBayesErrRate : nan, CfsSubsetEval_NaiveBayesKappa : nan, CfsSubsetEval_kNN1NAUC : nan, CfsSubsetEval_kNN1NErrRate : nan, CfsSubsetEval_kNN1NKappa : nan, ClassEntropy : nan, DecisionStumpAUC : nan, DecisionStumpErrRate : nan, DecisionStumpKappa : nan, Dimensionality : 0.04, EquivalentNumberOfAtts : nan, J48.00001.AUC : nan, J48.00001.ErrRate : nan, J48.00001.Kappa : nan, J48.0001.AUC : nan, J48.0001.ErrRate : nan, J48.0001.Kappa : nan, J48.001.AUC : nan, J48.001.ErrRate : nan, J48.001.Kappa : nan, MajorityClassPercentage : nan, MajorityClassSize : nan, MaxAttributeEntropy : nan, MaxKurtosisOfNumericAtts : 3.1484095157236704, MaxMeansOfNumericAtts : 57.44, MaxMutualInformation : nan, MaxNominalAttDistinctValues : 3.0, MaxSkewnessOfNumericAtts : 1.5903052309118162, MaxStdDevOfNumericAtts : 17.563892580537072, MeanAttributeEntropy : nan, MeanKurtosisOfNumericAtts : 0.7789944524450039, MeanMeansOfNumericAtts : 27.241653333333332, MeanMutualInformation : nan, MeanNoiseToSignalRatio : nan, MeanNominalAttDistinctValues : 3.0, MeanSkewnessOfNumericAtts : 0.3135430433813126, MeanStdDevOfNumericAtts : 11.173398006246252, MinAttributeEntropy : nan, MinKurtosisOfNumericAtts : -0.410404642598019, MinMeansOfNumericAtts : 0.82096, MinMutualInformation : nan, MinNominalAttDistinctValues : 3.0, MinSkewnessOfNumericAtts : -0.6380573853536728, MinStdDevOfNumericAtts : 0.07745366981455389, MinorityClassPercentage : nan, MinorityClassSize : nan, NaiveBayesAUC : nan, NaiveBayesErrRate : nan, NaiveBayesKappa : nan, NumberOfBinaryFeatures : 0.0, NumberOfClasses : 0.0, NumberOfFeatures : 5.0, NumberOfInstances : 125.0, NumberOfInstancesWithMissingValues : 0.0, NumberOfMissingValues : 0.0, NumberOfNumericFeatures : 3.0, NumberOfSymbolicFeatures : 2.0, PercentageOfBinaryFeatures : 0.0, PercentageOfInstancesWithMissingValues : 0.0, PercentageOfMissingValues : 0.0, PercentageOfNumericFeatures : 60.0, PercentageOfSymbolicFeatures : 40.0, Quartile1AttributeEntropy : nan, Quartile1KurtosisOfNumericAtts : -0.410404642598019, Quartile1MeansOfNumericAtts : 0.82096, Quartile1MutualInformation : nan, Quartile1SkewnessOfNumericAtts : -0.6380573853536728, Quartile1StdDevOfNumericAtts : 0.07745366981455389, Quartile2AttributeEntropy : nan, Quartile2KurtosisOfNumericAtts : -0.4010215157906396, Quartile2MeansOfNumericAtts : 23.464, Quartile2MutualInformation : nan, Quartile2SkewnessOfNumericAtts : -0.011618715414205413, Quartile2StdDevOfNumericAtts : 15.878847768387132, Quartile3AttributeEntropy : nan, Quartile3KurtosisOfNumericAtts : 3.1484095157236704, Quartile3MeansOfNumericAtts : 57.44, Quartile3MutualInformation : nan, Quartile3SkewnessOfNumericAtts : 1.5903052309118162, Quartile3StdDevOfNumericAtts : 17.563892580537072, REPTreeDepth1AUC : nan, REPTreeDepth1ErrRate : nan, REPTreeDepth1Kappa : nan, REPTreeDepth2AUC : nan, REPTreeDepth2ErrRate : nan, REPTreeDepth2Kappa : nan, REPTreeDepth3AUC : nan, REPTreeDepth3ErrRate : nan, REPTreeDepth3Kappa : nan, RandomTreeDepth1AUC : nan, RandomTreeDepth1ErrRate : nan, RandomTreeDepth1Kappa : nan, RandomTreeDepth2AUC : nan, RandomTreeDepth2ErrRate : nan, RandomTreeDepth2Kappa : nan, RandomTreeDepth3AUC : nan, RandomTreeDepth3ErrRate : nan, RandomTreeDepth3Kappa : nan, StdvNominalAttDistinctValues : 0.0, kNN1NAUC : nan, kNN1NErrRate : nan, kNN1NKappa : nan,', 'status': 'active', 'uploader': 1, 'version': 1}, page_content='_Annual Review of Public Health_, 4, 155-180.\\n \\n Hanley, J. A., and Shapiro, S. H. (1994), \"Sexual Activity and the\\n Lifespan of Male Fruitflies:  A Dataset That Gets Attention,\" _Journal\\n of Statistics Education_, Volume 2, Number 1.\\n \\n SUBMITTED BY:\\n James A. Hanley and Stanley H. Shapiro\\n Department of Epidemiology and Biostatistics\\n McGill University\\n 1020 Pine Avenue West\\n Montreal, Quebec, H3A 1A2\\n Canada\\n tel: +1 (514) 398-6270 (JH) \\n      +1 (514) 398-6272 (SS)\\n fax: +1 (514) 398-4503')]</code>\n</pre> <pre><code>res[0].metadata\n</code></pre> <pre>\n<code>{'MajorityClassSize': 4208.0,\n 'MaxNominalAttDistinctValues': 12.0,\n 'MinorityClassSize': 3916.0,\n 'NumberOfClasses': 2.0,\n 'NumberOfFeatures': 23.0,\n 'NumberOfInstances': 8124.0,\n 'NumberOfInstancesWithMissingValues': 2480.0,\n 'NumberOfMissingValues': 2480.0,\n 'NumberOfNumericFeatures': 0.0,\n 'NumberOfSymbolicFeatures': 23.0,\n 'Unnamed: 0': 19,\n 'description': \"**Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \\n**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n\\n### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\\n\\n### Attributes Information\\n```\\n1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s \\n2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s \\n3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y \\n4. bruises?: bruises=t,no=f \\n5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s \\n6. gill-attachment: attached=a,descending=d,free=f,notched=n \\n7. gill-spacing: close=c,crowded=w,distant=d \\n8. gill-size: broad=b,narrow=n \\n9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y \\n10. stalk-shape: enlarging=e,tapering=t \\n11. stalk-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=? \\n12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s \\n13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s \\n14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \\n15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \\n16. veil-type: partial=p,universal=u \\n17. veil-color: brown=n,orange=o,white=w,yellow=y \\n18. ring-number: none=n,one=o,two=t \\n19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z \\n20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y \\n21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y \\n22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d\\n```\\n\\n### Relevant papers\\n\\nSchlimmer,J.S. (1987). Concept Acquisition Through Representational Adjustment (Technical Report 87-19). Doctoral disseration, Department of Information and Computer Science, University of California, Irvine. \\n\\nIba,W., Wogulis,J., &amp; Langley,P. (1988). Trading off Simplicity and Coverage in Incremental Concept Learning. In Proceedings of the 5th International Conference on Machine Learning, 73-79. Ann Arbor, Michigan: Morgan Kaufmann. \\n\\nDuch W, Adamczak R, Grabczewski K (1996) Extraction of logical rules from training data using backpropagation networks, in: Proc. of the The 1st Online Workshop on Soft Computing, 19-30.Aug.1996, pp. 25-30, [Web Link] \\n\\nDuch W, Adamczak R, Grabczewski K, Ishikawa M, Ueda H, Extraction of crisp logical rules using constrained backpropagation networks - comparison of two new approaches, in: Proc. of the European Symposium on Artificial Neural Networks (ESANN'97), Bruge, Belgium 16-18.4.1997.\",\n 'did': 24,\n 'features': '0 : [0 - cap-shape (nominal)], 1 : [1 - cap-surface (nominal)], 2 : [2 - cap-color (nominal)], 3 : [3 - bruises%3F (nominal)], 4 : [4 - odor (nominal)], 5 : [5 - gill-attachment (nominal)], 6 : [6 - gill-spacing (nominal)], 7 : [7 - gill-size (nominal)], 8 : [8 - gill-color (nominal)], 9 : [9 - stalk-shape (nominal)], 10 : [10 - stalk-root (nominal)], 11 : [11 - stalk-surface-above-ring (nominal)], 12 : [12 - stalk-surface-below-ring (nominal)], 13 : [13 - stalk-color-above-ring (nominal)], 14 : [14 - stalk-color-below-ring (nominal)], 15 : [15 - veil-type (nominal)], 16 : [16 - veil-color (nominal)], 17 : [17 - ring-number (nominal)], 18 : [18 - ring-type (nominal)], 19 : [19 - spore-print-color (nominal)], 20 : [20 - population (nominal)], 21 : [21 - habitat (nominal)], 22 : [22 - class (nominal)],',\n 'format': 'ARFF',\n 'name': 'mushroom',\n 'qualities': 'AutoCorrelation : 0.726332635725717, CfsSubsetEval_DecisionStumpAUC : 0.9910519616800724, CfsSubsetEval_DecisionStumpErrRate : 0.013047759724273756, CfsSubsetEval_DecisionStumpKappa : 0.9738461616958994, CfsSubsetEval_NaiveBayesAUC : 0.9910519616800724, CfsSubsetEval_NaiveBayesErrRate : 0.013047759724273756, CfsSubsetEval_NaiveBayesKappa : 0.9738461616958994, CfsSubsetEval_kNN1NAUC : 0.9910519616800724, CfsSubsetEval_kNN1NErrRate : 0.013047759724273756, CfsSubsetEval_kNN1NKappa : 0.9738461616958994, ClassEntropy : 0.9990678968724604, DecisionStumpAUC : 0.8894935275772204, DecisionStumpErrRate : 0.11324470704086657, DecisionStumpKappa : 0.77457574608175, Dimensionality : 0.002831117676021664, EquivalentNumberOfAtts : 5.0393135801657, J48.00001.AUC : 1.0, J48.00001.ErrRate : 0.0, J48.00001.Kappa : 1.0, J48.0001.AUC : 1.0, J48.0001.ErrRate : 0.0, J48.0001.Kappa : 1.0, J48.001.AUC : 1.0, J48.001.ErrRate : 0.0, J48.001.Kappa : 1.0, MajorityClassPercentage : 51.7971442639094, MajorityClassSize : 4208.0, MaxAttributeEntropy : 3.030432883772633, MaxKurtosisOfNumericAtts : nan, MaxMeansOfNumericAtts : nan, MaxMutualInformation : 0.906074977384, MaxNominalAttDistinctValues : 12.0, MaxSkewnessOfNumericAtts : nan, MaxStdDevOfNumericAtts : nan, MeanAttributeEntropy : 1.4092554739602103, MeanKurtosisOfNumericAtts : nan, MeanMeansOfNumericAtts : nan, MeanMutualInformation : 0.19825475850613955, MeanNoiseToSignalRatio : 6.108305922031972, MeanNominalAttDistinctValues : 5.130434782608695, MeanSkewnessOfNumericAtts : nan, MeanStdDevOfNumericAtts : nan, MinAttributeEntropy : -0.0, MinKurtosisOfNumericAtts : nan, MinMeansOfNumericAtts : nan, MinMutualInformation : 0.0, MinNominalAttDistinctValues : 1.0, MinSkewnessOfNumericAtts : nan, MinStdDevOfNumericAtts : nan, MinorityClassPercentage : 48.20285573609059, MinorityClassSize : 3916.0, NaiveBayesAUC : 0.9976229672941662, NaiveBayesErrRate : 0.04899064500246184, NaiveBayesKappa : 0.9015972799616292, NumberOfBinaryFeatures : 5.0, NumberOfClasses : 2.0, NumberOfFeatures : 23.0, NumberOfInstances : 8124.0, NumberOfInstancesWithMissingValues : 2480.0, NumberOfMissingValues : 2480.0, NumberOfNumericFeatures : 0.0, NumberOfSymbolicFeatures : 23.0, PercentageOfBinaryFeatures : 21.73913043478261, PercentageOfInstancesWithMissingValues : 30.526834071885773, PercentageOfMissingValues : 1.3272536552993814, PercentageOfNumericFeatures : 0.0, PercentageOfSymbolicFeatures : 100.0, Quartile1AttributeEntropy : 0.8286618104993447, Quartile1KurtosisOfNumericAtts : nan, Quartile1MeansOfNumericAtts : nan, Quartile1MutualInformation : 0.034184520425602494, Quartile1SkewnessOfNumericAtts : nan, Quartile1StdDevOfNumericAtts : nan, Quartile2AttributeEntropy : 1.467128011861462, Quartile2KurtosisOfNumericAtts : nan, Quartile2MeansOfNumericAtts : nan, Quartile2MutualInformation : 0.174606545183155, Quartile2SkewnessOfNumericAtts : nan, Quartile2StdDevOfNumericAtts : nan, Quartile3AttributeEntropy : 2.0533554351937426, Quartile3KurtosisOfNumericAtts : nan, Quartile3MeansOfNumericAtts : nan, Quartile3MutualInformation : 0.27510225484918505, Quartile3SkewnessOfNumericAtts : nan, Quartile3StdDevOfNumericAtts : nan, REPTreeDepth1AUC : 0.9999987256143267, REPTreeDepth1ErrRate : 0.00036927621861152144, REPTreeDepth1Kappa : 0.9992605118549308, REPTreeDepth2AUC : 0.9999987256143267, REPTreeDepth2ErrRate : 0.00036927621861152144, REPTreeDepth2Kappa : 0.9992605118549308, REPTreeDepth3AUC : 0.9999987256143267, REPTreeDepth3ErrRate : 0.00036927621861152144, REPTreeDepth3Kappa : 0.9992605118549308, RandomTreeDepth1AUC : 0.9995247148288974, RandomTreeDepth1ErrRate : 0.0004923682914820286, RandomTreeDepth1Kappa : 0.9990140245420991, RandomTreeDepth2AUC : 0.9995247148288974, RandomTreeDepth2ErrRate : 0.0004923682914820286, RandomTreeDepth2Kappa : 0.9990140245420991, RandomTreeDepth3AUC : 0.9995247148288974, RandomTreeDepth3ErrRate : 0.0004923682914820286, RandomTreeDepth3Kappa : 0.9990140245420991, StdvNominalAttDistinctValues : 3.1809710899501766, kNN1NAUC : 1.0, kNN1NErrRate : 0.0, kNN1NKappa : 1.0,',\n 'status': 'active',\n 'uploader': 1,\n 'version': 1}</code>\n</pre> <pre><code>print(res[0].page_content)\n</code></pre> <pre>\n<code>### Description\n\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\n\n### Source\n<pre><code>(a) Origin: \nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \n\n(b) Donor: \nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\n</code></pre>\n\n### Dataset description\n\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\n</code>\n</pre> <pre><code>output_df, ids_order = QueryProcessor(\n    query=query,\n    qa=qa_dataset,\n    type_of_query=config[\"type_of_data\"],\n    config=config,\n).get_result_from_query()\n</code></pre> <pre><code>ids_order\n</code></pre> <pre>\n<code>[24,\n 24,\n 294,\n 120,\n 120,\n 42,\n 188,\n 42,\n 187,\n 199,\n 183,\n 134,\n 23,\n 134,\n 287,\n 334,\n 335,\n 333,\n 42,\n 42,\n 287,\n 343,\n 8,\n 334,\n 24,\n 333,\n 179,\n 335,\n 61,\n 13]</code>\n</pre> <pre><code>output_df.head()\n</code></pre> id name Description OpenML URL Command 0 24 mushroom StdvNominalAttDistinctValues : 3.1809710899501... &lt;a href=\"https://www.openml.org/search?type=da... dataset = openml.datasets.get_dataset(24) 2 294 satellite_image Data Set Information: &lt;a href=\"https://www.openml.org/search?type=da... dataset = openml.datasets.get_dataset(294) 3 120 BNG(mushroom) RandomTreeDepth3ErrRate : 0.024243, RandomTree... &lt;a href=\"https://www.openml.org/search?type=da... dataset = openml.datasets.get_dataset(120) 5 42 soybean did - 42, name - soybean, version - 1, uploade... &lt;a href=\"https://www.openml.org/search?type=da... dataset = openml.datasets.get_dataset(42) 6 188 eucalyptus Kirsten Thomson and Robert J. McQueen (1996) M... &lt;a href=\"https://www.openml.org/search?type=da... dataset = openml.datasets.get_dataset(188)"},{"location":"Rag%20Pipeline/Developer%20Tutorials/load%20vectordb%20and%20get%20results/#load-the-chroma-db-and-get-retrieval-results-for-a-given-query","title":"Load the Chroma Db and get retrieval results for a given query","text":"<ul> <li>How would you load the Chroma Db and get retrieval results for a given query?</li> </ul>"},{"location":"Rag%20Pipeline/Developer%20Tutorials/load%20vectordb%20and%20get%20results/#just-get-documents","title":"Just get documents","text":""},{"location":"Rag%20Pipeline/Developer%20Tutorials/load%20vectordb%20and%20get%20results/#process-the-results-and-return-a-dataframe-instead","title":"Process the results and return a dataframe instead","text":""},{"location":"UI/api_reference/","title":"Api reference","text":""},{"location":"UI/api_reference/#ui_utils.LLMResponseParser","title":"<code>LLMResponseParser</code>","text":"<p>Description: Parse the response from the LLM service and update the columns based on the response.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>class LLMResponseParser:\n    \"\"\"\n    Description: Parse the response from the LLM service and update the columns based on the response.\n    \"\"\"\n\n    def __init__(self, llm_response):\n        self.llm_response = llm_response\n        self.subset_cols = [\"did\", \"name\"]\n        self.size_sort = None\n        self.classification_type = None\n        self.uploader_name = None\n\n    def process_size_attribute(self, attr_size: str):\n        size, sort = attr_size.split(\",\") if \",\" in attr_size else (attr_size, None)\n        if size == \"yes\":\n            self.subset_cols.append(\"NumberOfInstances\")\n        if sort:\n            self.size_sort = sort\n\n    def missing_values_attribute(self, attr_missing: str):\n        if attr_missing == \"yes\":\n            self.subset_cols.append(\"NumberOfMissingValues\")\n\n    def classification_type_attribute(self, attr_classification: str):\n        if attr_classification != \"none\":\n            self.subset_cols.append(\"NumberOfClasses\")\n            self.classification_type = attr_classification\n\n    def uploader_attribute(self, attr_uploader: str):\n        if attr_uploader != \"none\":\n            self.subset_cols.append(\"uploader\")\n            self.uploader_name = attr_uploader.split(\"=\")[1].strip()\n\n    def get_attributes_from_response(self):\n        attribute_processors = {\n            \"size_of_dataset\": self.process_size_attribute,\n            \"missing_values\": self.missing_values_attribute,\n            \"classification_type\": self.classification_type_attribute,\n            \"uploader\": self.uploader_attribute,\n        }\n\n        for attribute, value in self.llm_response.items():\n            if attribute in attribute_processors:\n                attribute_processors[attribute](value)\n\n    def update_subset_cols(self, metadata: pd.DataFrame):\n        \"\"\"\n        Description: Filter the metadata based on the updated subset columns and extra conditions\n        \"\"\"\n        if self.classification_type is not None:\n            if \"multi\" in self.classification_type:\n                metadata = metadata[metadata[\"NumberOfClasses\"] &gt; 2]\n            elif \"binary\" in self.classification_type:\n                metadata = metadata[metadata[\"NumberOfClasses\"] == 2]\n        if self.uploader_name is not None:\n            try:\n                uploader = int(self.uploader_name)\n                metadata = metadata[metadata[\"uploader\"] == uploader]\n            except:\n                pass\n\n        return metadata[self.subset_cols]\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.LLMResponseParser.update_subset_cols","title":"<code>update_subset_cols(metadata)</code>","text":"<p>Description: Filter the metadata based on the updated subset columns and extra conditions</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def update_subset_cols(self, metadata: pd.DataFrame):\n    \"\"\"\n    Description: Filter the metadata based on the updated subset columns and extra conditions\n    \"\"\"\n    if self.classification_type is not None:\n        if \"multi\" in self.classification_type:\n            metadata = metadata[metadata[\"NumberOfClasses\"] &gt; 2]\n        elif \"binary\" in self.classification_type:\n            metadata = metadata[metadata[\"NumberOfClasses\"] == 2]\n    if self.uploader_name is not None:\n        try:\n            uploader = int(self.uploader_name)\n            metadata = metadata[metadata[\"uploader\"] == uploader]\n        except:\n            pass\n\n    return metadata[self.subset_cols]\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.ResponseParser","title":"<code>ResponseParser</code>","text":"<p>Description : This classe is used to decide the order of operations and run the response parsing. It loads the paths, fetches the Query parsing LLM response, the rag response, loads the metadatas and then based on the config, decides the order in which to apply each of them.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>class ResponseParser:\n    \"\"\"\n    Description : This classe is used to decide the order of operations and run the response parsing.\n    It loads the paths, fetches the Query parsing LLM response, the rag response, loads the metadatas and then based on the config, decides the order in which to apply each of them.\n    \"\"\"\n\n    def __init__(self, query_type: str, apply_llm_before_rag: bool = False):\n        self.query_type = query_type\n        self.paths = self.load_paths()\n        self.rag_response = None\n        self.llm_response = None\n        self.apply_llm_before_rag = apply_llm_before_rag\n        self.database_filtered = None\n        self.structured_query_response = None\n\n    def load_paths(self):\n        \"\"\"\n        Description: Load paths from paths.json\n        \"\"\"\n        with open(\"paths.json\", \"r\") as file:\n            return json.load(file)\n\n    def fetch_llm_response(self, query: str):\n        \"\"\"\n        Description: Fetch the response from the query parsing LLM service as a json\n        \"\"\"\n        llm_response_path = self.paths[\"llm_response\"]\n        try:\n            self.llm_response = requests.get(\n                f\"{llm_response_path['docker']}{query}\"\n            ).json()\n        except:\n            self.llm_response = requests.get(\n                f\"{llm_response_path['local']}{query}\"\n            ).json()\n        return self.llm_response\n\n    def fetch_structured_query(self, query_type: str, query: str):\n        \"\"\"\n        Description: Fetch the response for a structured query from the LLM service as a JSON\n        \"\"\"\n        structured_response_path = self.paths[\"structured_query\"]\n        try:\n            self.structured_query_response = requests.get(\n                f\"{structured_response_path['docker']}{query}\",\n                json={\"query\": query},\n            ).json()\n        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n            # Print the error for debugging purposes\n            print(f\"Error occurred: {e}\")\n            # Set structured_query_response to None on error\n            self.structured_query_response = None\n        try:\n            self.structured_query_response = requests.get(\n                f\"{structured_response_path['local']}{query}\",\n                json={\"query\": query},\n            ).json()\n        except Exception as e:\n            # Print the error for debugging purposes\n            print(f\"Error occurred while fetching from local endpoint: {e}\")\n            # Set structured_query_response to None if the local request also fails\n            self.structured_query_response = None\n\n        return self.structured_query_response\n\n    def database_filter(self, filter_condition, collec):\n        \"\"\"\n        Apply database filter on the rag_response\n        \"\"\"\n        ids = list(map(str, self.rag_response[\"initial_response\"]))\n        self.database_filtered = collec.get(ids=ids, where=filter_condition)[\"ids\"]\n        self.database_filtered = list(map(int, self.database_filtered))\n        # print(self.database_filtered)\n        return self.database_filtered\n\n    def fetch_rag_response(self, query_type, query):\n        \"\"\"\n        Description: Fetch the response from RAG pipeline\n\n        \"\"\"\n        rag_response_path = self.paths[\"rag_response\"]\n        try:\n            self.rag_response = requests.get(\n                f\"{rag_response_path['docker']}{query_type.lower()}/{query}\",\n                json={\"query\": query, \"type\": query_type.lower()},\n            ).json()\n        except:\n            self.rag_response = requests.get(\n                f\"{rag_response_path['local']}{query_type.lower()}/{query}\",\n                json={\"query\": query, \"type\": query_type.lower()},\n            ).json()\n        ordered_set = self._order_results()\n        self.rag_response[\"initial_response\"] = ordered_set\n\n        return self.rag_response\n\n    def _order_results(self):\n        doc_set = set()\n        ordered_set = []\n        for docid in self.rag_response[\"initial_response\"]:\n            if docid not in doc_set:\n                ordered_set.append(docid)\n            doc_set.add(docid)\n        return ordered_set\n\n    def parse_and_update_response(self, metadata: pd.DataFrame):\n        \"\"\"\n         Description: Parse the response from the RAG and LLM services and update the metadata based on the response.\n         Decide which order to apply them\n         -  self.apply_llm_before_rag == False\n             - Metadata is filtered based on the rag response first and then by the Query parsing LLM\n        -  self.apply_llm_before_rag == False\n             - Metadata is filtered based by the Query parsing LLM first and the rag response second\n        - in case structured_query == true, take results are applying data filters.\n        \"\"\"\n        if self.apply_llm_before_rag is None or self.llm_response is None:\n            print(\"No LLM filter.\")\n            # print(self.rag_response, flush=True)\n            filtered_metadata = self._no_filter(metadata)\n\n            # print(filtered_metadata)\n            # if no llm response is required, return the initial response\n            return filtered_metadata\n\n        elif (\n            self.rag_response is not None and self.llm_response is not None\n        ) and not config[\"structured_query\"]:\n            if not self.apply_llm_before_rag:\n                filtered_metadata, llm_parser = self._rag_before_llm(metadata)\n\n                if self.query_type.lower() == \"dataset\":\n                    llm_parser.get_attributes_from_response()\n                    return llm_parser.update_subset_cols(filtered_metadata)\n\n            elif self.apply_llm_before_rag:\n                filtered_metadata = self._filter_before_rag(metadata)\n                return filtered_metadata\n\n        elif (\n            self.rag_response is not None and self.structured_query_response is not None\n        ):\n            col_name = [\n                \"status\",\n                \"NumberOfClasses\",\n                \"NumberOfFeatures\",\n                \"NumberOfInstances\",\n            ]\n            # print(self.structured_query_response)  # Only for debugging. Comment later.\n            if self.structured_query_response[0] is not None and isinstance(\n                self.structured_query_response[1], dict\n            ):\n                # Safely attempt to access the \"filter\" key in the first element\n\n                self._structured_query_on_success(metadata)\n\n            else:\n                filtered_metadata = self._structured_query_on_fail(metadata)\n                # print(\"Showing only rag response\")\n            return filtered_metadata[[\"did\", \"name\", *col_name]]\n\n    def _structured_query_on_fail(self, metadata):\n        filtered_metadata = metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n        return filtered_metadata\n\n    def _structured_query_on_success(self, metadata):\n        if (\n            self.structured_query_response[0].get(\"filter\", None)\n            and self.database_filtered\n        ):\n            filtered_metadata = metadata[metadata[\"did\"].isin(self.database_filtered)]\n            # print(\"Showing database filtered data\")\n        else:\n            filtered_metadata = metadata[\n                metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n            ]\n            # print(\n            #     \"Showing only rag response as filter is empty or none of the rag data satisfies filter conditions.\"\n            # )\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n    def _filter_before_rag(self, metadata):\n        print(\"LLM filter before RAG\")\n        llm_parser = LLMResponseParser(self.llm_response)\n        llm_parser.get_attributes_from_response()\n        filtered_metadata = llm_parser.update_subset_cols(metadata)\n        filtered_metadata = filtered_metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n        return filtered_metadata\n\n    def _rag_before_llm(self, metadata):\n        print(\"RAG before LLM filter.\")\n        filtered_metadata = metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n        llm_parser = LLMResponseParser(self.llm_response)\n        return filtered_metadata, llm_parser\n\n    def _no_filter(self, metadata):\n        filtered_metadata = metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n        return filtered_metadata\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.ResponseParser.database_filter","title":"<code>database_filter(filter_condition, collec)</code>","text":"<p>Apply database filter on the rag_response</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def database_filter(self, filter_condition, collec):\n    \"\"\"\n    Apply database filter on the rag_response\n    \"\"\"\n    ids = list(map(str, self.rag_response[\"initial_response\"]))\n    self.database_filtered = collec.get(ids=ids, where=filter_condition)[\"ids\"]\n    self.database_filtered = list(map(int, self.database_filtered))\n    # print(self.database_filtered)\n    return self.database_filtered\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.ResponseParser.fetch_llm_response","title":"<code>fetch_llm_response(query)</code>","text":"<p>Description: Fetch the response from the query parsing LLM service as a json</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_llm_response(self, query: str):\n    \"\"\"\n    Description: Fetch the response from the query parsing LLM service as a json\n    \"\"\"\n    llm_response_path = self.paths[\"llm_response\"]\n    try:\n        self.llm_response = requests.get(\n            f\"{llm_response_path['docker']}{query}\"\n        ).json()\n    except:\n        self.llm_response = requests.get(\n            f\"{llm_response_path['local']}{query}\"\n        ).json()\n    return self.llm_response\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.ResponseParser.fetch_rag_response","title":"<code>fetch_rag_response(query_type, query)</code>","text":"<p>Description: Fetch the response from RAG pipeline</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_rag_response(self, query_type, query):\n    \"\"\"\n    Description: Fetch the response from RAG pipeline\n\n    \"\"\"\n    rag_response_path = self.paths[\"rag_response\"]\n    try:\n        self.rag_response = requests.get(\n            f\"{rag_response_path['docker']}{query_type.lower()}/{query}\",\n            json={\"query\": query, \"type\": query_type.lower()},\n        ).json()\n    except:\n        self.rag_response = requests.get(\n            f\"{rag_response_path['local']}{query_type.lower()}/{query}\",\n            json={\"query\": query, \"type\": query_type.lower()},\n        ).json()\n    ordered_set = self._order_results()\n    self.rag_response[\"initial_response\"] = ordered_set\n\n    return self.rag_response\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.ResponseParser.fetch_structured_query","title":"<code>fetch_structured_query(query_type, query)</code>","text":"<p>Description: Fetch the response for a structured query from the LLM service as a JSON</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_structured_query(self, query_type: str, query: str):\n    \"\"\"\n    Description: Fetch the response for a structured query from the LLM service as a JSON\n    \"\"\"\n    structured_response_path = self.paths[\"structured_query\"]\n    try:\n        self.structured_query_response = requests.get(\n            f\"{structured_response_path['docker']}{query}\",\n            json={\"query\": query},\n        ).json()\n    except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n        # Print the error for debugging purposes\n        print(f\"Error occurred: {e}\")\n        # Set structured_query_response to None on error\n        self.structured_query_response = None\n    try:\n        self.structured_query_response = requests.get(\n            f\"{structured_response_path['local']}{query}\",\n            json={\"query\": query},\n        ).json()\n    except Exception as e:\n        # Print the error for debugging purposes\n        print(f\"Error occurred while fetching from local endpoint: {e}\")\n        # Set structured_query_response to None if the local request also fails\n        self.structured_query_response = None\n\n    return self.structured_query_response\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.ResponseParser.load_paths","title":"<code>load_paths()</code>","text":"<p>Description: Load paths from paths.json</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def load_paths(self):\n    \"\"\"\n    Description: Load paths from paths.json\n    \"\"\"\n    with open(\"paths.json\", \"r\") as file:\n        return json.load(file)\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.ResponseParser.parse_and_update_response","title":"<code>parse_and_update_response(metadata)</code>","text":"<p>Description: Parse the response from the RAG and LLM services and update the metadata based on the response.  Decide which order to apply them  -  self.apply_llm_before_rag == False      - Metadata is filtered based on the rag response first and then by the Query parsing LLM -  self.apply_llm_before_rag == False      - Metadata is filtered based by the Query parsing LLM first and the rag response second - in case structured_query == true, take results are applying data filters.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def parse_and_update_response(self, metadata: pd.DataFrame):\n    \"\"\"\n     Description: Parse the response from the RAG and LLM services and update the metadata based on the response.\n     Decide which order to apply them\n     -  self.apply_llm_before_rag == False\n         - Metadata is filtered based on the rag response first and then by the Query parsing LLM\n    -  self.apply_llm_before_rag == False\n         - Metadata is filtered based by the Query parsing LLM first and the rag response second\n    - in case structured_query == true, take results are applying data filters.\n    \"\"\"\n    if self.apply_llm_before_rag is None or self.llm_response is None:\n        print(\"No LLM filter.\")\n        # print(self.rag_response, flush=True)\n        filtered_metadata = self._no_filter(metadata)\n\n        # print(filtered_metadata)\n        # if no llm response is required, return the initial response\n        return filtered_metadata\n\n    elif (\n        self.rag_response is not None and self.llm_response is not None\n    ) and not config[\"structured_query\"]:\n        if not self.apply_llm_before_rag:\n            filtered_metadata, llm_parser = self._rag_before_llm(metadata)\n\n            if self.query_type.lower() == \"dataset\":\n                llm_parser.get_attributes_from_response()\n                return llm_parser.update_subset_cols(filtered_metadata)\n\n        elif self.apply_llm_before_rag:\n            filtered_metadata = self._filter_before_rag(metadata)\n            return filtered_metadata\n\n    elif (\n        self.rag_response is not None and self.structured_query_response is not None\n    ):\n        col_name = [\n            \"status\",\n            \"NumberOfClasses\",\n            \"NumberOfFeatures\",\n            \"NumberOfInstances\",\n        ]\n        # print(self.structured_query_response)  # Only for debugging. Comment later.\n        if self.structured_query_response[0] is not None and isinstance(\n            self.structured_query_response[1], dict\n        ):\n            # Safely attempt to access the \"filter\" key in the first element\n\n            self._structured_query_on_success(metadata)\n\n        else:\n            filtered_metadata = self._structured_query_on_fail(metadata)\n            # print(\"Showing only rag response\")\n        return filtered_metadata[[\"did\", \"name\", *col_name]]\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.UILoader","title":"<code>UILoader</code>","text":"<p>Description : Create the chat interface</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>class UILoader:\n    \"\"\"\n    Description : Create the chat interface\n    \"\"\"\n\n    def __init__(self, config_path):\n        with open(config_path, \"r\") as file:\n            # Load config\n            self.config = json.load(file)\n        # Paths and display information\n\n        # Load metadata chroma database for structured query\n        self.collec = load_chroma_metadata()\n\n        # Metadata paths\n        self.data_metadata_path = (\n            Path(config[\"data_dir\"]) / \"all_dataset_description.csv\"\n        )\n        self.flow_metadata_path = Path(config[\"data_dir\"]) / \"all_flow_description.csv\"\n\n        # Read metadata\n        self.data_metadata = pd.read_csv(self.data_metadata_path)\n        self.flow_metadata = pd.read_csv(self.flow_metadata_path)\n\n        # defaults\n        self.query_type = \"Dataset\"\n        self.llm_filter = False\n        self.paths = self.load_paths()\n        self.info = \"\"\"\n        &lt;p style='text-align: center; color: white;'&gt;Machine learning research should be easily accessible and reusable. &lt;a href = \"https://openml.org/\"&gt;OpenML&lt;/a&gt; is an open platform for sharing datasets, algorithms, and experiments - to learn how to learn better, together. &lt;/p&gt;\n        \"\"\"\n        self.logo = \"images/favicon.ico\"\n        self.chatbot_display = \"How do I do X using OpenML? / Find me a dataset about Y\"\n\n        if \"messages\" not in st.session_state:\n            st.session_state.messages = []\n\n    # container for company description and logo\n    def _generate_logo_header(\n        self,\n    ):\n\n        col1, col2 = st.columns([1, 4])\n        with col1:\n            st.image(self.logo, width=100)\n        with col2:\n            st.markdown(\n                self.info,\n                unsafe_allow_html=True,\n            )\n\n    def generate_complete_ui(self):\n\n        self._generate_logo_header()\n        chat_container = st.container()\n        # self.disclaimer_dialog()\n        with chat_container:\n            with st.form(key=\"chat_form\"):\n                user_input = st.text_input(\n                    label=\"Query\", placeholder=self.chatbot_display\n                )\n                query_type = st.selectbox(\n                    \"Select Query Type\",\n                    [\"General Query\", \"Dataset\", \"Flow\"],\n                    help=\"Are you looking for a dataset or a flow or just have a general query?\",\n                )\n                ai_filter = st.toggle(\n                    \"Use AI powered filtering\",\n                    value=True,\n                    help=\"Uses an AI model to identify what columns might be useful to you.\",\n                )\n                st.form_submit_button(label=\"Search\")\n\n            self.create_chat_interface(user_input=None)\n            if user_input:\n                self.create_chat_interface(\n                    user_input, query_type=query_type, ai_filter=ai_filter\n                )\n\n    def create_chat_interface(self, user_input, query_type=None, ai_filter=False):\n        \"\"\"\n        Description: Create the chat interface and display the chat history and results. Show the user input and the response from the OpenML Agent.\n\n        \"\"\"\n        self.query_type = query_type\n        self.ai_filter = ai_filter\n\n        if user_input is None:\n            with st.chat_message(name=\"ai\"):\n                st.write(\"OpenML Agent: \", \"Hello! How can I help you today?\")\n                st.write(\n                    \":warning: Note that results are powered by local LLM models and may not be accurate. Please refer to the official OpenML website for accurate information.\"\n                )\n\n        # Handle user input\n        if user_input:\n            self._handle_user_input(user_input, query_type)\n\n    def _handle_user_input(self, user_input, query_type):\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n        with st.spinner(\"Waiting for results...\"):\n            results = self.process_query_chat(user_input)\n\n        if not self.query_type == \"General Query\":\n            st.session_state.messages.append(\n                    {\"role\": \"OpenML Agent\", \"content\": results}\n                )\n        else:\n            self._stream_results(results)\n\n            # reverse messages to show the latest message at the top\n        reversed_messages = self._reverse_session_history()\n\n            # Display chat history\n        self._display_chat_history(query_type, reversed_messages)\n        self.create_download_button()\n\n    def _display_chat_history(self, query_type, reversed_messages):\n        for message in reversed_messages:\n            if query_type == \"General Query\":\n                pass\n            if message[\"role\"] == \"user\":\n                with st.chat_message(name=\"user\"):\n                    self.display_results(message[\"content\"], \"user\")\n            else:\n                with st.chat_message(name=\"ai\"):\n                    self.display_results(message[\"content\"], \"ai\")\n\n    def _reverse_session_history(self):\n        reversed_messages = []\n        for index in range(0, len(st.session_state.messages), 2):\n            reversed_messages.insert(0, st.session_state.messages[index])\n            reversed_messages.insert(1, st.session_state.messages[index + 1])\n        return reversed_messages\n\n    def _stream_results(self, results):\n        with st.spinner(\"Fetching results...\"):\n            with requests.get(results, stream=True) as r:\n                resp_contain = st.empty()\n                streamed_response = \"\"\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk:\n                        streamed_response += chunk.decode(\"utf-8\")\n                        resp_contain.markdown(streamed_response)\n                resp_contain.empty()\n            st.session_state.messages.append(\n                {\"role\": \"OpenML Agent\", \"content\": streamed_response}\n            )\n\n    @st.experimental_fragment()\n    def create_download_button(self):\n        data = \"\\n\".join(\n            [str(message[\"content\"]) for message in st.session_state.messages]\n        )\n        st.download_button(\n            label=\"Download chat history\",\n            data=data,\n            file_name=\"chat_history.txt\",\n        )\n\n    def display_results(self, initial_response, role):\n        \"\"\"\n        Description: Display the results in a DataFrame\n        \"\"\"\n        try:\n            st.dataframe(initial_response)\n        except:\n            st.write(initial_response)\n\n    # Function to handle query processing\n    def process_query_chat(self, query):\n        \"\"\"\n        Description: Process the query and return the results based on the query type and the LLM filter.\n\n        \"\"\"\n        apply_llm_before_rag = None if not self.llm_filter else False\n        response_parser = ResponseParser(\n            self.query_type, apply_llm_before_rag=apply_llm_before_rag\n        )\n\n        if self.query_type == \"Dataset\" or self.query_type == \"Flow\":\n            if not self.ai_filter:\n                response_parser.fetch_rag_response(self.query_type, query)\n                return response_parser.parse_and_update_response(self.data_metadata)\n            else:\n                # get structured query\n                self._display_structured_query_results(query, response_parser)\n\n            results = response_parser.parse_and_update_response(self.data_metadata)\n            return results\n\n        elif self.query_type == \"General Query\":\n            # Return documentation response path\n            return self.paths[\"documentation_query\"][\"local\"] + query\n\n    def _display_structured_query_results(self, query, response_parser):\n        response_parser.fetch_structured_query(self.query_type, query)\n        try:\n            # get rag response\n            # using original query instead of extracted topics.\n            response_parser.fetch_rag_response(\n                self.query_type,\n                response_parser.structured_query_response[0][\"query\"],\n            )\n\n            if response_parser.structured_query_response:\n                st.write(\n                    \"Detected Filter(s): \",\n                    json.dumps(\n                        response_parser.structured_query_response[0].get(\"filter\", None)\n                    ),\n                )\n            else:\n                st.write(\"Detected Filter(s): \", None)\n            if response_parser.structured_query_response[1].get(\"filter\"):\n                with st.spinner(\"Applying LLM Detected Filter(s)...\"):\n                    response_parser.database_filter(\n                        response_parser.structured_query_response[1][\"filter\"],\n                        collec,\n                    )\n        except:\n            # fallback to RAG response\n            response_parser.fetch_rag_response(self.query_type, query)\n\n    def load_paths(self):\n        \"\"\"\n        Description: Load paths from paths.json\n        \"\"\"\n        with open(\"paths.json\", \"r\") as file:\n            return json.load(file)\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.UILoader.create_chat_interface","title":"<code>create_chat_interface(user_input, query_type=None, ai_filter=False)</code>","text":"<p>Description: Create the chat interface and display the chat history and results. Show the user input and the response from the OpenML Agent.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def create_chat_interface(self, user_input, query_type=None, ai_filter=False):\n    \"\"\"\n    Description: Create the chat interface and display the chat history and results. Show the user input and the response from the OpenML Agent.\n\n    \"\"\"\n    self.query_type = query_type\n    self.ai_filter = ai_filter\n\n    if user_input is None:\n        with st.chat_message(name=\"ai\"):\n            st.write(\"OpenML Agent: \", \"Hello! How can I help you today?\")\n            st.write(\n                \":warning: Note that results are powered by local LLM models and may not be accurate. Please refer to the official OpenML website for accurate information.\"\n            )\n\n    # Handle user input\n    if user_input:\n        self._handle_user_input(user_input, query_type)\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.UILoader.display_results","title":"<code>display_results(initial_response, role)</code>","text":"<p>Description: Display the results in a DataFrame</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def display_results(self, initial_response, role):\n    \"\"\"\n    Description: Display the results in a DataFrame\n    \"\"\"\n    try:\n        st.dataframe(initial_response)\n    except:\n        st.write(initial_response)\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.UILoader.load_paths","title":"<code>load_paths()</code>","text":"<p>Description: Load paths from paths.json</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def load_paths(self):\n    \"\"\"\n    Description: Load paths from paths.json\n    \"\"\"\n    with open(\"paths.json\", \"r\") as file:\n        return json.load(file)\n</code></pre>"},{"location":"UI/api_reference/#ui_utils.UILoader.process_query_chat","title":"<code>process_query_chat(query)</code>","text":"<p>Description: Process the query and return the results based on the query type and the LLM filter.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def process_query_chat(self, query):\n    \"\"\"\n    Description: Process the query and return the results based on the query type and the LLM filter.\n\n    \"\"\"\n    apply_llm_before_rag = None if not self.llm_filter else False\n    response_parser = ResponseParser(\n        self.query_type, apply_llm_before_rag=apply_llm_before_rag\n    )\n\n    if self.query_type == \"Dataset\" or self.query_type == \"Flow\":\n        if not self.ai_filter:\n            response_parser.fetch_rag_response(self.query_type, query)\n            return response_parser.parse_and_update_response(self.data_metadata)\n        else:\n            # get structured query\n            self._display_structured_query_results(query, response_parser)\n\n        results = response_parser.parse_and_update_response(self.data_metadata)\n        return results\n\n    elif self.query_type == \"General Query\":\n        # Return documentation response path\n        return self.paths[\"documentation_query\"][\"local\"] + query\n</code></pre>"},{"location":"UI/api_reference/#ui.LLMResponseParser","title":"<code>LLMResponseParser</code>","text":"<p>Description: Parse the response from the LLM service and update the columns based on the response.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>class LLMResponseParser:\n    \"\"\"\n    Description: Parse the response from the LLM service and update the columns based on the response.\n    \"\"\"\n\n    def __init__(self, llm_response):\n        self.llm_response = llm_response\n        self.subset_cols = [\"did\", \"name\"]\n        self.size_sort = None\n        self.classification_type = None\n        self.uploader_name = None\n\n    def process_size_attribute(self, attr_size: str):\n        size, sort = attr_size.split(\",\") if \",\" in attr_size else (attr_size, None)\n        if size == \"yes\":\n            self.subset_cols.append(\"NumberOfInstances\")\n        if sort:\n            self.size_sort = sort\n\n    def missing_values_attribute(self, attr_missing: str):\n        if attr_missing == \"yes\":\n            self.subset_cols.append(\"NumberOfMissingValues\")\n\n    def classification_type_attribute(self, attr_classification: str):\n        if attr_classification != \"none\":\n            self.subset_cols.append(\"NumberOfClasses\")\n            self.classification_type = attr_classification\n\n    def uploader_attribute(self, attr_uploader: str):\n        if attr_uploader != \"none\":\n            self.subset_cols.append(\"uploader\")\n            self.uploader_name = attr_uploader.split(\"=\")[1].strip()\n\n    def get_attributes_from_response(self):\n        attribute_processors = {\n            \"size_of_dataset\": self.process_size_attribute,\n            \"missing_values\": self.missing_values_attribute,\n            \"classification_type\": self.classification_type_attribute,\n            \"uploader\": self.uploader_attribute,\n        }\n\n        for attribute, value in self.llm_response.items():\n            if attribute in attribute_processors:\n                attribute_processors[attribute](value)\n\n    def update_subset_cols(self, metadata: pd.DataFrame):\n        \"\"\"\n        Description: Filter the metadata based on the updated subset columns and extra conditions\n        \"\"\"\n        if self.classification_type is not None:\n            if \"multi\" in self.classification_type:\n                metadata = metadata[metadata[\"NumberOfClasses\"] &gt; 2]\n            elif \"binary\" in self.classification_type:\n                metadata = metadata[metadata[\"NumberOfClasses\"] == 2]\n        if self.uploader_name is not None:\n            try:\n                uploader = int(self.uploader_name)\n                metadata = metadata[metadata[\"uploader\"] == uploader]\n            except:\n                pass\n\n        return metadata[self.subset_cols]\n</code></pre>"},{"location":"UI/api_reference/#ui.LLMResponseParser.update_subset_cols","title":"<code>update_subset_cols(metadata)</code>","text":"<p>Description: Filter the metadata based on the updated subset columns and extra conditions</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def update_subset_cols(self, metadata: pd.DataFrame):\n    \"\"\"\n    Description: Filter the metadata based on the updated subset columns and extra conditions\n    \"\"\"\n    if self.classification_type is not None:\n        if \"multi\" in self.classification_type:\n            metadata = metadata[metadata[\"NumberOfClasses\"] &gt; 2]\n        elif \"binary\" in self.classification_type:\n            metadata = metadata[metadata[\"NumberOfClasses\"] == 2]\n    if self.uploader_name is not None:\n        try:\n            uploader = int(self.uploader_name)\n            metadata = metadata[metadata[\"uploader\"] == uploader]\n        except:\n            pass\n\n    return metadata[self.subset_cols]\n</code></pre>"},{"location":"UI/api_reference/#ui.ResponseParser","title":"<code>ResponseParser</code>","text":"<p>Description : This classe is used to decide the order of operations and run the response parsing. It loads the paths, fetches the Query parsing LLM response, the rag response, loads the metadatas and then based on the config, decides the order in which to apply each of them.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>class ResponseParser:\n    \"\"\"\n    Description : This classe is used to decide the order of operations and run the response parsing.\n    It loads the paths, fetches the Query parsing LLM response, the rag response, loads the metadatas and then based on the config, decides the order in which to apply each of them.\n    \"\"\"\n\n    def __init__(self, query_type: str, apply_llm_before_rag: bool = False):\n        self.query_type = query_type\n        self.paths = self.load_paths()\n        self.rag_response = None\n        self.llm_response = None\n        self.apply_llm_before_rag = apply_llm_before_rag\n        self.database_filtered = None\n        self.structured_query_response = None\n\n    def load_paths(self):\n        \"\"\"\n        Description: Load paths from paths.json\n        \"\"\"\n        with open(\"paths.json\", \"r\") as file:\n            return json.load(file)\n\n    def fetch_llm_response(self, query: str):\n        \"\"\"\n        Description: Fetch the response from the query parsing LLM service as a json\n        \"\"\"\n        llm_response_path = self.paths[\"llm_response\"]\n        try:\n            self.llm_response = requests.get(\n                f\"{llm_response_path['docker']}{query}\"\n            ).json()\n        except:\n            self.llm_response = requests.get(\n                f\"{llm_response_path['local']}{query}\"\n            ).json()\n        return self.llm_response\n\n    def fetch_structured_query(self, query_type: str, query: str):\n        \"\"\"\n        Description: Fetch the response for a structured query from the LLM service as a JSON\n        \"\"\"\n        structured_response_path = self.paths[\"structured_query\"]\n        try:\n            self.structured_query_response = requests.get(\n                f\"{structured_response_path['docker']}{query}\",\n                json={\"query\": query},\n            ).json()\n        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n            # Print the error for debugging purposes\n            print(f\"Error occurred: {e}\")\n            # Set structured_query_response to None on error\n            self.structured_query_response = None\n        try:\n            self.structured_query_response = requests.get(\n                f\"{structured_response_path['local']}{query}\",\n                json={\"query\": query},\n            ).json()\n        except Exception as e:\n            # Print the error for debugging purposes\n            print(f\"Error occurred while fetching from local endpoint: {e}\")\n            # Set structured_query_response to None if the local request also fails\n            self.structured_query_response = None\n\n        return self.structured_query_response\n\n    def database_filter(self, filter_condition, collec):\n        \"\"\"\n        Apply database filter on the rag_response\n        \"\"\"\n        ids = list(map(str, self.rag_response[\"initial_response\"]))\n        self.database_filtered = collec.get(ids=ids, where=filter_condition)[\"ids\"]\n        self.database_filtered = list(map(int, self.database_filtered))\n        # print(self.database_filtered)\n        return self.database_filtered\n\n    def fetch_rag_response(self, query_type, query):\n        \"\"\"\n        Description: Fetch the response from RAG pipeline\n\n        \"\"\"\n        rag_response_path = self.paths[\"rag_response\"]\n        try:\n            self.rag_response = requests.get(\n                f\"{rag_response_path['docker']}{query_type.lower()}/{query}\",\n                json={\"query\": query, \"type\": query_type.lower()},\n            ).json()\n        except:\n            self.rag_response = requests.get(\n                f\"{rag_response_path['local']}{query_type.lower()}/{query}\",\n                json={\"query\": query, \"type\": query_type.lower()},\n            ).json()\n        ordered_set = self._order_results()\n        self.rag_response[\"initial_response\"] = ordered_set\n\n        return self.rag_response\n\n    def _order_results(self):\n        doc_set = set()\n        ordered_set = []\n        for docid in self.rag_response[\"initial_response\"]:\n            if docid not in doc_set:\n                ordered_set.append(docid)\n            doc_set.add(docid)\n        return ordered_set\n\n    def parse_and_update_response(self, metadata: pd.DataFrame):\n        \"\"\"\n         Description: Parse the response from the RAG and LLM services and update the metadata based on the response.\n         Decide which order to apply them\n         -  self.apply_llm_before_rag == False\n             - Metadata is filtered based on the rag response first and then by the Query parsing LLM\n        -  self.apply_llm_before_rag == False\n             - Metadata is filtered based by the Query parsing LLM first and the rag response second\n        - in case structured_query == true, take results are applying data filters.\n        \"\"\"\n        if self.apply_llm_before_rag is None or self.llm_response is None:\n            print(\"No LLM filter.\")\n            # print(self.rag_response, flush=True)\n            filtered_metadata = self._no_filter(metadata)\n\n            # print(filtered_metadata)\n            # if no llm response is required, return the initial response\n            return filtered_metadata\n\n        elif (\n            self.rag_response is not None and self.llm_response is not None\n        ) and not config[\"structured_query\"]:\n            if not self.apply_llm_before_rag:\n                filtered_metadata, llm_parser = self._rag_before_llm(metadata)\n\n                if self.query_type.lower() == \"dataset\":\n                    llm_parser.get_attributes_from_response()\n                    return llm_parser.update_subset_cols(filtered_metadata)\n\n            elif self.apply_llm_before_rag:\n                filtered_metadata = self._filter_before_rag(metadata)\n                return filtered_metadata\n\n        elif (\n            self.rag_response is not None and self.structured_query_response is not None\n        ):\n            col_name = [\n                \"status\",\n                \"NumberOfClasses\",\n                \"NumberOfFeatures\",\n                \"NumberOfInstances\",\n            ]\n            # print(self.structured_query_response)  # Only for debugging. Comment later.\n            if self.structured_query_response[0] is not None and isinstance(\n                self.structured_query_response[1], dict\n            ):\n                # Safely attempt to access the \"filter\" key in the first element\n\n                self._structured_query_on_success(metadata)\n\n            else:\n                filtered_metadata = self._structured_query_on_fail(metadata)\n                # print(\"Showing only rag response\")\n            return filtered_metadata[[\"did\", \"name\", *col_name]]\n\n    def _structured_query_on_fail(self, metadata):\n        filtered_metadata = metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n        return filtered_metadata\n\n    def _structured_query_on_success(self, metadata):\n        if (\n            self.structured_query_response[0].get(\"filter\", None)\n            and self.database_filtered\n        ):\n            filtered_metadata = metadata[metadata[\"did\"].isin(self.database_filtered)]\n            # print(\"Showing database filtered data\")\n        else:\n            filtered_metadata = metadata[\n                metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n            ]\n            # print(\n            #     \"Showing only rag response as filter is empty or none of the rag data satisfies filter conditions.\"\n            # )\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n    def _filter_before_rag(self, metadata):\n        print(\"LLM filter before RAG\")\n        llm_parser = LLMResponseParser(self.llm_response)\n        llm_parser.get_attributes_from_response()\n        filtered_metadata = llm_parser.update_subset_cols(metadata)\n        filtered_metadata = filtered_metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n        return filtered_metadata\n\n    def _rag_before_llm(self, metadata):\n        print(\"RAG before LLM filter.\")\n        filtered_metadata = metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n        llm_parser = LLMResponseParser(self.llm_response)\n        return filtered_metadata, llm_parser\n\n    def _no_filter(self, metadata):\n        filtered_metadata = metadata[\n            metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n        ]\n        filtered_metadata[\"did\"] = pd.Categorical(\n            filtered_metadata[\"did\"],\n            categories=self.rag_response[\"initial_response\"],\n            ordered=True,\n        )\n        filtered_metadata = filtered_metadata.sort_values(\"did\").reset_index(drop=True)\n\n        return filtered_metadata\n</code></pre>"},{"location":"UI/api_reference/#ui.ResponseParser.database_filter","title":"<code>database_filter(filter_condition, collec)</code>","text":"<p>Apply database filter on the rag_response</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def database_filter(self, filter_condition, collec):\n    \"\"\"\n    Apply database filter on the rag_response\n    \"\"\"\n    ids = list(map(str, self.rag_response[\"initial_response\"]))\n    self.database_filtered = collec.get(ids=ids, where=filter_condition)[\"ids\"]\n    self.database_filtered = list(map(int, self.database_filtered))\n    # print(self.database_filtered)\n    return self.database_filtered\n</code></pre>"},{"location":"UI/api_reference/#ui.ResponseParser.fetch_llm_response","title":"<code>fetch_llm_response(query)</code>","text":"<p>Description: Fetch the response from the query parsing LLM service as a json</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_llm_response(self, query: str):\n    \"\"\"\n    Description: Fetch the response from the query parsing LLM service as a json\n    \"\"\"\n    llm_response_path = self.paths[\"llm_response\"]\n    try:\n        self.llm_response = requests.get(\n            f\"{llm_response_path['docker']}{query}\"\n        ).json()\n    except:\n        self.llm_response = requests.get(\n            f\"{llm_response_path['local']}{query}\"\n        ).json()\n    return self.llm_response\n</code></pre>"},{"location":"UI/api_reference/#ui.ResponseParser.fetch_rag_response","title":"<code>fetch_rag_response(query_type, query)</code>","text":"<p>Description: Fetch the response from RAG pipeline</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_rag_response(self, query_type, query):\n    \"\"\"\n    Description: Fetch the response from RAG pipeline\n\n    \"\"\"\n    rag_response_path = self.paths[\"rag_response\"]\n    try:\n        self.rag_response = requests.get(\n            f\"{rag_response_path['docker']}{query_type.lower()}/{query}\",\n            json={\"query\": query, \"type\": query_type.lower()},\n        ).json()\n    except:\n        self.rag_response = requests.get(\n            f\"{rag_response_path['local']}{query_type.lower()}/{query}\",\n            json={\"query\": query, \"type\": query_type.lower()},\n        ).json()\n    ordered_set = self._order_results()\n    self.rag_response[\"initial_response\"] = ordered_set\n\n    return self.rag_response\n</code></pre>"},{"location":"UI/api_reference/#ui.ResponseParser.fetch_structured_query","title":"<code>fetch_structured_query(query_type, query)</code>","text":"<p>Description: Fetch the response for a structured query from the LLM service as a JSON</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_structured_query(self, query_type: str, query: str):\n    \"\"\"\n    Description: Fetch the response for a structured query from the LLM service as a JSON\n    \"\"\"\n    structured_response_path = self.paths[\"structured_query\"]\n    try:\n        self.structured_query_response = requests.get(\n            f\"{structured_response_path['docker']}{query}\",\n            json={\"query\": query},\n        ).json()\n    except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n        # Print the error for debugging purposes\n        print(f\"Error occurred: {e}\")\n        # Set structured_query_response to None on error\n        self.structured_query_response = None\n    try:\n        self.structured_query_response = requests.get(\n            f\"{structured_response_path['local']}{query}\",\n            json={\"query\": query},\n        ).json()\n    except Exception as e:\n        # Print the error for debugging purposes\n        print(f\"Error occurred while fetching from local endpoint: {e}\")\n        # Set structured_query_response to None if the local request also fails\n        self.structured_query_response = None\n\n    return self.structured_query_response\n</code></pre>"},{"location":"UI/api_reference/#ui.ResponseParser.load_paths","title":"<code>load_paths()</code>","text":"<p>Description: Load paths from paths.json</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def load_paths(self):\n    \"\"\"\n    Description: Load paths from paths.json\n    \"\"\"\n    with open(\"paths.json\", \"r\") as file:\n        return json.load(file)\n</code></pre>"},{"location":"UI/api_reference/#ui.ResponseParser.parse_and_update_response","title":"<code>parse_and_update_response(metadata)</code>","text":"<p>Description: Parse the response from the RAG and LLM services and update the metadata based on the response.  Decide which order to apply them  -  self.apply_llm_before_rag == False      - Metadata is filtered based on the rag response first and then by the Query parsing LLM -  self.apply_llm_before_rag == False      - Metadata is filtered based by the Query parsing LLM first and the rag response second - in case structured_query == true, take results are applying data filters.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def parse_and_update_response(self, metadata: pd.DataFrame):\n    \"\"\"\n     Description: Parse the response from the RAG and LLM services and update the metadata based on the response.\n     Decide which order to apply them\n     -  self.apply_llm_before_rag == False\n         - Metadata is filtered based on the rag response first and then by the Query parsing LLM\n    -  self.apply_llm_before_rag == False\n         - Metadata is filtered based by the Query parsing LLM first and the rag response second\n    - in case structured_query == true, take results are applying data filters.\n    \"\"\"\n    if self.apply_llm_before_rag is None or self.llm_response is None:\n        print(\"No LLM filter.\")\n        # print(self.rag_response, flush=True)\n        filtered_metadata = self._no_filter(metadata)\n\n        # print(filtered_metadata)\n        # if no llm response is required, return the initial response\n        return filtered_metadata\n\n    elif (\n        self.rag_response is not None and self.llm_response is not None\n    ) and not config[\"structured_query\"]:\n        if not self.apply_llm_before_rag:\n            filtered_metadata, llm_parser = self._rag_before_llm(metadata)\n\n            if self.query_type.lower() == \"dataset\":\n                llm_parser.get_attributes_from_response()\n                return llm_parser.update_subset_cols(filtered_metadata)\n\n        elif self.apply_llm_before_rag:\n            filtered_metadata = self._filter_before_rag(metadata)\n            return filtered_metadata\n\n    elif (\n        self.rag_response is not None and self.structured_query_response is not None\n    ):\n        col_name = [\n            \"status\",\n            \"NumberOfClasses\",\n            \"NumberOfFeatures\",\n            \"NumberOfInstances\",\n        ]\n        # print(self.structured_query_response)  # Only for debugging. Comment later.\n        if self.structured_query_response[0] is not None and isinstance(\n            self.structured_query_response[1], dict\n        ):\n            # Safely attempt to access the \"filter\" key in the first element\n\n            self._structured_query_on_success(metadata)\n\n        else:\n            filtered_metadata = self._structured_query_on_fail(metadata)\n            # print(\"Showing only rag response\")\n        return filtered_metadata[[\"did\", \"name\", *col_name]]\n</code></pre>"},{"location":"UI/api_reference/#ui.UILoader","title":"<code>UILoader</code>","text":"<p>Description : Create the chat interface</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>class UILoader:\n    \"\"\"\n    Description : Create the chat interface\n    \"\"\"\n\n    def __init__(self, config_path):\n        with open(config_path, \"r\") as file:\n            # Load config\n            self.config = json.load(file)\n        # Paths and display information\n\n        # Load metadata chroma database for structured query\n        self.collec = load_chroma_metadata()\n\n        # Metadata paths\n        self.data_metadata_path = (\n            Path(config[\"data_dir\"]) / \"all_dataset_description.csv\"\n        )\n        self.flow_metadata_path = Path(config[\"data_dir\"]) / \"all_flow_description.csv\"\n\n        # Read metadata\n        self.data_metadata = pd.read_csv(self.data_metadata_path)\n        self.flow_metadata = pd.read_csv(self.flow_metadata_path)\n\n        # defaults\n        self.query_type = \"Dataset\"\n        self.llm_filter = False\n        self.paths = self.load_paths()\n        self.info = \"\"\"\n        &lt;p style='text-align: center; color: white;'&gt;Machine learning research should be easily accessible and reusable. &lt;a href = \"https://openml.org/\"&gt;OpenML&lt;/a&gt; is an open platform for sharing datasets, algorithms, and experiments - to learn how to learn better, together. &lt;/p&gt;\n        \"\"\"\n        self.logo = \"images/favicon.ico\"\n        self.chatbot_display = \"How do I do X using OpenML? / Find me a dataset about Y\"\n\n        if \"messages\" not in st.session_state:\n            st.session_state.messages = []\n\n    # container for company description and logo\n    def _generate_logo_header(\n        self,\n    ):\n\n        col1, col2 = st.columns([1, 4])\n        with col1:\n            st.image(self.logo, width=100)\n        with col2:\n            st.markdown(\n                self.info,\n                unsafe_allow_html=True,\n            )\n\n    def generate_complete_ui(self):\n\n        self._generate_logo_header()\n        chat_container = st.container()\n        # self.disclaimer_dialog()\n        with chat_container:\n            with st.form(key=\"chat_form\"):\n                user_input = st.text_input(\n                    label=\"Query\", placeholder=self.chatbot_display\n                )\n                query_type = st.selectbox(\n                    \"Select Query Type\",\n                    [\"General Query\", \"Dataset\", \"Flow\"],\n                    help=\"Are you looking for a dataset or a flow or just have a general query?\",\n                )\n                ai_filter = st.toggle(\n                    \"Use AI powered filtering\",\n                    value=True,\n                    help=\"Uses an AI model to identify what columns might be useful to you.\",\n                )\n                st.form_submit_button(label=\"Search\")\n\n            self.create_chat_interface(user_input=None)\n            if user_input:\n                self.create_chat_interface(\n                    user_input, query_type=query_type, ai_filter=ai_filter\n                )\n\n    def create_chat_interface(self, user_input, query_type=None, ai_filter=False):\n        \"\"\"\n        Description: Create the chat interface and display the chat history and results. Show the user input and the response from the OpenML Agent.\n\n        \"\"\"\n        self.query_type = query_type\n        self.ai_filter = ai_filter\n\n        if user_input is None:\n            with st.chat_message(name=\"ai\"):\n                st.write(\"OpenML Agent: \", \"Hello! How can I help you today?\")\n                st.write(\n                    \":warning: Note that results are powered by local LLM models and may not be accurate. Please refer to the official OpenML website for accurate information.\"\n                )\n\n        # Handle user input\n        if user_input:\n            self._handle_user_input(user_input, query_type)\n\n    def _handle_user_input(self, user_input, query_type):\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n        with st.spinner(\"Waiting for results...\"):\n            results = self.process_query_chat(user_input)\n\n        if not self.query_type == \"General Query\":\n            st.session_state.messages.append(\n                    {\"role\": \"OpenML Agent\", \"content\": results}\n                )\n        else:\n            self._stream_results(results)\n\n            # reverse messages to show the latest message at the top\n        reversed_messages = self._reverse_session_history()\n\n            # Display chat history\n        self._display_chat_history(query_type, reversed_messages)\n        self.create_download_button()\n\n    def _display_chat_history(self, query_type, reversed_messages):\n        for message in reversed_messages:\n            if query_type == \"General Query\":\n                pass\n            if message[\"role\"] == \"user\":\n                with st.chat_message(name=\"user\"):\n                    self.display_results(message[\"content\"], \"user\")\n            else:\n                with st.chat_message(name=\"ai\"):\n                    self.display_results(message[\"content\"], \"ai\")\n\n    def _reverse_session_history(self):\n        reversed_messages = []\n        for index in range(0, len(st.session_state.messages), 2):\n            reversed_messages.insert(0, st.session_state.messages[index])\n            reversed_messages.insert(1, st.session_state.messages[index + 1])\n        return reversed_messages\n\n    def _stream_results(self, results):\n        with st.spinner(\"Fetching results...\"):\n            with requests.get(results, stream=True) as r:\n                resp_contain = st.empty()\n                streamed_response = \"\"\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk:\n                        streamed_response += chunk.decode(\"utf-8\")\n                        resp_contain.markdown(streamed_response)\n                resp_contain.empty()\n            st.session_state.messages.append(\n                {\"role\": \"OpenML Agent\", \"content\": streamed_response}\n            )\n\n    @st.experimental_fragment()\n    def create_download_button(self):\n        data = \"\\n\".join(\n            [str(message[\"content\"]) for message in st.session_state.messages]\n        )\n        st.download_button(\n            label=\"Download chat history\",\n            data=data,\n            file_name=\"chat_history.txt\",\n        )\n\n    def display_results(self, initial_response, role):\n        \"\"\"\n        Description: Display the results in a DataFrame\n        \"\"\"\n        try:\n            st.dataframe(initial_response)\n        except:\n            st.write(initial_response)\n\n    # Function to handle query processing\n    def process_query_chat(self, query):\n        \"\"\"\n        Description: Process the query and return the results based on the query type and the LLM filter.\n\n        \"\"\"\n        apply_llm_before_rag = None if not self.llm_filter else False\n        response_parser = ResponseParser(\n            self.query_type, apply_llm_before_rag=apply_llm_before_rag\n        )\n\n        if self.query_type == \"Dataset\" or self.query_type == \"Flow\":\n            if not self.ai_filter:\n                response_parser.fetch_rag_response(self.query_type, query)\n                return response_parser.parse_and_update_response(self.data_metadata)\n            else:\n                # get structured query\n                self._display_structured_query_results(query, response_parser)\n\n            results = response_parser.parse_and_update_response(self.data_metadata)\n            return results\n\n        elif self.query_type == \"General Query\":\n            # Return documentation response path\n            return self.paths[\"documentation_query\"][\"local\"] + query\n\n    def _display_structured_query_results(self, query, response_parser):\n        response_parser.fetch_structured_query(self.query_type, query)\n        try:\n            # get rag response\n            # using original query instead of extracted topics.\n            response_parser.fetch_rag_response(\n                self.query_type,\n                response_parser.structured_query_response[0][\"query\"],\n            )\n\n            if response_parser.structured_query_response:\n                st.write(\n                    \"Detected Filter(s): \",\n                    json.dumps(\n                        response_parser.structured_query_response[0].get(\"filter\", None)\n                    ),\n                )\n            else:\n                st.write(\"Detected Filter(s): \", None)\n            if response_parser.structured_query_response[1].get(\"filter\"):\n                with st.spinner(\"Applying LLM Detected Filter(s)...\"):\n                    response_parser.database_filter(\n                        response_parser.structured_query_response[1][\"filter\"],\n                        collec,\n                    )\n        except:\n            # fallback to RAG response\n            response_parser.fetch_rag_response(self.query_type, query)\n\n    def load_paths(self):\n        \"\"\"\n        Description: Load paths from paths.json\n        \"\"\"\n        with open(\"paths.json\", \"r\") as file:\n            return json.load(file)\n</code></pre>"},{"location":"UI/api_reference/#ui.UILoader.create_chat_interface","title":"<code>create_chat_interface(user_input, query_type=None, ai_filter=False)</code>","text":"<p>Description: Create the chat interface and display the chat history and results. Show the user input and the response from the OpenML Agent.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def create_chat_interface(self, user_input, query_type=None, ai_filter=False):\n    \"\"\"\n    Description: Create the chat interface and display the chat history and results. Show the user input and the response from the OpenML Agent.\n\n    \"\"\"\n    self.query_type = query_type\n    self.ai_filter = ai_filter\n\n    if user_input is None:\n        with st.chat_message(name=\"ai\"):\n            st.write(\"OpenML Agent: \", \"Hello! How can I help you today?\")\n            st.write(\n                \":warning: Note that results are powered by local LLM models and may not be accurate. Please refer to the official OpenML website for accurate information.\"\n            )\n\n    # Handle user input\n    if user_input:\n        self._handle_user_input(user_input, query_type)\n</code></pre>"},{"location":"UI/api_reference/#ui.UILoader.display_results","title":"<code>display_results(initial_response, role)</code>","text":"<p>Description: Display the results in a DataFrame</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def display_results(self, initial_response, role):\n    \"\"\"\n    Description: Display the results in a DataFrame\n    \"\"\"\n    try:\n        st.dataframe(initial_response)\n    except:\n        st.write(initial_response)\n</code></pre>"},{"location":"UI/api_reference/#ui.UILoader.load_paths","title":"<code>load_paths()</code>","text":"<p>Description: Load paths from paths.json</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def load_paths(self):\n    \"\"\"\n    Description: Load paths from paths.json\n    \"\"\"\n    with open(\"paths.json\", \"r\") as file:\n        return json.load(file)\n</code></pre>"},{"location":"UI/api_reference/#ui.UILoader.process_query_chat","title":"<code>process_query_chat(query)</code>","text":"<p>Description: Process the query and return the results based on the query type and the LLM filter.</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def process_query_chat(self, query):\n    \"\"\"\n    Description: Process the query and return the results based on the query type and the LLM filter.\n\n    \"\"\"\n    apply_llm_before_rag = None if not self.llm_filter else False\n    response_parser = ResponseParser(\n        self.query_type, apply_llm_before_rag=apply_llm_before_rag\n    )\n\n    if self.query_type == \"Dataset\" or self.query_type == \"Flow\":\n        if not self.ai_filter:\n            response_parser.fetch_rag_response(self.query_type, query)\n            return response_parser.parse_and_update_response(self.data_metadata)\n        else:\n            # get structured query\n            self._display_structured_query_results(query, response_parser)\n\n        results = response_parser.parse_and_update_response(self.data_metadata)\n        return results\n\n    elif self.query_type == \"General Query\":\n        # Return documentation response path\n        return self.paths[\"documentation_query\"][\"local\"] + query\n</code></pre>"},{"location":"UI/frontend/","title":"Frontend Overview","text":"<ul> <li>This page is only an overview. Please refer to the api reference for more detailed information.</li> <li>Currently the frontend is based on Streamlit. The hope is to integrate it with the OpenML website in the future.</li> <li>This is what it looks like at the moment : </li> <li>This component runs the Streamlit frontend. It is the UI that you see when you navigate to <code>http://localhost:8501</code>.</li> <li>You can start it by running <code>cd frontend &amp;&amp; streamlit run ui.py &amp;</code></li> </ul>"},{"location":"UI/frontend/#design-methodology","title":"Design Methodology","text":"<ul> <li>The main point to note here is that the UI is responsible for all the post-processing of the results, including the displayed metadata information etc. </li> <li>The RAG pipeline only returns IDs of the relevant datasets and then it is upto the frontend to decide what to do with it. This was a conscious choice as the final objective was to let elasticsearch handle the results.</li> <li>This includes the logic for filtering the metadata, applying the filters obtained from the query parsing LLM and also what to do with the output of the RAG pipeline.</li> </ul>"},{"location":"UI/frontend/#main-logic","title":"Main logic","text":"<ul> <li>Streamlit is used for displaying the results. A selectbox is used for the user to select what kind of data they want and then a text_input box is used for them to enter their query.</li> <li>To make it easier to see what is happening, a spinning indicator with text (eg; Waiting for LLM results) was also added.</li> <li>Once the query is entered, the RAG pipeline is sent the query as a get request. </li> <li>Once the results of the RAG pipeline are obtained, the resulting list of IDs is queried from the metadata files (to be replaced with elasticsearch later) and then the relevant data is displayed.</li> <li>Now it is possible for the query parsing LLM to read the query and infer the columns that the user finds relevant. (eg: \"find me a dataset with multiple classes\" would enable the filters where <code>num_classes &gt;=2</code>).</li> </ul>"},{"location":"UI/frontend/#pathsjson","title":"paths.json","text":"<ul> <li>Configure this file if any of the endpoints change.</li> </ul>"},{"location":"UI/frontend/#uipy","title":"ui.py","text":"<ul> <li>This is where all the above logic is executed and displayed using Streamlit.</li> </ul>"},{"location":"UI/frontend/#ui_utilspy","title":"ui_utils.py","text":"<ul> <li>This is where all the logic is defined.</li> <li>Query filtering<ul> <li>During the processing of the RAG pipeline data ingestion pipeline, the metadata for all the datasets are saved as a csv file <code>Path(config[\"data_dir\"]) / \"all_dataset_description.csv\"</code>. This file contains information like number of classes, number of instances, authors etc. </li> <li>Using this file, it is possible to \"filter\" out what is needed and decide which columns to show.</li> <li>The <code>ResponseParser</code> and <code>LLMResponseParser</code> classes are probably what you are looking for if you want to modify the behavior of how the filters are created and used.</li> </ul> </li> <li>RAG pipeline</li> <li>The RAG pipeline is used to get the relevant IDs for the query.</li> <li>Feedback</li> <li>For now feedback is collected in a feedback.json file. This can be changed to something more meaningful later on.</li> </ul>"},{"location":"evaluation/","title":"Evaluating the AI search","text":"<ul> <li>The challenge with evaluation in this case was the lack of labels. To solve that, we created a simple streamlit app that let us label datasets according to a few tags. </li> <li>The evaluation pipeline runs the entire RAG + Query LLM pipeline on the subset of labelled data. The RAG does not have access to the entire OpenML database but just the subset that was labelled.</li> </ul>"},{"location":"evaluation/#manual-labelling","title":"Manual labelling","text":""},{"location":"evaluation/#streamlit-labelling-app","title":"Streamlit labelling app","text":"<ul> <li>Refer to labelling app for more information.</li> </ul>"},{"location":"evaluation/#merging-labels","title":"Merging labels","text":"<ul> <li>Since there were multiple people who labelled the datasets, it was useful to have a script that would merge them to create a single dataframe. </li> <li>The labels were generated per person using the labelling app and then merged into a single consistent dataframe using this script.</li> <li>Refer to merging labels for more information.</li> </ul>"},{"location":"evaluation/#consistency-evaluation","title":"Consistency evaluation","text":"<ul> <li>Since multiple people labelled the same dataset differently, Kohn's Kappa score was used to evaluate the consistency of the labelling. A value of ~4.5 was obtained, which shows moderate consistency. </li> </ul>"},{"location":"evaluation/#running-the-evaluation","title":"Running the evaluation","text":"<ul> <li>Refer to run training  for more information</li> </ul>"},{"location":"evaluation/api_reference/","title":"Api reference","text":""},{"location":"evaluation/api_reference/#consistency-evaluation","title":"Consistency evaluation","text":""},{"location":"evaluation/api_reference/#streamlit-labelling-app","title":"Streamlit labelling app","text":"<p>Small tool for labeling data.</p> <p>pip install streamlit streamlit run labellingapp.py</p> <p>Expects the metadata csv and the topic csv in the <code>data</code> directory.</p>"},{"location":"evaluation/api_reference/#labellingapp.update_this_relevancy","title":"<code>update_this_relevancy(var_, topic_)</code>","text":"<p>Helper function to bind the variables to scope.</p> Source code in <code>tools/labellingapp.py</code> <pre><code>def update_this_relevancy(var_, topic_):\n    \"\"\"Helper function to bind the variables to scope.\"\"\"\n    return lambda: update_relevancy(f\"q{var_}\", topic_)\n</code></pre>"},{"location":"evaluation/api_reference/#merging-labels","title":"Merging labels","text":""},{"location":"evaluation/api_reference/#merge_labels.merge_labels","title":"<code>merge_labels()</code>","text":"<p>Description : Merge labels from multiple JSON label files into a single dictionary.</p> Source code in <code>tools/merge_labels.py</code> <pre><code>def merge_labels() -&gt; dict[Any, list]:\n    \"\"\"\n    Description : Merge labels from multiple JSON label files into a single dictionary.\n    \"\"\"\n    # Read all files and merge them into a single dictionary\n    merged_labels = defaultdict(set)\n    for file in all_files:\n        with open(file, \"r\") as f:\n            try:\n                data = json.load(f)\n                for key, values in data.items():\n                    merged_labels[key].update(values)\n            except json.JSONDecodeError:\n                print(f\"Error reading {file}\")\n    # Remove empty lists\n    merged_labels = {k: list(v) for k, v in merged_labels.items() if v}\n\n    # Reverse the dictionary so we have topic -&gt; [dataset_ids]\n    reversed_labels = defaultdict(set)\n    for key, values in merged_labels.items():\n        for value in values:\n            reversed_labels[value].add(key)\n\n    # Convert sets to lists for each value\n    return {k: list(v) for k, v in reversed_labels.items()}\n</code></pre>"},{"location":"evaluation/api_reference/#run-batch-training","title":"Run Batch Training","text":""},{"location":"evaluation/api_reference/#run_all_training.ExperimentRunner","title":"<code>ExperimentRunner</code>","text":"<p>Description: This class is used to run all the experiments. If you want to modify any behavior, change the functions in this class according to what you want. You may also want to check out ResponseParser.</p> Source code in <code>evaluation/training_utils.py</code> <pre><code>class ExperimentRunner:\n    \"\"\"\n    Description: This class is used to run all the experiments. If you want to modify any behavior, change the functions in this class according to what you want.\n    You may also want to check out ResponseParser.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        eval_path,\n        queries,\n        list_of_embedding_models,\n        list_of_llm_models,\n        types_of_llm_apply=None,\n        subset_ids=None,\n        use_cached_experiment=False,\n        custom_name=None,\n    ):\n        if types_of_llm_apply is None:\n            types_of_llm_apply = [True, False, None]\n        self.config = config\n        self.eval_path = eval_path\n        self.queries = queries\n        self.list_of_embedding_models = list_of_embedding_models\n        self.list_of_llm_models = list_of_llm_models\n        self.subset_ids = subset_ids\n        self.use_cached_experiment = use_cached_experiment\n        self.custom_name = custom_name\n        self.types_of_llm_apply = types_of_llm_apply\n\n    def run_experiments(self):\n        # across all embedding models\n        for embedding_model in tqdm(\n            self.list_of_embedding_models,\n            desc=\"Embedding Models\",\n        ):\n            main_experiment_directory = (\n                self.eval_path / f\"{process_embedding_model_name_hf(embedding_model)}\"\n            )\n\n            os.makedirs(main_experiment_directory, exist_ok=True)\n\n            # update the config with the new experiment directories\n            self.config[\"data_dir\"] = str(main_experiment_directory)\n            self.config[\"persist_dir\"] = str(main_experiment_directory / \"chroma_db\")\n\n            # save training details and config in a dataframe\n            config_df = pd.DataFrame.from_dict(\n                self.config, orient=\"index\"\n            ).reset_index()\n            config_df.columns = [\"Hyperparameter\", \"Value\"]\n\n            # load the persistent database using ChromaDB\n            client = chromadb.PersistentClient(path=self.config[\"persist_dir\"])\n\n            # Note : I was not sure how to move this to the next loop, we need the QA setup going forward..\n            # Check if the chroma db as well as metadata files exist.\n            if os.path.exists(self.config[\"persist_dir\"]) and os.path.exists(\n                main_experiment_directory / \"all_dataset_description.csv\"\n            ):\n                # load the qa from the persistent database if it exists. Disabling training does this for us.\n                self.config[\"training\"] = False\n\n                qa_dataset_handler = QASetup(\n                    config=self.config,\n                    data_type=self.config[\"type_of_data\"],\n                    client=client,\n                    subset_ids=self.subset_ids,\n                )\n\n                qa_dataset, _ = qa_dataset_handler.setup_vector_db_and_qa()\n                self.config[\"training\"] = True\n            else:\n                self.config[\"training\"] = True\n                qa_dataset_handler = QASetup(\n                    config=self.config,\n                    data_type=self.config[\"type_of_data\"],\n                    client=client,\n                    subset_ids=self.subset_ids,\n                )\n\n                qa_dataset, _ = qa_dataset_handler.setup_vector_db_and_qa()\n\n            # across all llm models\n            for llm_model in tqdm(self.list_of_llm_models, desc=\"LLM Models\"):\n                # update the config with the new embedding and llm models\n                self.config[\"embedding_model\"] = embedding_model\n                self.config[\"llm_model\"] = llm_model\n\n                # create a new experiment directory using a combination of the embedding model and llm model names\n                experiment_name = f\"{process_embedding_model_name_hf(embedding_model)}_{process_llm_model_name_ollama(llm_model)}\"\n                if self.custom_name is not None:\n                    experiment_path = (\n                        # main_experiment_directory / (self.custom_name + experiment_name)\n                        main_experiment_directory\n                        / f\"{self.custom_name}@{experiment_name}\"\n                    )\n                else:\n                    experiment_path = main_experiment_directory / experiment_name\n                os.makedirs(experiment_path, exist_ok=True)\n                config_df.to_csv(experiment_path / \"config.csv\", index=False)\n\n                # we do not want to run the models again for no reason. So we use existing caches if they exit.\n                if self.use_cached_experiment and os.path.exists(\n                    experiment_path / \"results.csv\"\n                ):\n                    print(\n                        f\"Experiment {experiment_name} already exists. Skipping... To disable this behavior, set use_cached_experiment = False\"\n                    )\n                    continue\n                else:\n                    data_metadata_path = (\n                        Path(self.config[\"data_dir\"]) / \"all_dataset_description.csv\"\n                    )\n                    data_metadata = pd.read_csv(data_metadata_path)\n\n                    combined_df = self.aggregate_multiple_queries(\n                        qa_dataset=qa_dataset,\n                        data_metadata=data_metadata,\n                        types_of_llm_apply=self.types_of_llm_apply,\n                    )\n\n                    combined_df.to_csv(experiment_path / \"results.csv\")\n\n    def aggregate_multiple_queries(self, qa_dataset, data_metadata, types_of_llm_apply):\n        \"\"\"\n        Description: Aggregate the results of multiple queries into a single dataframe and count the number of times a dataset appears in the results. This was done here and not in evaluate to make it a little easier to manage as each of them requires a different chroma_db and config\n        \"\"\"\n\n        combined_results = pd.DataFrame()\n\n        # Initialize the ResponseParser once per query type\n        response_parsers = {\n            apply_llm: ResponseParser(\n                query_type=self.config[\"type_of_data\"], apply_llm_before_rag=apply_llm\n            )\n            for apply_llm in types_of_llm_apply\n        }\n\n        for query in tqdm(self.queries, total=len(self.queries), leave=True):\n            for apply_llm_before_rag in types_of_llm_apply:\n                combined_results = self.run_query(\n                    apply_llm_before_rag,\n                    combined_results,\n                    data_metadata,\n                    qa_dataset,\n                    query,\n                    response_parsers,\n                )\n\n        # Concatenate all collected DataFrames at once\n        # combined_df = pd.concat(combined_results, ignore_index=True)\n\n        return combined_results\n\n    def run_query(\n        self,\n        apply_llm_before_rag,\n        combined_results,\n        data_metadata,\n        qa_dataset,\n        query,\n        response_parsers,\n    ):\n        response_parser = response_parsers[apply_llm_before_rag]\n        result_data_frame, _ = QueryProcessor(\n            query=query,\n            qa=qa_dataset,\n            type_of_query=\"dataset\",\n            config=self.config,\n        ).get_result_from_query()\n        response_parser.rag_response = {\n            \"initial_response\": result_data_frame[\"id\"].to_list()\n        }\n        response_parser.fetch_llm_response(query)\n        result_data_frame = response_parser.parse_and_update_response(\n            data_metadata\n        ).copy()[[\"did\", \"name\"]]\n        result_data_frame[\"query\"] = query\n        result_data_frame[\"llm_model\"] = self.config[\"llm_model\"]\n        result_data_frame[\"embedding_model\"] = self.config[\"embedding_model\"]\n        result_data_frame[\"llm_before_rag\"] = apply_llm_before_rag\n        combined_results = pd.concat(\n            [combined_results, result_data_frame], ignore_index=True\n        )\n        return combined_results\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.ExperimentRunner.aggregate_multiple_queries","title":"<code>aggregate_multiple_queries(qa_dataset, data_metadata, types_of_llm_apply)</code>","text":"<p>Description: Aggregate the results of multiple queries into a single dataframe and count the number of times a dataset appears in the results. This was done here and not in evaluate to make it a little easier to manage as each of them requires a different chroma_db and config</p> Source code in <code>evaluation/training_utils.py</code> <pre><code>def aggregate_multiple_queries(self, qa_dataset, data_metadata, types_of_llm_apply):\n    \"\"\"\n    Description: Aggregate the results of multiple queries into a single dataframe and count the number of times a dataset appears in the results. This was done here and not in evaluate to make it a little easier to manage as each of them requires a different chroma_db and config\n    \"\"\"\n\n    combined_results = pd.DataFrame()\n\n    # Initialize the ResponseParser once per query type\n    response_parsers = {\n        apply_llm: ResponseParser(\n            query_type=self.config[\"type_of_data\"], apply_llm_before_rag=apply_llm\n        )\n        for apply_llm in types_of_llm_apply\n    }\n\n    for query in tqdm(self.queries, total=len(self.queries), leave=True):\n        for apply_llm_before_rag in types_of_llm_apply:\n            combined_results = self.run_query(\n                apply_llm_before_rag,\n                combined_results,\n                data_metadata,\n                qa_dataset,\n                query,\n                response_parsers,\n            )\n\n    # Concatenate all collected DataFrames at once\n    # combined_df = pd.concat(combined_results, ignore_index=True)\n\n    return combined_results\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.ResponseParser","title":"<code>ResponseParser</code>","text":"<p>               Bases: <code>ResponseParser</code></p> Source code in <code>evaluation/training_utils.py</code> <pre><code>class ResponseParser(ResponseParser):\n    def load_paths(self):\n        \"\"\"\n        Description: Load paths from paths.json\n        \"\"\"\n        with open(\"../frontend/paths.json\", \"r\") as file:\n            return json.load(file)\n\n    def parse_and_update_response(self, metadata):\n        \"\"\"\n        Description: Parse the response from the RAG and LLM services and update the metadata based on the response\n        \"\"\"\n        if self.rag_response is not None and self.llm_response is not None:\n            if not self.apply_llm_before_rag:\n                filtered_metadata = metadata[\n                    metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n                ]\n                llm_parser = LLMResponseParser(self.llm_response)\n                llm_parser.subset_cols = [\"did\", \"name\"]\n\n                if self.query_type.lower() == \"dataset\":\n                    llm_parser.get_attributes_from_response()\n                    return llm_parser.update_subset_cols(filtered_metadata)\n\n            elif self.apply_llm_before_rag:\n                llm_parser = LLMResponseParser(self.llm_response)\n                llm_parser.subset_cols = [\"did\", \"name\"]\n                llm_parser.get_attributes_from_response()\n                filtered_metadata = llm_parser.update_subset_cols(metadata)\n\n                return filtered_metadata[\n                    filtered_metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n                ]\n\n            elif self.apply_llm_before_rag is None:\n                # if no llm response is required, return the initial response\n                return metadata\n        elif (\n            self.rag_response is not None and self.structured_query_response is not None\n        ):\n            return metadata[[\"did\", \"name\"]]\n        else:\n            return metadata\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.ResponseParser.load_paths","title":"<code>load_paths()</code>","text":"<p>Description: Load paths from paths.json</p> Source code in <code>evaluation/training_utils.py</code> <pre><code>def load_paths(self):\n    \"\"\"\n    Description: Load paths from paths.json\n    \"\"\"\n    with open(\"../frontend/paths.json\", \"r\") as file:\n        return json.load(file)\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.ResponseParser.parse_and_update_response","title":"<code>parse_and_update_response(metadata)</code>","text":"<p>Description: Parse the response from the RAG and LLM services and update the metadata based on the response</p> Source code in <code>evaluation/training_utils.py</code> <pre><code>def parse_and_update_response(self, metadata):\n    \"\"\"\n    Description: Parse the response from the RAG and LLM services and update the metadata based on the response\n    \"\"\"\n    if self.rag_response is not None and self.llm_response is not None:\n        if not self.apply_llm_before_rag:\n            filtered_metadata = metadata[\n                metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n            ]\n            llm_parser = LLMResponseParser(self.llm_response)\n            llm_parser.subset_cols = [\"did\", \"name\"]\n\n            if self.query_type.lower() == \"dataset\":\n                llm_parser.get_attributes_from_response()\n                return llm_parser.update_subset_cols(filtered_metadata)\n\n        elif self.apply_llm_before_rag:\n            llm_parser = LLMResponseParser(self.llm_response)\n            llm_parser.subset_cols = [\"did\", \"name\"]\n            llm_parser.get_attributes_from_response()\n            filtered_metadata = llm_parser.update_subset_cols(metadata)\n\n            return filtered_metadata[\n                filtered_metadata[\"did\"].isin(self.rag_response[\"initial_response\"])\n            ]\n\n        elif self.apply_llm_before_rag is None:\n            # if no llm response is required, return the initial response\n            return metadata\n    elif (\n        self.rag_response is not None and self.structured_query_response is not None\n    ):\n        return metadata[[\"did\", \"name\"]]\n    else:\n        return metadata\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.exp_0","title":"<code>exp_0(process_query_elastic_search, eval_path, query_key_dict)</code>","text":"<p>EXPERIMENT 0 Get results from elastic search</p> Source code in <code>evaluation/experiments.py</code> <pre><code>def exp_0(process_query_elastic_search, eval_path, query_key_dict):\n    \"\"\"\n    EXPERIMENT 0\n    Get results from elastic search\n    \"\"\"\n    # cols = ,did,name,query,llm_model,embedding_model,llm_before_rag\n    # for every query, get the results from elastic search\n    if not os.path.exists(eval_path / \"elasticsearch\" / \"elasticsearch\"):\n        os.makedirs(eval_path / \"elasticsearch\" / \"elasticsearch\")\n    output_file_path = eval_path / \"elasticsearch\" / \"elasticsearch\" / \"results.csv\"\n    # check if the file exists and skip\n    if os.path.exists(output_file_path) == False:\n        with open(output_file_path, \"w\") as f:\n            f.write(\"did,name,query,llm_model,embedding_model,llm_before_rag\\n\")\n\n            # Use ThreadPoolExecutor to parallelize requests\n            with ThreadPoolExecutor(max_workers=10) as executor:\n                # Start a future for each query\n                futures = {\n                    executor.submit(\n                        process_query_elastic_search, query, dataset_id\n                    ): query\n                    for query, dataset_id in query_key_dict.items()\n                }\n\n                for future in tqdm(as_completed(futures), total=len(futures)):\n                    result = future.result()\n                    # Save the results to a CSV file\n                    for id, query in result:\n                        f.write(f\"{id},None,{query},es,es,None\\n\")\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.exp_1","title":"<code>exp_1(eval_path, config, list_of_embedding_models, list_of_llm_models, subset_ids, query_key_dict)</code>","text":"<p>EXPERIMENT 1 Main evaluation loop that is used to run the base experiments using different models and embeddings. Takes into account the following: original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing list_of_embedding_models = [     \"BAAI/bge-large-en-v1.5\",     \"BAAI/bge-base-en-v1.5\",     \"Snowflake/snowflake-arctic-embed-l\", ] list_of_llm_models = [\"llama3\", \"phi3\"] types_of_llm_apply : llm applied as filter before the RAG pipeline, llm applied as reranker after the RAG pipeline, llm not used at all</p> Source code in <code>evaluation/experiments.py</code> <pre><code>def exp_1(\n    eval_path,\n    config,\n    list_of_embedding_models,\n    list_of_llm_models,\n    subset_ids,\n    query_key_dict,\n):\n    \"\"\"\n    EXPERIMENT 1\n    Main evaluation loop that is used to run the base experiments using different models and embeddings.\n    Takes into account the following:\n    original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing\n    list_of_embedding_models = [\n        \"BAAI/bge-large-en-v1.5\",\n        \"BAAI/bge-base-en-v1.5\",\n        \"Snowflake/snowflake-arctic-embed-l\",\n    ]\n    list_of_llm_models = [\"llama3\", \"phi3\"]\n    types_of_llm_apply : llm applied as filter before the RAG pipeline, llm applied as reranker after the RAG pipeline, llm not used at all\n    \"\"\"\n\n    expRunner = ExperimentRunner(\n        config=config,\n        eval_path=eval_path,\n        queries=query_key_dict.keys(),\n        list_of_embedding_models=list_of_embedding_models,\n        list_of_llm_models=list_of_llm_models,\n        subset_ids=subset_ids,\n        use_cached_experiment=True,\n    )\n    expRunner.run_experiments()\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.exp_2","title":"<code>exp_2(eval_path, config, subset_ids, query_key_dict)</code>","text":"<p>EXPERIMENT 2 Evaluating temperature = 1 (default was 0.95) Takes into account the following: original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing list_of_embedding_models = [     \"BAAI/bge-large-en-v1.5\", ] list_of_llm_models = [\"llama3\"] types_of_llm_apply : llm applied as filter before the RAG pipeline, llm applied as reranker after the RAG pipeline, llm not used at all</p> Source code in <code>evaluation/experiments.py</code> <pre><code>def exp_2(eval_path, config, subset_ids, query_key_dict):\n    \"\"\"\n    EXPERIMENT 2\n    Evaluating temperature = 1 (default was 0.95)\n    Takes into account the following:\n    original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing\n    list_of_embedding_models = [\n        \"BAAI/bge-large-en-v1.5\",\n    ]\n    list_of_llm_models = [\"llama3\"]\n    types_of_llm_apply : llm applied as filter before the RAG pipeline, llm applied as reranker after the RAG pipeline, llm not used at all\n    \"\"\"\n\n    list_of_embedding_models = [\n        \"BAAI/bge-large-en-v1.5\",\n    ]\n    list_of_llm_models = [\"llama3\"]\n    config[\"temperature\"] = 1\n\n    expRunner = ExperimentRunner(\n        config=config,\n        eval_path=eval_path,\n        queries=query_key_dict.keys(),\n        list_of_embedding_models=list_of_embedding_models,\n        list_of_llm_models=list_of_llm_models,\n        subset_ids=subset_ids,\n        use_cached_experiment=True,\n        custom_name=\"temperature_1\",\n    )\n    expRunner.run_experiments()\n\n    # reset the temperature to the default value\n    config[\"temperature\"] = 0.95\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.exp_3","title":"<code>exp_3(eval_path, config, subset_ids, query_key_dict)</code>","text":"<p>EXPERIMENT 3 Evaluating search type [mmr, similarity_score_threshold] (default was similarity) Takes into account the following: original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing list_of_embedding_models = [     \"BAAI/bge-large-en-v1.5\", ] list_of_llm_models = [\"llama3\"] types_of_llm_apply : llm applied as reranker after the RAG pipeline</p> Source code in <code>evaluation/experiments.py</code> <pre><code>def exp_3(eval_path, config, subset_ids, query_key_dict):\n    \"\"\"\n    EXPERIMENT 3\n    Evaluating search type [mmr, similarity_score_threshold] (default was similarity)\n    Takes into account the following:\n    original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing\n    list_of_embedding_models = [\n        \"BAAI/bge-large-en-v1.5\",\n    ]\n    list_of_llm_models = [\"llama3\"]\n    types_of_llm_apply : llm applied as reranker after the RAG pipeline\n    \"\"\"\n\n    list_of_embedding_models = [\n        \"BAAI/bge-large-en-v1.5\",\n    ]\n    list_of_llm_models = [\"llama3\"]\n    types_of_llm_apply = [False]\n    types_of_search = [\"mmr\", \"similarity_score_threshold\"]\n\n    for type_of_search in types_of_search:\n        config[\"search_type\"] = type_of_search\n        expRunner = ExperimentRunner(\n            config=config,\n            eval_path=eval_path,\n            queries=query_key_dict.keys(),\n            list_of_embedding_models=list_of_embedding_models,\n            list_of_llm_models=list_of_llm_models,\n            subset_ids=subset_ids,\n            use_cached_experiment=True,\n            custom_name=f\"{type_of_search}_search\",\n            types_of_llm_apply=types_of_llm_apply,\n        )\n        expRunner.run_experiments()\n\n    # reset the search type to the default value\n    config[\"search_type\"] = \"similarity\"\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.exp_4","title":"<code>exp_4(eval_path, config, subset_ids, query_key_dict)</code>","text":"<p>EXPERIMENT 4 Evaluating chunk size. The default is 1000, trying out 512,128 Takes into account the following: original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing list_of_embedding_models = [     \"BAAI/bge-large-en-v1.5\", ] list_of_llm_models = [\"llama3\"] types_of_llm_apply : llm applied as reranker after the RAG pipeline</p> Source code in <code>evaluation/experiments.py</code> <pre><code>def exp_4(eval_path, config, subset_ids, query_key_dict):\n    \"\"\"\n    EXPERIMENT 4\n    Evaluating chunk size. The default is 1000, trying out 512,128\n    Takes into account the following:\n    original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing\n    list_of_embedding_models = [\n        \"BAAI/bge-large-en-v1.5\",\n    ]\n    list_of_llm_models = [\"llama3\"]\n    types_of_llm_apply : llm applied as reranker after the RAG pipeline\n    \"\"\"\n\n    list_of_embedding_models = [\n        \"BAAI/bge-large-en-v1.5\",\n    ]\n    list_of_llm_models = [\"llama3\"]\n    types_of_llm_apply = [False]\n    types_of_chunk = [512, 128]\n    for type_of_chunk in types_of_chunk:\n        config[\"chunk_size\"] = type_of_chunk\n        expRunner = ExperimentRunner(\n            config=config,\n            eval_path=eval_path,\n            queries=query_key_dict.keys(),\n            list_of_embedding_models=list_of_embedding_models,\n            list_of_llm_models=list_of_llm_models,\n            subset_ids=subset_ids,\n            use_cached_experiment=True,\n            custom_name=f\"{type_of_chunk}_chunk\",\n            types_of_llm_apply=types_of_llm_apply,\n        )\n        expRunner.run_experiments()\n\n    # reset the search type to the default value\n    config[\"chunk_size\"] = 1000\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.get_queries","title":"<code>get_queries(query_templates, load_eval_queries)</code>","text":"<p>Get queries from the dataset templates and format it</p> Source code in <code>evaluation/training_utils.py</code> <pre><code>def get_queries(query_templates, load_eval_queries):\n    \"\"\"\n    Get queries from the dataset templates and format it\n    \"\"\"\n    query_key_dict = {}\n    for template in query_templates:\n        for row in load_eval_queries.itertuples():\n            new_query = f\"{template} {row[1]}\".strip()\n            if new_query not in query_key_dict:\n                query_key_dict[new_query.strip()] = row[2]\n    return query_key_dict\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.ollama_setup","title":"<code>ollama_setup(list_of_llm_models)</code>","text":"<p>Description: Setup Ollama server and pull the llm_model that is being used</p> Source code in <code>evaluation/training_utils.py</code> <pre><code>def ollama_setup(list_of_llm_models: list):\n    \"\"\"\n    Description: Setup Ollama server and pull the llm_model that is being used\n    \"\"\"\n    os.system(\"ollama serve&amp;\")\n    print(\"Waiting for Ollama server to be active...\")\n    while os.system(\"ollama list | grep 'NAME'\") == \"\":\n        pass\n\n    for llm_model in list_of_llm_models:\n        os.system(f\"ollama pull {llm_model}\")\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.process_embedding_model_name_hf","title":"<code>process_embedding_model_name_hf(name)</code>","text":"<p>Description: This function processes the name of the embedding model from Hugging Face to use as experiment name.</p> <p>Input: name (str) - name of the embedding model from Hugging Face.</p> <p>Returns: name (str) - processed name of the embedding model.</p> Source code in <code>evaluation/training_utils.py</code> <pre><code>def process_embedding_model_name_hf(name: str) -&gt; str:\n    \"\"\"\n    Description: This function processes the name of the embedding model from Hugging Face to use as experiment name.\n\n    Input: name (str) - name of the embedding model from Hugging Face.\n\n    Returns: name (str) - processed name of the embedding model.\n    \"\"\"\n    return name.replace(\"/\", \"_\")\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.process_llm_model_name_ollama","title":"<code>process_llm_model_name_ollama(name)</code>","text":"<p>Description: This function processes the name of the llm model from Ollama to use as experiment name.</p> <p>Input: name (str) - name of the llm model from Ollama.</p> <p>Returns: name (str) - processed name of the llm model.</p> Source code in <code>evaluation/training_utils.py</code> <pre><code>def process_llm_model_name_ollama(name: str) -&gt; str:\n    \"\"\"\n    Description: This function processes the name of the llm model from Ollama to use as experiment name.\n\n    Input: name (str) - name of the llm model from Ollama.\n\n    Returns: name (str) - processed name of the llm model.\n    \"\"\"\n    return name.replace(\":\", \"_\")\n</code></pre>"},{"location":"evaluation/api_reference/#run_all_training.process_query_elastic_search","title":"<code>process_query_elastic_search(query, dataset_id)</code>","text":"<p>Get the results from elastic search opemml server</p> Source code in <code>evaluation/training_utils.py</code> <pre><code>def process_query_elastic_search(query, dataset_id):\n    \"\"\"\n    Get the results from elastic search opemml server\n    \"\"\"\n    res = get_elastic_search_results(query)\n    ids = [val[\"_id\"] for val in res]\n    return [(id, query) for id in ids]\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation-utils","title":"Evaluation Utils","text":""},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor","title":"<code>EvaluationProcessor</code>","text":"<p>Description: Process all the evaluated results, add the required metrics and save results as a csv/generate plots</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>class EvaluationProcessor:\n    \"\"\"\n    Description: Process all the evaluated results, add the required metrics and save results as a csv/generate plots\n    \"\"\"\n\n    def __init__(self, eval_path, metrics=None, sort_by=\"precision\"):\n        if metrics is None:\n            metrics = [\"precision\", \"recall\", \"map\"]\n        self.eval_path = eval_path\n        self.load_eval_queries = self.load_queries_from_csv()\n        self.query_templates = self.load_query_templates()\n        self.query_key_dict = self.create_query_key_dict()\n        self.metrics = metrics\n        self.sort_by = sort_by\n\n        # Define a dictionary to map metric names to their corresponding methods\n        self.metric_methods = {\n            \"precision\": self.add_precision,\n            \"recall\": self.add_recall,\n            \"map\": self.add_map,\n        }\n\n    def run(self):\n        \"\"\"\n        Description: Load files, Run the evaluation process and display the results\n\n        \"\"\"\n        csv_files = self.load_result_files()\n        results_df = self.generate_results(csv_files)\n        results_display = self.display_results(results_df)\n        return results_display\n\n    def load_result_files(self):\n        \"\"\"\n        Description: Find all the csv files in the evaluation directory.\n\n        \"\"\"\n        return glob.glob(str(self.eval_path / \"*/*/results.csv\"))\n\n    def generate_results(self, csv_files):\n        \"\"\"\n        Description: Load the results from the csv files, group them and compute metrics for each group. Then merge the results and sort them by the metric specified.\n        \"\"\"\n        merged_df = pd.DataFrame()\n\n        for exp_path in tqdm(csv_files):\n            exp = pd.read_csv(exp_path).rename(columns={\"did\": \"y_pred\"})\n            exp[\"exp_folder_name\"] = Path(exp_path).parent.name\n            exp[\"custom_experiment\"] = \"\"\n            # split exp_folder_name by @ to get extra information\n            exp[\"custom_experiment\"] = exp[\"exp_folder_name\"].apply(\n                lambda x: x.split(\"@\")[0] if \"@\" in x else \"\"\n            )\n            exp.drop(\"exp_folder_name\", axis=1, inplace=True)\n            exp = self.preprocess_results(exp)\n\n            grouped_results_for_y_true_and_pred = exp.groupby(\n                [\n                    \"embedding_model\",\n                    \"llm_model\",\n                    \"query\",\n                    \"llm_before_rag\",\n                    \"custom_experiment\",\n                ]\n            ).agg({\"y_true\": \",\".join, \"y_pred\": \",\".join})\n\n            grouped_results_for_y_true_and_pred = self.add_metrics(\n                grouped_results_for_y_true_and_pred\n            )\n\n            # aggregate by computing the average of the metrics for each group\n            grouped_results_for_y_true_and_pred = (\n                grouped_results_for_y_true_and_pred.groupby(\n                    [\n                        \"embedding_model\",\n                        \"llm_model\",\n                        \"llm_before_rag\",\n                        \"custom_experiment\",\n                    ]\n                ).agg({metric: \"mean\" for metric in self.metrics})\n            )\n\n            # merge with the results\n            merged_df = pd.concat([merged_df, grouped_results_for_y_true_and_pred])\n\n            # sort by metric\n            if self.sort_by in self.metrics:\n                merged_df = merged_df.sort_values(by=self.sort_by, ascending=False)\n        return merged_df\n\n    def add_metrics(self, grouped_results_for_y_true_and_pred):\n        # Iterate over the metrics and apply the corresponding method if it exists\n        for metric in self.metrics:\n            if metric in self.metric_methods:\n                grouped_results_for_y_true_and_pred = self.metric_methods[metric](\n                    grouped_results_for_y_true_and_pred\n                )\n\n        return grouped_results_for_y_true_and_pred\n\n    def load_queries_from_csv(self):\n        \"\"\"\n        Description: Load the queries from the csv file\n\n        \"\"\"\n        return pd.read_csv(self.eval_path / \"merged_labels.csv\")[\n            [\"Topics\", \"Dataset IDs\"]\n        ]\n\n    def load_query_templates(self):\n        \"\"\"\n        Description: Load the query templates from the txt file. This is used to generate the queries for the evaluation process. eg: {query_template} {query}\n        {find me a dataset about} {cancer}\n        \"\"\"\n        with open(self.eval_path / \"query_templates.txt\", \"r\") as f:\n            query_templates = f.readlines()\n        return [x.strip() for x in query_templates]\n\n    def create_query_key_dict(self):\n        \"\"\"\n        Description: Use the manual evaluation to create a dictionary of queries and their corresponding ground truth dataset ids. eg: Math,\"45617,43383,2,45748\"\n        \"\"\"\n        query_key_dict = {}\n        for template in self.query_templates:\n            for row in self.load_eval_queries.itertuples():\n                new_query = f\"{template} {row[1]}\".strip()\n                if new_query not in query_key_dict:\n                    query_key_dict[new_query.strip()] = row[2]\n        return query_key_dict\n\n    def preprocess_results(self, results_df):\n        \"\"\"\n        Description: Preprocess the results dataframe by filling missing values and converting the columns to the correct data types.\n        \"\"\"\n        results_df[\"llm_before_rag\"] = results_df[\"llm_before_rag\"].fillna(\n            \"No LLM filtering\"\n        )\n        results_df[\"y_pred\"] = results_df[\"y_pred\"].astype(str)\n        results_df[\"query\"] = results_df[\"query\"].str.strip()\n        results_df[\"y_true\"] = results_df[\"query\"].map(self.query_key_dict)\n        return results_df\n\n    @staticmethod\n    def add_precision(grouped_df):\n        \"\"\"\n        Description: Compute the precision metric for each group in the dataframe\n        \"\"\"\n        grouped_df[\"precision\"] = [\n            len(set(y_true).intersection(set(y_pred))) / len(set(y_pred))\n            for y_true, y_pred in zip(grouped_df[\"y_true\"], grouped_df[\"y_pred\"])\n        ]\n        return grouped_df\n\n    @staticmethod\n    def add_recall(grouped_df):\n        \"\"\"\n        Description: Compute the recall metric for each group in the dataframe\n\n        \"\"\"\n        grouped_df[\"recall\"] = [\n            len(set(y_true).intersection(set(y_pred))) / len(set(y_true))\n            for y_true, y_pred in zip(grouped_df[\"y_true\"], grouped_df[\"y_pred\"])\n        ]\n        return grouped_df\n\n    @staticmethod\n    def add_map(grouped_df):\n        \"\"\"\n        Description: Compute the mean average precision metric for each group in the dataframe\n        \"\"\"\n        grouped_df[\"map\"] = [\n            sum(\n                [\n                    len(set(y_true).intersection(set(y_pred[:i]))) / i\n                    for i in range(1, len(set(y_pred)))\n                ]\n            )\n            / len(set(y_true))\n            for y_true, y_pred in zip(grouped_df[\"y_true\"], grouped_df[\"y_pred\"])\n        ]\n        return grouped_df\n\n    @staticmethod\n    def display_results(results_df):\n        # add more preprocessing here\n        results_df = pd.DataFrame(results_df)\n        # heatmap results\n        # return results_df.style.background_gradient(cmap='coolwarm', axis=0)\n        return results_df\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.add_map","title":"<code>add_map(grouped_df)</code>  <code>staticmethod</code>","text":"<p>Description: Compute the mean average precision metric for each group in the dataframe</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>@staticmethod\ndef add_map(grouped_df):\n    \"\"\"\n    Description: Compute the mean average precision metric for each group in the dataframe\n    \"\"\"\n    grouped_df[\"map\"] = [\n        sum(\n            [\n                len(set(y_true).intersection(set(y_pred[:i]))) / i\n                for i in range(1, len(set(y_pred)))\n            ]\n        )\n        / len(set(y_true))\n        for y_true, y_pred in zip(grouped_df[\"y_true\"], grouped_df[\"y_pred\"])\n    ]\n    return grouped_df\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.add_precision","title":"<code>add_precision(grouped_df)</code>  <code>staticmethod</code>","text":"<p>Description: Compute the precision metric for each group in the dataframe</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>@staticmethod\ndef add_precision(grouped_df):\n    \"\"\"\n    Description: Compute the precision metric for each group in the dataframe\n    \"\"\"\n    grouped_df[\"precision\"] = [\n        len(set(y_true).intersection(set(y_pred))) / len(set(y_pred))\n        for y_true, y_pred in zip(grouped_df[\"y_true\"], grouped_df[\"y_pred\"])\n    ]\n    return grouped_df\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.add_recall","title":"<code>add_recall(grouped_df)</code>  <code>staticmethod</code>","text":"<p>Description: Compute the recall metric for each group in the dataframe</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>@staticmethod\ndef add_recall(grouped_df):\n    \"\"\"\n    Description: Compute the recall metric for each group in the dataframe\n\n    \"\"\"\n    grouped_df[\"recall\"] = [\n        len(set(y_true).intersection(set(y_pred))) / len(set(y_true))\n        for y_true, y_pred in zip(grouped_df[\"y_true\"], grouped_df[\"y_pred\"])\n    ]\n    return grouped_df\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.create_query_key_dict","title":"<code>create_query_key_dict()</code>","text":"<p>Description: Use the manual evaluation to create a dictionary of queries and their corresponding ground truth dataset ids. eg: Math,\"45617,43383,2,45748\"</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>def create_query_key_dict(self):\n    \"\"\"\n    Description: Use the manual evaluation to create a dictionary of queries and their corresponding ground truth dataset ids. eg: Math,\"45617,43383,2,45748\"\n    \"\"\"\n    query_key_dict = {}\n    for template in self.query_templates:\n        for row in self.load_eval_queries.itertuples():\n            new_query = f\"{template} {row[1]}\".strip()\n            if new_query not in query_key_dict:\n                query_key_dict[new_query.strip()] = row[2]\n    return query_key_dict\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.generate_results","title":"<code>generate_results(csv_files)</code>","text":"<p>Description: Load the results from the csv files, group them and compute metrics for each group. Then merge the results and sort them by the metric specified.</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>def generate_results(self, csv_files):\n    \"\"\"\n    Description: Load the results from the csv files, group them and compute metrics for each group. Then merge the results and sort them by the metric specified.\n    \"\"\"\n    merged_df = pd.DataFrame()\n\n    for exp_path in tqdm(csv_files):\n        exp = pd.read_csv(exp_path).rename(columns={\"did\": \"y_pred\"})\n        exp[\"exp_folder_name\"] = Path(exp_path).parent.name\n        exp[\"custom_experiment\"] = \"\"\n        # split exp_folder_name by @ to get extra information\n        exp[\"custom_experiment\"] = exp[\"exp_folder_name\"].apply(\n            lambda x: x.split(\"@\")[0] if \"@\" in x else \"\"\n        )\n        exp.drop(\"exp_folder_name\", axis=1, inplace=True)\n        exp = self.preprocess_results(exp)\n\n        grouped_results_for_y_true_and_pred = exp.groupby(\n            [\n                \"embedding_model\",\n                \"llm_model\",\n                \"query\",\n                \"llm_before_rag\",\n                \"custom_experiment\",\n            ]\n        ).agg({\"y_true\": \",\".join, \"y_pred\": \",\".join})\n\n        grouped_results_for_y_true_and_pred = self.add_metrics(\n            grouped_results_for_y_true_and_pred\n        )\n\n        # aggregate by computing the average of the metrics for each group\n        grouped_results_for_y_true_and_pred = (\n            grouped_results_for_y_true_and_pred.groupby(\n                [\n                    \"embedding_model\",\n                    \"llm_model\",\n                    \"llm_before_rag\",\n                    \"custom_experiment\",\n                ]\n            ).agg({metric: \"mean\" for metric in self.metrics})\n        )\n\n        # merge with the results\n        merged_df = pd.concat([merged_df, grouped_results_for_y_true_and_pred])\n\n        # sort by metric\n        if self.sort_by in self.metrics:\n            merged_df = merged_df.sort_values(by=self.sort_by, ascending=False)\n    return merged_df\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.load_queries_from_csv","title":"<code>load_queries_from_csv()</code>","text":"<p>Description: Load the queries from the csv file</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>def load_queries_from_csv(self):\n    \"\"\"\n    Description: Load the queries from the csv file\n\n    \"\"\"\n    return pd.read_csv(self.eval_path / \"merged_labels.csv\")[\n        [\"Topics\", \"Dataset IDs\"]\n    ]\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.load_query_templates","title":"<code>load_query_templates()</code>","text":"<p>Description: Load the query templates from the txt file. This is used to generate the queries for the evaluation process. eg: {query_template} {query} {find me a dataset about} {cancer}</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>def load_query_templates(self):\n    \"\"\"\n    Description: Load the query templates from the txt file. This is used to generate the queries for the evaluation process. eg: {query_template} {query}\n    {find me a dataset about} {cancer}\n    \"\"\"\n    with open(self.eval_path / \"query_templates.txt\", \"r\") as f:\n        query_templates = f.readlines()\n    return [x.strip() for x in query_templates]\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.load_result_files","title":"<code>load_result_files()</code>","text":"<p>Description: Find all the csv files in the evaluation directory.</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>def load_result_files(self):\n    \"\"\"\n    Description: Find all the csv files in the evaluation directory.\n\n    \"\"\"\n    return glob.glob(str(self.eval_path / \"*/*/results.csv\"))\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.preprocess_results","title":"<code>preprocess_results(results_df)</code>","text":"<p>Description: Preprocess the results dataframe by filling missing values and converting the columns to the correct data types.</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>def preprocess_results(self, results_df):\n    \"\"\"\n    Description: Preprocess the results dataframe by filling missing values and converting the columns to the correct data types.\n    \"\"\"\n    results_df[\"llm_before_rag\"] = results_df[\"llm_before_rag\"].fillna(\n        \"No LLM filtering\"\n    )\n    results_df[\"y_pred\"] = results_df[\"y_pred\"].astype(str)\n    results_df[\"query\"] = results_df[\"query\"].str.strip()\n    results_df[\"y_true\"] = results_df[\"query\"].map(self.query_key_dict)\n    return results_df\n</code></pre>"},{"location":"evaluation/api_reference/#evaluation_utils.EvaluationProcessor.run","title":"<code>run()</code>","text":"<p>Description: Load files, Run the evaluation process and display the results</p> Source code in <code>evaluation/evaluation_utils.py</code> <pre><code>def run(self):\n    \"\"\"\n    Description: Load files, Run the evaluation process and display the results\n\n    \"\"\"\n    csv_files = self.load_result_files()\n    results_df = self.generate_results(csv_files)\n    results_display = self.display_results(results_df)\n    return results_display\n</code></pre>"},{"location":"evaluation/api_reference/#elastic-search-eval","title":"Elastic Search Eval","text":""},{"location":"evaluation/evaluation/","title":"Evaluation of LLM models and techniques","text":""},{"location":"evaluation/evaluation/#how-to-run","title":"How to run","text":"<ul> <li>Start the language server at the root of this repository with <code>./start_llm_service.sh</code> . This is important, do not skip it.</li> <li>Run <code>python get_elastic_search_results.py</code> to get the results from the Elastic search implementation of OpenML.</li> <li>Run <code>python run_all_training.py</code> to train all models (get data, create vector store for each etc) and run the models on all possible versions of the queries.</li> <li>Query templates are in <code>data/evaluation/query_templates.txt</code>. Add to this if you want different types of queries.</li> <li>Run <code>python evaluate.py</code> to aggregate the results from the previous query. (This does not run the models on the queries) </li> <li>Results are found in in <code>./evaluation_results.csv</code> and <code>evaluation_results.png</code></li> <li>Important note : If you want to re-run some experiments because things have changed and if the models that you use are the same but the data/labels are new.</li> <li>Go to <code>/data/evaluation/{rag-model}/{llm-model}</code> and remove/move all the folders under it except <code>chroma_db</code>. If new data is added, the training loop will take care of adding them to the vector database. But if you remove this, it will take a lot longer for the data to be embedded from scratch.</li> </ul>"},{"location":"evaluation/evaluation/#how-to-add-a-new-evaluation","title":"How to add a new evaluation","text":"<ul> <li>It is \"pretty easy\" to add a new evaluation. </li> <li>(Note that <code>training_utils.py</code> already overloads some classes from the original training. Which means that you can modify this to your hearts content without affecting the main code. Enjoy~)</li> <li>Step 1: Find the method you want to override and overload the class/method in <code>experiments.py</code>.</li> <li>Step 2: Add some if statements in <code>class ExperimentRunner</code> to ensure you dont break everything.</li> <li>Step 3: Follow the ExperimentRunner templates in <code>run_all_training.py</code> to add whatever you added in Step 2 as a new experiment.<ul> <li>Give it a custom name so it is easy to understand what happens</li> <li>Do not worry, the experiments are cached and won't run again if you have run them before.</li> </ul> </li> <li>Step 4: If you changed something from config, make sure you reset it. Since the file runs in one go, it will affect the following experiments otherwise.</li> </ul>"},{"location":"evaluation/evaluation/#how-to-add-a-new-metric","title":"How to add a new metric","text":"<ul> <li>In <code>evaluation_utils.py</code>, go to <code>class EvaluationProcessor</code>, add a new function that calculates your metric. (You can use the templates provided)</li> <li>Update the metric in <code>self.metric_methods</code></li> <li>While running the evaluation, add them to your metrics list : <pre><code>metrics = [\"precision\", \"recall\", \"map\"]\neval_path = Path(\"../data/evaluation/\")\nprocessor = EvaluationProcessor(eval_path, sort_by=None, metrics=metrics)\n</code></pre></li> </ul>"},{"location":"evaluation/labelling_tool/","title":"Labeling Tool","text":"<p>Simple streamlit app to help label (query, dataset) pairs.</p>"},{"location":"evaluation/labelling_tool/#installation","title":"Installation","text":"<p>The app only requires streamlit:</p> <pre><code>python -m venv venv\nsource venv/bin/activate\npython -m pip install streamlit\n</code></pre>"},{"location":"evaluation/labelling_tool/#data","title":"Data","text":"<p>The app requires some data to operate, specifically it expects:</p> <ul> <li> <p><code>data/all_dataset_description.csv</code>: any file with openml metadata with the following columns (e.g., the one Subha shared):</p> <ul> <li>did</li> <li>description</li> <li>NumberOfFeatures</li> <li>NumberOfNumericFeatures</li> <li>NumberOfSymbolicFeatures</li> <li>NumberOfClasses</li> <li>NumberOfMissingValues</li> </ul> </li> <li> <p><code>data/LLM Evaluation - Topic Queries.csv</code>: export from our shared google sheet. i.e., a csv with column \"topic\".</p> </li> </ul>"},{"location":"evaluation/labelling_tool/#using-the-app","title":"Using the App","text":"<p>With everything in place, just run <code>streamlit run app.py</code>. If you already have a file with stored label data, it will load it, otherwise you start making a new one. You can now browse through datasets, and for each dataset you can select which of the queries are relevant. Changes are not automatically persisted. If the 'save me' button is red, there are local unsaved changes. Click it to persist the changes.</p> <p>We should be able to merge the different label files later without problem.</p>"},{"location":"evaluation/merging_labels/","title":"Merging labels","text":"<ul> <li>Takes multiple JSON files as input and merges them into a single csv file with columns <code>Topics,Dataset IDs</code></li> </ul>"},{"location":"evaluation/merging_labels/#how-to-use","title":"How to use","text":"<ul> <li>Place all the label.json files in the folder <code>/tools/data/all_labels</code></li> <li>Run <code>python merge_labels.py</code> from the <code>tools</code> directory.</li> <li>The results would be present in <code>/data/evaluation/merged_labels.csv</code></li> </ul>"},{"location":"evaluation/testing/","title":"Testing","text":""},{"location":"evaluation/testing/#unit-testing","title":"Unit Testing","text":"<ul> <li>Run <code>python -m unittest tests/unit_testing.py</code> to run the unit tests.</li> </ul>"},{"location":"evaluation/testing/#load-testing","title":"Load Testing","text":"<ul> <li>Load testing can be done using Locust, a load testing tool that allows you to simulate users querying the API and measure the performance of the API under load from numerous users.</li> <li>It is possible to configure the number of users, the hatch rate, and the time to run the test for.</li> </ul>"},{"location":"evaluation/testing/#running-the-load-test","title":"Running the load test","text":"<ul> <li>Start the FastAPI server using <code>uvicorn main:app</code> (or <code>./start_local.sh</code> )</li> <li>Load testing using Locust (<code>locust -f tests/locust_test.py --host http://127.0.0.1:8000</code> ) using a different terminal</li> </ul>"}]}