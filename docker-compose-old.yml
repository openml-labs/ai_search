services:
  ollama:
    image: ollama/ollama:latest
    ports: 
      - 11434:11434
    networks:
      - deploy_network
    container_name: ollama
    volumes:
      - ./ollama/get_ollama.sh:/get_ollama.sh
      # - ./data/:/data
    entrypoint: ["/bin/bash", "/get_ollama.sh", "&&", "ollama", "run", "llama3"]
    

  fastapi:
    build:
      context: .
      dockerfile: backend/Dockerfile
    ports:
      - 8000:8000
    networks:
      - deploy_network
    depends_on:
      - ollama
    container_name: fastapi
    volumes:
      - ./data/:/data

  llmservice:
    build: structured_query
    ports:
      - 8081:8081
    networks:
      - deploy_network
    depends_on:
      - ollama
    container_name: llmservice
    volumes:
      - ./data/:/data
    
  documentation_bot:
    build: documentation_bot
    ports:
      - 8083:8083
    networks:
      - deploy_network
    depends_on:
      - fastapi
      - ollama
    container_name: documentation_query
    volumes:
      - ./data/:/data
    deploy:
      resources:
        limits:
          memory: "8g"
      restart_policy:
        condition: on-failure
        delay: 50s

  streamlit:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    depends_on:
      - fastapi
      - ollama
    ports:
      - 8501:8501
    networks:
      - deploy_network
    container_name: streamlit
    volumes:
      - ./data/:/data

networks:
  deploy_network:
    driver: bridge
