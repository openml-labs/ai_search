{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAG pipeline for OpenML","text":"<ul> <li>This repository contains the code for the RAG pipeline for OpenML. </li> <li>Project roadmap</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Clone the repository</li> <li>Create a virtual environment and activate it</li> <li>Install the requirements using <code>pip install -r requirements.txt</code></li> <li>Run training.py (for the first time/to update the model). This takes care of basically everything. (Refer to the training section for more details)</li> <li>Install Ollama (https://ollama.com/) for your machine</li> </ul> <ul> <li>For a local setup, you can run ./start_local.sh to start Olama, FastAPI and Streamlit servers. The Streamlit server will be available at http://localhost:8501</li> <li>For docker, refer to Docker</li> <li>For a complete usage example refer to pipeline usage</li> <li>Enjoy :)</li> </ul>"},{"location":"#example-usage","title":"Example usage","text":"<ul> <li>Note that in this picture, I am using a very very tiny model for demonstration purposes. The actual results would be a lot better :)</li> <li></li> </ul>"},{"location":"#where-do-i-go-from-here","title":"Where do I go from here?","text":""},{"location":"#i-am-a-developer-and-i-want-to-contribute-to-the-project","title":"I am a developer and I want to contribute to the project","text":"<ul> <li>Hello! We are glad you are here. To get started, refer to the tutorials in the developer tutorial section.</li> <li>If you have any questions, feel free to ask or post an issue.</li> </ul>"},{"location":"#i-just-want-to-use-the-pipeline","title":"I just want to use the pipeline","text":"<ul> <li>You can use the pipeline by running the Streamlit frontend. Refer to the getting started section above for more details.</li> </ul>"},{"location":"#i-am-on-the-wrong-page","title":"I am on the wrong page","text":""},{"location":"configuration/","title":"Configuration","text":"<ul> <li>The main config file is <code>config.json</code> </li> <li>Possible options are as follows:</li> <li>rqa_prompt_template: The template for the RAG pipeline search prompt. This is used by the model to query the database. </li> <li>llm_prompt_template: The template for the summary generator LLM prompt.</li> <li>num_return_documents: Number of documents to return for a query. Too high a number can lead to Out of Memory errors. (Defaults to 50)</li> <li>embedding_model: The model to use for generating embeddings. This is used to generate embeddings for the documents as a means of comparison using the LLM's embeddings. (Defaults to BAAI/bge-large-en-v1.5)<ul> <li>Other possible tested models<ul> <li>BAAI/bge-base-en-v1.5</li> <li>BAAI/bge-large-en-v1.5</li> <li>WhereIsAI/UAE-Large-V1</li> </ul> </li> </ul> </li> <li>llm_model: The model used for generating the result summary. (Defaults to qwen2:1.5b)</li> <li>data_dir: The directory to store the intermediate data like tables/databases etc. (Defaults to ./data/)</li> <li>persist_dir: The directory to store the cached data. Defaults to ./data/chroma_db/ and stores the embeddings for the documents with a unique hash. (Defaults to ./data/chroma_db/)</li> <li>testing_flag: Enables testing mode by using subsets of the data for quick debugging. This is used to test the pipeline and is not recommended for normal use. (Defaults to False)</li> <li>data_download_n_jobs: Number of jobs to run in parallel for downloading data. (Defaults to 20)</li> <li>training: Whether to train the model or not. (Defaults to False) this is automatically set to True when when running the training.py script. Do NOT set this to True manually.</li> <li>search_type : The type of vector comparison to use. (Defaults to \"similarity\")</li> <li>reraanking: Whether to rerank the results using the FlashRank algorithm. (Defaults to False)</li> <li>long_context_reordering: Whether to reorder the results using the Long Context Reordering algorithm. (Defaults to False)</li> </ul>"},{"location":"docker/","title":"Docker container","text":""},{"location":"docker/#building","title":"Building","text":"<ul> <li>Run <code>docker compose build --progress=plain</code></li> </ul>"},{"location":"docker/#running","title":"Running","text":"<ul> <li>Run <code>./start_docker.sh</code></li> <li>This uses the docker compose file to run the docker process in the background.</li> <li>The required LLM model is also pulled from the docker hub and the container is started.</li> </ul>"},{"location":"docker/#stopping","title":"Stopping","text":"<ul> <li>Run <code>./stop_docker.sh</code></li> </ul>"},{"location":"docker/#potential-errors","title":"Potential Errors","text":"<ul> <li>Permission errors : Run <code>chmod +x *.sh</code></li> <li>If you get a memory error you can run <code>docker system prune</code>. Please be careful with this command as it will remove all stopped containers, all dangling images, and all unused networks. So ensure you have no important data in any of the containers before running this command.</li> <li>On docker desktop for Mac, increase memory limits to as much as your system can handle.</li> </ul>"},{"location":"inference/","title":"Inference","text":"<ul> <li>Just run ./start_local.sh and it will take care of everything.</li> <li>The UI should either pop up or you can navigate to http://localhost:8501/ in your browser.</li> <li>Note that it takes a decent bit of time to load everything. </li> </ul>"},{"location":"inference/#stopping","title":"Stopping","text":"<ul> <li>Run ./stop_local.sh</li> <li>./start_local.sh stores the PIDs of all the processes it starts in files in all the directories it starts them in. stop_local.sh reads these files and kills the processes.</li> </ul>"},{"location":"inference/#starting-individual-services","title":"Starting individual services","text":""},{"location":"inference/#start-processes","title":"Start processes","text":""},{"location":"inference/#ollama","title":"Ollama","text":"<p>cd ollama ./get_ollama.sh &amp;</p>"},{"location":"inference/#llm-service","title":"LLM Service","text":"<p>cd ../llm_service uvicorn llm_service:app --host 0.0.0.0 --port 8081 &amp;</p>"},{"location":"inference/#backend","title":"Backend","text":"<p>cd ../backend uvicorn backend:app --host 0.0.0.0 --port 8000 &amp;</p>"},{"location":"inference/#frontend","title":"Frontend","text":"<p>cd ../frontend streamlit run ui.py &amp;</p>"},{"location":"inference/#errors","title":"Errors","text":"<ul> <li>If you get an error about file permissions, run <code>chmod +x start_local.sh</code> and <code>chmod +x stop_local.sh</code> to make them executable.</li> </ul>"},{"location":"inference/#ui_utils.display_results","title":"<code>display_results(initial_response)</code>","text":"<p>Description: Display the results in a DataFrame</p> <p>Input: initial_response (DataFrame)</p> <p>Returns: None</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def display_results(initial_response):\n    \"\"\"\n    Description: Display the results in a DataFrame\n\n    Input: initial_response (DataFrame)\n\n    Returns: None\n    \"\"\"\n    st.write(\"Results:\")\n    st.dataframe(initial_response)\n</code></pre>"},{"location":"inference/#ui_utils.feedback_cb","title":"<code>feedback_cb()</code>","text":"<p>Description: Callback function to save feedback to a file</p> <p>Input: None</p> <p>Returns: None</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def feedback_cb():\n    \"\"\"\n    Description: Callback function to save feedback to a file\n\n    Input: None\n\n    Returns: None\n    \"\"\"\n    file_path = \"feedback.json\"\n\n    if os.path.exists(file_path):\n        with open(file_path, \"r\") as file:\n            try:\n                data = json.load(file)\n            except json.JSONDecodeError:\n                data = []\n    else:\n        data = []\n\n    # Append new feedback\n    data.append({\"ss\": ss.fb_k, \"query\": ss.query})\n\n    # Write updated content back to the file\n    with open(file_path, \"w\") as file:\n        json.dump(data, file, indent=4)\n</code></pre>"},{"location":"inference/#ui_utils.fetch_llm_response","title":"<code>fetch_llm_response(query)</code>","text":"<p>Description: Fetch the response from the LLM service</p> <p>Input: query (str)</p> <p>Returns: llm_response (dict)</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_llm_response(query):\n    \"\"\"\n    Description: Fetch the response from the LLM service\n\n    Input: query (str)\n\n    Returns: llm_response (dict)\n    \"\"\"\n    llm_response_path = paths[\"llm_response\"]\n    try:\n        llm_response = requests.get(f\"{llm_response_path['docker']}{query}\").json()\n    except:\n        llm_response = requests.get(f\"{llm_response_path['local']}{query}\").json()\n    return llm_response\n</code></pre>"},{"location":"inference/#ui_utils.fetch_response","title":"<code>fetch_response(query_type, query)</code>","text":"<p>Description: Fetch the response from the FastAPI service</p> <p>Input: query_type (str), query (str)</p> <p>Returns: response (dict)</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def fetch_response(query_type, query):\n    \"\"\"\n    Description: Fetch the response from the FastAPI service\n\n    Input: query_type (str), query (str)\n\n    Returns: response (dict)\n    \"\"\"\n    rag_response_path = paths[\"rag_response\"]\n    try:\n        response = requests.get(\n            f\"{rag_response_path['docker']}{query_type.lower()}/{query}\",\n            json={\"query\": query, \"type\": query_type.lower()},\n        ).json()\n    except:\n        response = requests.get(\n            f\"{rag_response_path['local']}{query_type.lower()}/{query}\",\n            json={\"query\": query, \"type\": query_type.lower()},\n        ).json()\n    return response\n</code></pre>"},{"location":"inference/#ui_utils.filter_initial_response","title":"<code>filter_initial_response(response, classification, uploader)</code>","text":"<p>Description: Filter the initial response based on the classification</p> <p>Input: response (DataFrame), classification (str)</p> <p>Returns: response (DataFrame)</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def filter_initial_response(response, classification, uploader):\n    \"\"\"\n    Description: Filter the initial response based on the classification\n\n    Input: response (DataFrame), classification (str)\n\n    Returns: response (DataFrame)\n    \"\"\"\n    if classification != \"none\":\n        if \"multi\" in classification:\n            response = response[response[\"NumberOfClasses\"] &gt; 2]\n        elif \"binary\" in classification:\n            response = response[response[\"NumberOfClasses\"] == 2]\n    if uploader != \"none\":\n        try:\n            uploader = int(uploader)\n            response = response[response[\"uploader\"] == uploader]\n        except:\n            pass\n    return response\n</code></pre>"},{"location":"inference/#ui_utils.parse_and_update_response","title":"<code>parse_and_update_response(query_type, response, llm_response, data_metadata, flow_metadata)</code>","text":"<p>Description: Parse and update the response based on the query type</p> <p>Input: query_type (str), response (dict), llm_response (dict), data_metadata (DataFrame), flow_metadata (DataFrame)</p> <p>Returns: initial_response (DataFrame)</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def parse_and_update_response(query_type, response, llm_response, data_metadata, flow_metadata):\n    \"\"\"\n    Description: Parse and update the response based on the query type\n\n    Input: query_type (str), response (dict), llm_response (dict), data_metadata (DataFrame), flow_metadata (DataFrame)\n\n    Returns: initial_response (DataFrame)\n    \"\"\"\n    if query_type == \"Dataset\":\n        initial_response = data_metadata[data_metadata[\"did\"].isin(response[\"initial_response\"])]\n        subset_cols = [\"did\", \"name\"]\n        try:\n            dataset_size, dataset_missing, dataset_classification, dataset_sort, uploader = parse_llm_response(llm_response)\n            subset_cols = update_subset_cols(dataset_size, dataset_missing, dataset_classification, uploader)\n            initial_response = filter_initial_response(initial_response, dataset_classification, uploader)\n        except Exception as e:\n            st.error(f\"Error processing LLM response: {e}\")\n        initial_response = initial_response[subset_cols]\n    else:\n        initial_response = flow_metadata[flow_metadata[\"id\"].isin(response[\"initial_response\"])]\n    return initial_response\n</code></pre>"},{"location":"inference/#ui_utils.parse_llm_response","title":"<code>parse_llm_response(response)</code>","text":"<p>Description: Parse the answers from the LLM response</p> <p>Input: response (dict)</p> <p>Returns: size (str), missing (str), classification (str), sort (str)</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def parse_llm_response(response):\n    \"\"\"\n    Description: Parse the answers from the LLM response\n\n    Input: response (dict)\n\n    Returns: size (str), missing (str), classification (str), sort (str)\n    \"\"\"\n    size, missing, classification, uploader = response[\"answers\"]\n    # Split size and sort if there is a comma\n    size, sort = size.split(\",\") if \",\" in size else (size, None)\n\n    # split uploader by = to get the name\n    if uploader != \"none\":\n        uploader = uploader.split(\"=\")[1].strip()\n    return size, missing, classification, sort, uploader\n</code></pre>"},{"location":"inference/#ui_utils.update_subset_cols","title":"<code>update_subset_cols(size, missing, classification, uploader)</code>","text":"<p>Description: Update the subset columns based on LLM's response</p> <p>Input: size (str), missing (str), classification (str)</p> <p>Returns: cols (list)</p> Source code in <code>frontend/ui_utils.py</code> <pre><code>def update_subset_cols(size, missing, classification, uploader):\n    \"\"\"\n    Description: Update the subset columns based on LLM's response\n\n    Input: size (str), missing (str), classification (str)\n\n    Returns: cols (list)\n    \"\"\"\n    cols = [\"did\", \"name\"]\n    if size == \"yes\":\n        cols.append(\"NumberOfInstances\")\n    if missing == \"yes\":\n        cols.append(\"NumberOfMissingValues\")\n    if classification != \"none\":\n        cols.append(\"NumberOfClasses\")\n    if uploader != \"none\":\n        cols.append(\"uploader\")\n    return cols\n</code></pre>"},{"location":"query_llm/","title":"LLM Query parsing","text":"<ul> <li>The LLM reads the query and parses it into a list of filters based on a prompt </li> </ul>"},{"location":"query_llm/#llm_service.get_llm_query","title":"<code>get_llm_query(query)</code>  <code>async</code>","text":"<p>Description: Get the query, replace %20 with space and invoke the chain to get the answers based on the prompt</p> <p>Input: query: str</p> <p>Returns: JSONResponse: answers</p> Source code in <code>llm_service/llm_service.py</code> <pre><code>@app.get(\"/llmquery/{query}\", response_class=JSONResponse)\n@retry(stop=stop_after_attempt(3), retry=retry_if_exception_type(ConnectTimeout))\nasync def get_llm_query(query: str):\n    \"\"\"\n    Description: Get the query, replace %20 with space and invoke the chain to get the answers based on the prompt\n\n    Input: query: str\n\n    Returns: JSONResponse: answers\n    \"\"\"\n    query = query.replace(\"%20\", \" \")\n    response = chain.invoke({\"query\": query})\n    answers = parse_answers_initial(response, patterns)\n    return JSONResponse(content={\"answers\": answers})\n</code></pre>"},{"location":"query_llm/#llm_service_utils.create_chain","title":"<code>create_chain(prompt, model='llama3', temperature=0)</code>","text":"<p>Description: Create a chain with the given prompt and model</p> <p>Input: prompt (str), model (str), temperature (float)</p> <p>Returns: chain (Chain)</p> Source code in <code>llm_service/llm_service_utils.py</code> <pre><code>def create_chain(prompt, model=\"llama3\", temperature=0):\n    \"\"\"\n    Description: Create a chain with the given prompt and model\n\n    Input: prompt (str), model (str), temperature (float)\n\n    Returns: chain (Chain)\n    \"\"\"\n    llm = ChatOllama(model=model, temperature=temperature)\n    prompt = ChatPromptTemplate.from_template(prompt)\n\n    return prompt | llm | StrOutputParser()\n</code></pre>"},{"location":"query_llm/#llm_service_utils.parse_answers_initial","title":"<code>parse_answers_initial(response, patterns)</code>","text":"<p>Description: Parse the answers from the initial response</p> <p>Input: response (str), patterns (list)</p> <p>Returns: answers (list)</p> Source code in <code>llm_service/llm_service_utils.py</code> <pre><code>def parse_answers_initial(response, patterns):\n    \"\"\"\n    Description: Parse the answers from the initial response\n\n    Input: response (str), patterns (list)\n\n    Returns: answers (list)\n    \"\"\"\n\n    answers = []\n    # if the response contains a ? and a new line then join the next line with it (sometimes the LLM adds a new line after the ? instead of just printing it on the same line)\n    response = response.replace(\"?\\n\", \"?\")\n\n    # convert the response to lowercase and split it into lines\n    lines = response.lower().split(\"\\n\")\n\n\n    for line in lines:\n        if \"?\" in line:\n            # Extract the part of the line after the question mark\n            potential_answer = line.split(\"?\")[1].strip()\n        else:\n            potential_answer = line.strip()\n\n        # Check if the potential answer matches any of the patterns\n        for pattern in patterns:\n            if re.match(pattern, potential_answer):\n                answers.append(potential_answer)\n                break  # Stop checking other patterns if a match is found\n\n    return answers\n</code></pre>"},{"location":"testing/","title":"Testing","text":""},{"location":"testing/#unit-testing","title":"Unit Testing","text":"<ul> <li>Run <code>python -m unittest tests/unit_testing.py</code> to run the unit tests.</li> </ul>"},{"location":"testing/#load-testing","title":"Load Testing","text":"<ul> <li>Load testing can be done using Locust, a load testing tool that allows you to simulate users querying the API and measure the performance of the API under load from numerous users.</li> <li>It is possible to configure the number of users, the hatch rate, and the time to run the test for.</li> </ul>"},{"location":"testing/#running-the-load-test","title":"Running the load test","text":"<ul> <li>Start the FastAPI server using <code>uvicorn main:app</code> (or <code>./start_local.sh</code> )</li> <li>Load testing using Locust (<code>locust -f tests/locust_test.py --host http://127.0.0.1:8000</code> ) using a different terminal</li> </ul>"},{"location":"testing/#all-tests","title":"All tests","text":"<p>               Bases: <code>TestCase</code></p> Source code in <code>tests/unit_testing.py</code> <pre><code>class TestConfig(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.client = chromadb.PersistentClient(path=config[\"persist_dir\"])\n        self.config_keys = [\n            \"rqa_prompt_template\",\n            \"llm_prompt_template\",\n            \"num_return_documents\",\n            \"embedding_model\",\n            \"llm_model\",\n            \"num_documents_for_llm\",\n            \"data_dir\",\n            \"persist_dir\",\n            \"testing_flag\",\n            \"ignore_downloading_data\",\n            \"test_subset\",\n            \"data_download_n_jobs\",\n            \"training\",\n            \"temperature\",\n            \"top_p\",\n            \"search_type\",\n            \"reranking\",\n            \"long_context_reorder\",\n        ]\n        self.query_test_dict = {\n            \"dataset\": \"Find me a dataset about flowers that has a high number of instances.\",\n            \"flow\": \"Find me a flow that uses the RandomForestClassifier.\",\n        }\n\n    def test_check_data_dirs(self):\n        \"\"\"\n        Description: Check if the data directory exists.\n        Returns: None\n        \"\"\"\n        self.assertTrue(os.path.exists(config[\"data_dir\"]))\n        self.assertTrue(os.path.exists(config[\"persist_dir\"]))\n\n    def test_config(self):\n        \"\"\"\n        Description: Check if the config has the required keys.\n        Returns: None\n        \"\"\"\n        for key in self.config_keys:\n            self.assertIn(key, config.keys())\n\n    def test_setup_vector_db_and_qa(self):\n        \"\"\"\n        Description: Check if the setup_vector_db_and_qa function works as expected.\n        Returns: None\n        \"\"\"\n        for type_of_data in [\"dataset\", \"flow\"]:\n            self.qa = setup_vector_db_and_qa(\n                config=config, data_type=type_of_data, client=self.client\n            )\n            self.assertIsNotNone(self.qa)\n            self.result_data_frame = get_result_from_query(\n                query=self.query_test_dict[type_of_data],\n                qa=self.qa,\n                type_of_query=type_of_data,\n                config=config,\n            )\n            self.assertIsNotNone(self.result_data_frame)\n</code></pre>"},{"location":"testing/#unit_testing.TestConfig.test_check_data_dirs","title":"<code>test_check_data_dirs()</code>","text":"<p>Description: Check if the data directory exists. Returns: None</p> Source code in <code>tests/unit_testing.py</code> <pre><code>def test_check_data_dirs(self):\n    \"\"\"\n    Description: Check if the data directory exists.\n    Returns: None\n    \"\"\"\n    self.assertTrue(os.path.exists(config[\"data_dir\"]))\n    self.assertTrue(os.path.exists(config[\"persist_dir\"]))\n</code></pre>"},{"location":"testing/#unit_testing.TestConfig.test_config","title":"<code>test_config()</code>","text":"<p>Description: Check if the config has the required keys. Returns: None</p> Source code in <code>tests/unit_testing.py</code> <pre><code>def test_config(self):\n    \"\"\"\n    Description: Check if the config has the required keys.\n    Returns: None\n    \"\"\"\n    for key in self.config_keys:\n        self.assertIn(key, config.keys())\n</code></pre>"},{"location":"testing/#unit_testing.TestConfig.test_setup_vector_db_and_qa","title":"<code>test_setup_vector_db_and_qa()</code>","text":"<p>Description: Check if the setup_vector_db_and_qa function works as expected. Returns: None</p> Source code in <code>tests/unit_testing.py</code> <pre><code>def test_setup_vector_db_and_qa(self):\n    \"\"\"\n    Description: Check if the setup_vector_db_and_qa function works as expected.\n    Returns: None\n    \"\"\"\n    for type_of_data in [\"dataset\", \"flow\"]:\n        self.qa = setup_vector_db_and_qa(\n            config=config, data_type=type_of_data, client=self.client\n        )\n        self.assertIsNotNone(self.qa)\n        self.result_data_frame = get_result_from_query(\n            query=self.query_test_dict[type_of_data],\n            qa=self.qa,\n            type_of_query=type_of_data,\n            config=config,\n        )\n        self.assertIsNotNone(self.result_data_frame)\n</code></pre>"},{"location":"training/","title":"Training","text":"<ul> <li>While we are not creating a new model, we are using the existing model to create embeddings. The name might be misleading but this was chosen as an attempt to keep the naming consistent with other codebases.</li> <li>(Perhaps we might fine tune the model in the future)</li> <li>The training script is present in <code>training.py</code>. Running this script will take care of everything.</li> </ul>"},{"location":"training/#what-does-the-training-script-do","title":"What does the training script do?","text":"<ul> <li>Load the config file and set the necessary variables</li> <li>If <code>testing_flag</code> is set to True, the script will use a subset of the data for quick debugging</li> <li>testing_flag is set to True</li> <li>persist_dir is set to ./data/chroma_db_testing</li> <li>test_subset is set to True</li> <li>data_dir is set to ./data/testing_data/</li> <li>If <code>testing_flag</code> is set to False, the script will use the entire dataset</li> <li>For all datasets in the OpenML dataset list:</li> <li>Download the dataset</li> <li>Create the vector dataset with computed embeddings</li> <li>Create a vectordb retriever </li> <li>Run some test queries</li> </ul>"},{"location":"developer%20tutorials/","title":"Developer Tutorials","text":"<ul> <li>Hello there, future OpenML contributor! It is nice meeting you here. This page is a collection of tutorials that will help you get started with contributing to the OpenML RAG pipeline.</li> <li>The tutorials show you how to perform common tasks and should make it a lot easier to get started with contributing to this project.</li> <li>Note that you would have had to setup the project before you begin. If you missed this step, please refer to index</li> </ul>"},{"location":"developer%20tutorials/change%20model/","title":"Change model","text":"<pre><code>from __future__ import annotations\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import load_config_and_device\nfrom modules.llm import setup_vector_db_and_qa\n</code></pre> <pre><code>config = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../backend/data/\"\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = True\n# load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\nprint(config)\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: cpu\n{'rqa_prompt_template': 'This database is a list of metadata. Use the following pieces of context to find the relevant document. Answer only from the context given using the {question} given. If you do not know the answer, say you do not know. {context}', 'llm_prompt_template': 'The following is a set of documents {docs}. Based on these docs, please summarize the content concisely. Also give a list of main concepts found in the documents. Do not add any new information. Helpful Answer: ', 'num_return_documents': 50, 'embedding_model': 'BAAI/bge-large-en-v1.5', 'llm_model': 'qwen2:1.5b', 'num_documents_for_llm': 10, 'data_dir': '../backend/data/', 'persist_dir': '../backend/data/chroma_db/', 'testing_flag': False, 'ignore_downloading_data': False, 'test_subset': False, 'data_download_n_jobs': 20, 'training': True, 'temperature': 0.95, 'top_p': 0.95, 'search_type': 'similarity', 'reranking': False, 'long_context_reorder': False, 'device': 'cpu', 'type_of_data': 'dataset'}\n</code>\n</pre> <pre><code>config[\"embedding_model\"] = \"HuggingFaceH4/capybara\"\n</code></pre> <ul> <li>Pick a model from Ollama - https://ollama.com/library?sort=popular</li> <li>eg : mistral</li> </ul> <pre><code>config[\"llm_model\"] = \"mistral\"\n</code></pre> <pre><code>qa = setup_vector_db_and_qa(\n        config=config, data_type=config[\"type_of_data\"], client=client\n    )\n</code></pre>"},{"location":"developer%20tutorials/change%20model/#tutorial-on-changing-models","title":"Tutorial on changing models","text":"<ul> <li>How would you use a different embedding and llm model?</li> </ul>"},{"location":"developer%20tutorials/change%20model/#initial-config","title":"Initial config","text":""},{"location":"developer%20tutorials/change%20model/#embedding-model","title":"Embedding model","text":"<ul> <li>Pick a model from HF</li> </ul>"},{"location":"developer%20tutorials/change%20model/#llm-model","title":"LLM model","text":""},{"location":"developer%20tutorials/change%20model/#important","title":"IMPORTANT","text":"<ul> <li>Do NOT forget to change the model to the best model in ollama/get_ollama.sh</li> </ul>"},{"location":"developer%20tutorials/create%20vectordb/","title":"Create vectordb","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import get_all_metadata_from_openml, create_metadata_dataframe, load_config_and_device\nfrom modules.llm import load_document_and_create_vector_store, setup_vector_db_and_qa\n</code></pre> <pre><code>config = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../backend/data/\"\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = True\n\n# load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\nprint(config)\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: cpu\n{'rqa_prompt_template': 'This database is a list of metadata. Use the following pieces of context to find the relevant document. Answer only from the context given using the {question} given. If you do not know the answer, say you do not know. {context}', 'llm_prompt_template': 'The following is a set of documents {docs}. Based on these docs, please summarize the content concisely. Also give a list of main concepts found in the documents. Do not add any new information. Helpful Answer: ', 'num_return_documents': 50, 'embedding_model': 'BAAI/bge-large-en-v1.5', 'llm_model': 'qwen2:1.5b', 'num_documents_for_llm': 10, 'data_dir': '../backend/data/', 'persist_dir': '../backend/data/chroma_db/', 'testing_flag': False, 'ignore_downloading_data': False, 'test_subset': False, 'data_download_n_jobs': 20, 'training': True, 'temperature': 0.95, 'top_p': 0.95, 'search_type': 'similarity', 'reranking': False, 'long_context_reorder': False, 'device': 'cpu', 'type_of_data': 'dataset'}\n</code>\n</pre> <pre><code># Download the data if it does not exist\nopenml_data_object, data_id, all_metadata = get_all_metadata_from_openml(\n    config=config\n)\n# Create the combined metadata dataframe\nmetadata_df, all_metadata = create_metadata_dataframe(\n    openml_data_object, data_id, all_metadata, config=config\n)\n# Create the vector store\nvectordb = load_document_and_create_vector_store(\n    metadata_df, config=config, chroma_client=client\n)\n</code></pre> <pre><code>qa = setup_vector_db_and_qa(\n        config=config, data_type=config[\"type_of_data\"], client=client\n    )\n</code></pre>"},{"location":"developer%20tutorials/create%20vectordb/#tutorial-on-creating-a-vector-database-with-openml-objects","title":"Tutorial on creating a vector database with openml objects","text":"<ul> <li>How would you use the API to create a vector database with openml objects (datasets, flows etc)</li> </ul>"},{"location":"developer%20tutorials/create%20vectordb/#manually","title":"Manually","text":""},{"location":"developer%20tutorials/create%20vectordb/#api","title":"API","text":""},{"location":"developer%20tutorials/get%20an%20llm%20summary/","title":"Get an llm summary","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.llm import get_llm_chain, get_llm_result_from_string\nfrom modules.utils import load_config_and_device\n</code></pre> <pre><code># Config and DB\n\n# load the configuration and device\nconfig = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../../backend/data/\"\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: cpu\n</code>\n</pre> <pre><code>config[\"llm_prompt_template\"] = \"The following is a set of documents {docs}. Based on these docs, please summarize the content concisely. Also give a list of main concepts found in the documents. Do not add any new information. Helpful Answer: \"\nconfig[\"llm_model\"] = \"qwen2:1.5b\"\n</code></pre> <pre><code># get the llm chain and set the cache\nllm_chain = get_llm_chain(config=config, local=True)\n# use os path to ensure compatibility with all operating systems\nset_llm_cache(SQLiteCache(database_path=os.path.join(config[\"data_dir\"], \".langchain.db\")))\n</code></pre> <pre><code>get_llm_result_from_string(llm_chain, \"This document is about eating disorders and this one is about eating nice food\")\n</code></pre> <pre>\n<code>'Eating Disorders\\n\\n- Eating disorders refer to psychological and emotional conditions characterized by compulsive behaviors such as overeating or excessive restriction.\\n- These behaviors lead to significant weight loss, malnutrition, and serious health complications.\\n\\nEating Nice Food\\n\\n- This document focuses on the importance of eating good food for maintaining a healthy and balanced diet.\\n- It highlights how selecting nutrient-dense foods can aid in overall physical and mental well-being.'</code>\n</pre>"},{"location":"developer%20tutorials/get%20an%20llm%20summary/#getting-an-llm-summary-using-the-api","title":"Getting an LLM summary using the API","text":"<ul> <li>How would you use the API and an LLM model + prompt to generate a summary of the results obtained from the RAG pipeline?</li> </ul>"},{"location":"developer%20tutorials/get%20an%20llm%20summary/#get-llm-summary-of-a-string","title":"Get LLM summary of a string","text":"<ul> <li>Ensure that Ollama is running before this works <code>bash ollama/.get_ollama.sh</code> (or use the desktop Ollama app for testing)</li> <li>As you can tell, the data needs to be a string. To then get the results from a bunch of langchain documents, you must first concatenate the text you care about into a single string.</li> </ul>"},{"location":"developer%20tutorials/load%20vectordb%20and%20get%20results/","title":"Load vectordb and get results","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import load_config_and_device\nfrom modules.llm import setup_vector_db_and_qa\nfrom modules.results_gen import get_result_from_query\n</code></pre> <pre><code># Config and DB\n\n# load the configuration and device\nconfig = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../../backend/data/\"\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: cpu\n</code>\n</pre> <pre><code># load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\n</code></pre> <pre><code># Setup llm chain, initialize the retriever and llm, and setup Retrieval QA\nqa_dataset = setup_vector_db_and_qa(config=config, data_type=\"dataset\", client=client)\n</code></pre> <pre>\n<code>[INFO] Loading metadata from file.\n[INFO] Loading model...\n</code>\n</pre> <pre>\n<code>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n</code>\n</pre> <pre>\n<code>[INFO] Model loaded.\n</code>\n</pre> <pre><code>query = \"give me datasets about mushrooms\"\n</code></pre> <pre><code>res = qa_dataset.invoke(input = query, top_k=5)[:10]\nres\n</code></pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>[Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 25  \\n**\\\\# Images**: 15122  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44335, 'name': 'Meta_Album_FNG_Extended'}),\n Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 25  \\n**\\\\# Images**: 1000  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44302, 'name': 'Meta_Album_FNG_Mini'}),\n Document(page_content=\"### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\", metadata={'did': 24, 'name': 'mushroom'}),\n Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n Document(page_content='**Source**: Danish Fungi Dataset  \\n**Source URL**: https://sites.google.com/view/danish-fungi-dataset  \\n  \\n**Original Author**: Lukas Picek, Milan Sulc, Jiri Matas, Jacob Heilmann-Clausen, Thomas S. Jeppesen, Thomas Laessoe, Tobias Froslev  \\n**Original contact**: lukaspicek@gmail.com  \\n\\n**Meta Album author**: Felix Herron  \\n**Created Date**: 01 March 2022  \\n**Contact Name**: Ihsan Ullah  \\n**Contact Email**: meta-album@chalearn.org  \\n**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)  \\n\\n\\n\\n### **Cite this dataset**\\n```\\n@article{picek2021danish,\\n    title={Danish Fungi 2020 - Not Just Another Image Recognition Dataset},\\n    author={Lukas Picek and Milan Sulc and Jiri Matas and Jacob Heilmann-Clausen and Thomas S. Jeppesen and Thomas Laessoe and Tobias Froslev},\\n    year={2021},\\n    eprint={2103.10107},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.CV}\\n}\\n```', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n Document(page_content='did - 24, name - mushroom, version - 1, uploader - 1, status - active, format - ARFF, MajorityClassSize - 4208.0, MaxNominalAttDistinctValues - 12.0, MinorityClassSize - 3916.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 8124.0, NumberOfInstancesWithMissingValues - 2480.0, NumberOfMissingValues - 2480.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - **Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \\n**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n\\n### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.', metadata={'did': 24, 'name': 'mushroom'}),\n Document(page_content='Meta-Album Fungi dataset is created by sampling the Danish Fungi 2020 dataset(https://arxiv.org/abs/2103.10107), itself a sampling of the Atlas of Danish Fungi repository. The images and labels which enter this database are sourced by a group consisting of 3 300 citizen botanists, then verified by their peers using a ranking of each person reliability, then finally verified by experts working at the Atlas. Of the 128 classes in the original Danish Fungi 2020 dataset, FNG retains the 25 most populous classes, belonging to six genera, for a total of 15 122 images total, with min 372, and max 1 221 images per class. Each image contains a colored 128x128 image of a fungus or a piece of a fungus from the corresponding class. Because the initial data were of widely varying sizes, we needed to crop a significant portion of the images, which we implemented by taking the largest possible square with center at the middle of the initial image. We then scaled each squared image to the 128x128', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n Document(page_content='did - 44272, name - Meta_Album_FNG_Micro, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 40.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 40.0, NumberOfClasses - 20.0, NumberOfFeatures - 3.0, NumberOfInstances - 800.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Fungi Dataset (Micro)**\\n***', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n Document(page_content='did - 44335, name - Meta_Album_FNG_Extended, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 1221.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 372.0, NumberOfClasses - 25.0, NumberOfFeatures - 3.0, NumberOfInstances - 15122.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Fungi Dataset (Extended)**\\n***', metadata={'did': 44335, 'name': 'Meta_Album_FNG_Extended'}),\n Document(page_content='did - 43922, name - mushroom, version - 3, uploader - 30861, status - active, format - ARFF, MajorityClassSize - 4208.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 3916.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 8124.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - Mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf, qualities - AutoCorrelation : 0.726332635725717, Dimensionality : 0.002831117676021664, MajorityClassPercentage : 51.7971442639094, MajorityClassSize : 4208.0, MinorityClassPercentage : 48.20285573609059, MinorityClassSize : 3916.0, NumberOfBinaryFeatures : 6.0, NumberOfClasses : 2.0, NumberOfFeatures : 23.0, NumberOfInstances : 8124.0, NumberOfInstancesWithMissingValues : 0.0, NumberOfMissingValues : 0.0, NumberOfNumericFeatures : 0.0,', metadata={'did': 43922, 'name': 'mushroom'})]</code>\n</pre> <pre><code>res[0].metadata\n</code></pre> <pre>\n<code>{'did': 44335, 'name': 'Meta_Album_FNG_Extended'}</code>\n</pre> <pre><code>print(res[0].page_content)\n</code></pre> <pre>\n<code>### **Dataset Details**\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\n\n**Meta Album ID**: PLT.FNG  \n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \n**Domain ID**: PLT  \n**Domain Name**: Plants  \n**Dataset ID**: FNG  \n**Dataset Name**: Fungi  \n**Short Description**: Fungi dataset from Denmark  \n**\\# Classes**: 25  \n**\\# Images**: 15122  \n**Keywords**: fungi, ecology, plants  \n**Data Format**: images  \n**Image size**: 128x128  \n\n**License (original data release)**: BSD-3-Clause License  \n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\n\n**License (Meta-Album data release)**: BSD-3-Clause License  \n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)\n</code>\n</pre> <pre><code># Fetch the result data frame based on the query\nresult_data_frame, result_documents = get_result_from_query(\n    query=query, qa=qa_dataset, type_of_query=\"dataset\", config=config\n)\n</code></pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre><code>result_data_frame.head()\n</code></pre> id name command OpenML URL Description 0 44335 Meta_Album_FNG_Extended dataset = openml.datasets.get_dataset(44335) &lt;a href=\"https://www.openml.org/search?type=da... did - 44335, name - Meta_Album_FNG_Extended, v... 1 44302 Meta_Album_FNG_Mini dataset = openml.datasets.get_dataset(44302) &lt;a href=\"https://www.openml.org/search?type=da... ### **Dataset Details**\\n![](https://meta-albu... 2 24 mushroom dataset = openml.datasets.get_dataset(24) &lt;a href=\"https://www.openml.org/search?type=da... did - 24, name - mushroom, version - 1, upload... 3 44272 Meta_Album_FNG_Micro dataset = openml.datasets.get_dataset(44272) &lt;a href=\"https://www.openml.org/search?type=da... did - 44272, name - Meta_Album_FNG_Micro, vers... 10 1558 bank-marketing dataset = openml.datasets.get_dataset(1558) &lt;a href=\"https://www.openml.org/search?type=da... * Dataset:"},{"location":"developer%20tutorials/load%20vectordb%20and%20get%20results/#load-the-chroma-db-and-get-retrieval-results-for-a-given-query","title":"Load the Chroma Db and get retrieval results for a given query","text":"<ul> <li>How would you load the Chroma Db and get retrieval results for a given query?</li> </ul>"},{"location":"developer%20tutorials/load%20vectordb%20and%20get%20results/#just-get-documents","title":"Just get documents","text":""},{"location":"developer%20tutorials/load%20vectordb%20and%20get%20results/#process-the-results-and-return-a-dataframe-instead","title":"Process the results and return a dataframe instead","text":""},{"location":"developer%20tutorials/run%20multiple%20queries%20and%20aggregate/","title":"Run multiple queries and aggregate","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import load_config_and_device\nfrom modules.llm import setup_vector_db_and_qa\nfrom modules.results_gen import aggregate_multiple_queries_and_count\n</code></pre> <pre><code># Config and DB\n\n# load the configuration and device\nconfig = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../../backend/data/\"\n</code></pre> <pre><code># load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\n</code></pre> <pre><code># Setup llm chain, initialize the retriever and llm, and setup Retrieval QA\nqa_dataset = setup_vector_db_and_qa(config=config, data_type=\"dataset\", client=client)\n</code></pre> <pre><code>queries = [\"Find datasets related to COVID-19\", \"Find datasets related to COVID-19 and India\", \"COVID-19 dataset\", \"COVID-19 dataset India\", \"Mexico historical covid\"]\ncombined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\", count = True)\n</code></pre> <pre><code>combined_df.head()\n</code></pre> <pre><code>queries = [\"Find datasets related to COVID-19\", \"Find datasets related to COVID-19 and India\", \"COVID-19 dataset\", \"COVID-19 dataset India\", \"Mexico historical covid\"]\ncombined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\", count = False)\n</code></pre> <pre><code>combined_df.head()\n</code></pre>"},{"location":"developer%20tutorials/run%20multiple%20queries%20and%20aggregate/#aggregate-results","title":"Aggregate results","text":""},{"location":"developer%20tutorials/run%20multiple%20queries%20and%20aggregate/#just-collate","title":"Just collate","text":""},{"location":"developer%20tutorials/train%20and%20evaluate%20models/","title":"Train and evaluate models","text":"<pre><code>from __future__ import annotations\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport pandas as pd\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import load_config_and_device\nfrom modules.llm import *\nfrom modules.results_gen import aggregate_multiple_queries_and_count\n</code></pre> <pre>\n<code>/Users/smukherjee/.pyenv/versions/3.10.14/envs/openml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</code>\n</pre> <pre><code>new_path = Path(\"../../backend/\")\n\nconfig = load_config_and_device(str(new_path / \"config.json\"), training = True)\n\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = True\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: mps\n</code>\n</pre> <pre><code># list_of_embedding_models = [\"BAAI/bge-small-en-v1.5\", \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"]\nlist_of_embedding_models = [\"BAAI/bge-large-en-v1.5\"]\nlist_of_llm_models = [\"qwen2:1.5b\", \"phi3\", \"llama3\"]\n</code></pre> <pre><code>def process_embedding_model_name_hf(name : str) -&amp;gt; str:\n    \"\"\"\n    Description: This function processes the name of the embedding model from Hugging Face to use as experiment name.\n\n    Input: name (str) - name of the embedding model from Hugging Face.\n\n    Returns: name (str) - processed name of the embedding model.\n    \"\"\"\n    return name.replace(\"/\", \"_\")\n\ndef process_llm_model_name_ollama(name : str) -&amp;gt; str:\n    \"\"\"\n    Description: This function processes the name of the llm model from Ollama to use as experiment name.\n\n    Input: name (str) - name of the llm model from Ollama.\n\n    Returns: name (str) - processed name of the llm model.\n    \"\"\"\n    return name.replace(\":\", \"_\")\n</code></pre> <pre><code># queries = [\"Find datasets related to COVID-19\", \"Find datasets related to COVID-19 and India\", \"COVID-19 dataset\", \"COVID-19 dataset India\", \"Mexico historical covid\"]\n</code></pre> <pre><code># download the ollama llm models\n\n# os.system(\"curl -fsSL https://ollama.com/install.sh | sh\")\nos.system(\"ollama serve&amp;amp;\")\nprint(\"Waiting for Ollama server to be active...\")  \nwhile os.system(\"ollama list | grep 'NAME'\") == \"\":\n    pass\n\nfor llm_model in list_of_llm_models:\n    os.system(f\"ollama pull {llm_model}\")\n</code></pre> <pre>\n<code>Error: listen tcp 127.0.0.1:11434: bind: address already in use\n</code>\n</pre> <pre>\n<code>Waiting for Ollama server to be active...\nNAME            ID              SIZE    MODIFIED      \n</code>\n</pre> <pre>\n<code>pulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \u283c pulling manifest \u2834 pulling manifest \u2826 pulling manifest \u2827 pulling manifest \u2807 pulling manifest \u280f pulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \u283c pulling manifest \u2834 pulling manifest \u2826 pulling manifest \u2827 pulling manifest \u2807 pulling manifest \u280f pulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \u283c pulling manifest \u2834 pulling manifest \u2826 pulling manifest \u2827 pulling manifest \u2807 pulling manifest \u280f pulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \u283c pulling manifest \u2834 pulling manifest \npulling 405b56374e02... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 934 MB                         \npulling 62fbfd9ed093... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  182 B                         \npulling c156170b718e... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  11 KB                         \npulling f02dd72bb242... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   59 B                         \npulling c9f5e9ffbc5f... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  485 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \nsuccess \npulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \u283c pulling manifest \npulling 3e38718d00bb... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.2 GB                         \npulling fa8235e5b48f... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.1 KB                         \npulling 542b217f179c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  148 B                         \npulling 8dde1baf1db0... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   78 B                         \npulling ed7ab7698fdd... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  483 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \nsuccess \npulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \u283c pulling manifest \npulling 6a0746a1ec1a... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.7 GB                         \npulling 4fa551d4f938... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  12 KB                         \npulling 8ab4849b038c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  254 B                         \npulling 577073ffcc6c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  110 B                         \npulling 3f8eb4da87fa... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  485 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \u280b pulling manifest \npulling 6a0746a1ec1a... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.7 GB                         \npulling 4fa551d4f938... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  12 KB                         \npulling 8ab4849b038c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  254 B                         \npulling 577073ffcc6c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  110 B                         \npulling 3f8eb4da87fa... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  485 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \nsuccess \n</code>\n</pre> <pre><code>def setup_vector_db_and_qa(\n    config: dict, data_type: str, client: ClientAPI, subset_ids: list = None\n):\n    \"\"\"\n    Description: Create the vector database using Chroma db with each type of data in its own collection. Doing so allows us to have a single database with multiple collections, reducing the number of databases we need to manage.\n    This also downloads the embedding model if it does not exist. The QA chain is then initialized with the vector store and the configuration.\n\n    Input: config (dict), data_type (str), client (chromadb.PersistentClient)\n\n    Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)\n    \"\"\"\n\n    config[\"type_of_data\"] = data_type\n\n    # Download the data if it does not exist\n    openml_data_object, data_id, all_metadata, handler = get_all_metadata_from_openml(\n        config=config\n    )\n    # Create the combined metadata dataframe\n    metadata_df, all_metadata = create_metadata_dataframe(\n        handler, openml_data_object, data_id, all_metadata, config=config\n    )\n\n    # subset the metadata if subset_ids is not None\n    if subset_ids is not None:\n        metadata_df = metadata_df[metadata_df[\"did\"].isin(subset_ids)]\n\n    # Create the vector store\n    vectordb = load_document_and_create_vector_store(\n        metadata_df, config=config, chroma_client=client\n    )\n    # Initialize the LLM chain and setup Retrieval QA\n    qa = initialize_llm_chain(vectordb=vectordb, config=config)\n    return qa, all_metadata\n</code></pre> <pre><code># use a tiny subset of the data for testing (no need for the edited subset)\n</code></pre> <pre><code>use_custom_data = False\n\n\nif use_custom_data:\n    config[\"test_subset\"] = False\nelse:\n    config[\"test_subset\"] = True\n\nfor embedding_model in tqdm(list_of_embedding_models, desc=\"Embedding Models\", total=len(list_of_embedding_models)):\n    for llm_model in tqdm(list_of_llm_models, desc=\"LLM Models\", total=len(list_of_llm_models)):\n        # update the config with the new embedding and llm models\n        config[\"embedding_model\"] = embedding_model\n        config[\"llm_model\"] = llm_model\n\n        # create a new experiment directory using a combination of the embedding model and llm model names\n        experiment_name = f\"{process_embedding_model_name_hf(embedding_model)}_{process_llm_model_name_ollama(llm_model)}\"\n        experiment_path = new_path/Path(f\"../data/experiments/{experiment_name}\")\n\n        # create the experiment directory if it does not exist\n        os.makedirs(experiment_path, exist_ok=True)\n\n        # update the config with the new experiment directories\n        config[\"data_dir\"] = str(experiment_path)\n        config[\"persist_dir\"] = str(experiment_path / \"chroma_db\")\n\n        # save training details and config in a dataframe\n        config_df = pd.DataFrame.from_dict(config, orient='index').reset_index()\n        config_df.columns = ['Hyperparameter', 'Value']\n        config_df.to_csv(experiment_path / \"config.csv\", index=False)\n\n        # load the persistent database using ChromaDB\n        client = chromadb.PersistentClient(path=config[\"persist_dir\"])\n\n        # Run \"training\"\n        qa_dataset = setup_vector_db_and_qa(\n            config=config, data_type=config[\"type_of_data\"], client=client\n        )\n\n        # # Run an evaluation by aggregating multiple queries and counting the results\n        # # TODO : Replace this evaluation with a more meaningful one\n        combined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\", count = False)\n\n        # # TODO : ADD LLM evaluation here when the function is ready\n\n        combined_df.to_csv(experiment_path / \"results.csv\")\n</code></pre> <pre>\n<code>Embedding Models:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>[INFO] Training is set to True.\n[INFO] Subsetting the data.\n[INFO] Initializing cache.\n[INFO] Getting dataset metadata from OpenML.\n</code>\n</pre> <pre>\n<code>\nQUEUEING TASKS | : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:00&lt;00:00, 61440.60it/s]\n\n\n\n\nPROCESSING TASKS | : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:00&lt;00:00, 582.60it/s]\n\nCOLLECTING RESULTS | : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:00&lt;00:00, 410803.53it/s]\n</code>\n</pre> <pre>\n<code>[INFO] Saving metadata to file.\n[INFO] Loading model...\n[INFO] Model loaded.\n[INFO] Generating unique documents. Total documents: 11676\nNumber of unique documents: 9684 vs Total documents: 11676\n</code>\n</pre> <pre>\n<code>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [01:03&lt;00:00,  3.94s/it]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:57&lt;00:00,  3.57s/it]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [01:06&lt;00:00,  4.14s/it]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [01:10&lt;00:00,  4.39s/it]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [01:05&lt;00:00,  4.10s/it]\n\n\n\n\n\n\n\n\n\n\n\n\n\n</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#tutorial-on-using-multiple-models-for-evaluation","title":"Tutorial on using multiple models for evaluation","text":"<ul> <li>This tutorial is an example of how to test multiple models on the openml data to see which one performs the best.</li> <li>The evaluation is still a bit basic, but it is a good starting point for future research.</li> </ul>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#setting-the-config","title":"Setting the config","text":""},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#defining-the-models-used","title":"Defining the models used","text":"<ul> <li>Embedding models are any from Huggingface hub</li> <li>LLM models are any from Ollama library</li> </ul>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#defining-the-evaluation-queries","title":"Defining the evaluation queries","text":"<ul> <li>replace this with a proper dataframe for a more comprehensive evaluation</li> </ul>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#downloading-the-models","title":"Downloading the models","text":"<ul> <li>PLEASE MAKE SURE YOU HAVE DOWNLOADED OLLAMA (<code>curl -fsSL https://ollama.com/install.sh | sh</code>)</li> </ul>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#running-the-steps","title":"Running the steps","text":"<ul> <li>Create an experiment directory</li> <li>Save a config file with the models and the queries in the experiment directory</li> <li>Download openml data for each dataset and format into a string</li> <li>Create vectorb and embed the data</li> <li>Get the predictions for each model for a list of queries and evaluate the performance</li> <li>(note) At the moment, this runs for a very small subset of the entire data. To disable this behavior and run on the entire data, set <code>config[\"test_subset_2000\"] = False</code></li> </ul>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#override-setup_vector_db_and_qa-to-use-a-list-of-ids-instead-of-all-of-them","title":"Override setup_vector_db_and_qa to use a list of IDs instead of all of them","text":""},{"location":"modules/general_utils/","title":"General utils","text":""},{"location":"modules/general_utils/#general_utils.find_device","title":"<code>find_device(training=False)</code>","text":"<p>Description: Find the device to use for the pipeline. If cuda is available, use it. If not, check if MPS is available and use it. If not, use CPU.</p> <p>Input: training (bool) : Whether the pipeline is being used for training or not.</p> <p>Returns: device (str) : The device to use for the pipeline.</p> Source code in <code>backend/modules/general_utils.py</code> <pre><code>def find_device(training: bool = False) -&gt; str:\n    \"\"\"\n    Description: Find the device to use for the pipeline. If cuda is available, use it. If not, check if MPS is available and use it. If not, use CPU.\n\n    Input: training (bool) : Whether the pipeline is being used for training or not.\n\n    Returns: device (str) : The device to use for the pipeline.\n    \"\"\"\n    print(\"[INFO] Finding device.\")\n    if torch.cuda.is_available():\n        return \"cuda\"\n    elif torch.backends.mps.is_available():\n        return \"mps\"\n    else:\n        return \"cpu\"\n</code></pre>"},{"location":"modules/general_utils/#general_utils.load_config_and_device","title":"<code>load_config_and_device(config_file, training=False)</code>","text":"<p>Description: Load the config file and find the device to use for the pipeline.</p> <p>Input: config_file (str) : The path to the config file. training (bool) : Whether the pipeline is being used for training or not.</p> <p>Returns: config (dict) : The config dictionary + device (str) : The device to use for the pipeline.</p> Source code in <code>backend/modules/general_utils.py</code> <pre><code>def load_config_and_device(config_file: str, training: bool = False) -&gt; dict:\n    \"\"\"\n    Description: Load the config file and find the device to use for the pipeline.\n\n    Input: config_file (str) : The path to the config file.\n    training (bool) : Whether the pipeline is being used for training or not.\n\n    Returns: config (dict) : The config dictionary + device (str) : The device to use for the pipeline.\n    \"\"\"\n    # Check if the config file exists and load it\n    if not os.path.exists(config_file):\n        raise Exception(\"Config file does not exist.\")\n    with open(config_file, \"r\") as f:\n        config = json.load(f)\n\n    # Find device and set it in the config between cpu and cuda and mps if available\n    config[\"device\"] = find_device(training)\n    print(f\"[INFO] Device found: {config['device']}\")\n    return config\n</code></pre>"},{"location":"modules/llm_module/","title":"Llm module","text":""},{"location":"modules/llm_module/#llm.add_documents_to_db","title":"<code>add_documents_to_db(db, unique_docs, unique_ids)</code>","text":"<p>Description: Add documents to the vector store in batches of 200.</p> <p>Input: db (Chroma), unique_docs (list), unique_ids (list)</p> <p>Returns: None</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def add_documents_to_db(db, unique_docs, unique_ids):\n    \"\"\"\n    Description: Add documents to the vector store in batches of 200.\n\n    Input: db (Chroma), unique_docs (list), unique_ids (list)\n\n    Returns: None\n    \"\"\"\n    bs = 512\n    if len(unique_docs) &lt; bs:\n        db.add_documents(unique_docs, ids=unique_ids)\n    else:\n        for i in tqdm(range(0, len(unique_docs), bs)):\n            db.add_documents(unique_docs[i : i + bs], ids=unique_ids[i : i + bs])\n</code></pre>"},{"location":"modules/llm_module/#llm.create_vector_store","title":"<code>create_vector_store(metadata_df, chroma_client, config, embeddings, collection_name)</code>","text":"<p>Description: Create the vector store using Chroma db. The documents are loaded and processed, unique documents are generated, and the documents are added to the vector store.</p> <p>Input: metadata_df (pd.DataFrame), chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)</p> <p>Returns: db (Chroma)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def create_vector_store(\n    metadata_df: pd.DataFrame,\n    chroma_client: ClientAPI,\n    config: dict,\n    embeddings: HuggingFaceEmbeddings,\n    collection_name: str,\n) -&gt; Chroma:\n    \"\"\"\n    Description: Create the vector store using Chroma db. The documents are loaded and processed, unique documents are generated, and the documents are added to the vector store.\n\n    Input: metadata_df (pd.DataFrame), chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)\n\n    Returns: db (Chroma)\n    \"\"\"\n\n    db = Chroma(\n        client=chroma_client,\n        embedding_function=embeddings,\n        persist_directory=config[\"persist_dir\"],\n        collection_name=collection_name,\n    )\n\n    documents = load_and_process_data(\n        metadata_df, page_content_column=\"Combined_information\"\n    )\n    if config[\"testing_flag\"]:\n        # subset the data for testing\n        if config[\"test_subset\"] == True:\n            print(\"[INFO] Subsetting the data.\")\n            documents = documents[:500]\n    unique_docs, unique_ids = generate_unique_documents(documents, db)\n\n    print(\n        f\"Number of unique documents: {len(unique_docs)} vs Total documents: {len(documents)}\"\n    )\n    if len(unique_docs) == 0:\n        print(\"No new documents to add.\")\n        return db\n    else:\n        # db.add_documents(unique_docs, ids=unique_ids)\n        add_documents_to_db(db, unique_docs, unique_ids)\n\n    return db\n</code></pre>"},{"location":"modules/llm_module/#llm.generate_unique_documents","title":"<code>generate_unique_documents(documents, db)</code>","text":"Generate unique documents by removing duplicates. This is done by generating unique IDs for the documents and keeping only one of the duplicate IDs. <p>Source: https://stackoverflow.com/questions/76265631/chromadb-add-single-document-only-if-it-doesnt-exist</p> <p>Input: documents (list)</p> <p>Returns: unique_docs (list), unique_ids (list)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def generate_unique_documents(documents: list, db: Chroma) -&gt; tuple:\n    \"\"\"\n    Description: Generate unique documents by removing duplicates. This is done by generating unique IDs for the documents and keeping only one of the duplicate IDs.\n        Source: https://stackoverflow.com/questions/76265631/chromadb-add-single-document-only-if-it-doesnt-exist\n\n    Input: documents (list)\n\n    Returns: unique_docs (list), unique_ids (list)\n    \"\"\"\n\n    # Remove duplicates based on ID (from database)\n    new_document_ids = set([str(x.metadata[\"did\"]) for x in documents])\n    print(f\"[INFO] Generating unique documents. Total documents: {len(documents)}\")\n    try:\n        old_dids = set([str(x[\"did\"]) for x in db.get()[\"metadatas\"]])\n    except KeyError:\n        old_dids = set([str(x[\"id\"]) for x in db.get()[\"metadatas\"]])\n\n    new_dids = new_document_ids - old_dids\n    documents = [x for x in documents if str(x.metadata[\"did\"]) in new_dids]\n    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in documents]\n\n    # Remove duplicates based on document content (from new documents)\n    unique_ids = list(set(ids))\n    seen_ids = set()\n    unique_docs = [\n        doc\n        for doc, id in zip(documents, ids)\n        if id not in seen_ids and (seen_ids.add(id) or True)\n    ]\n\n    return unique_docs, unique_ids\n</code></pre>"},{"location":"modules/llm_module/#llm.get_collection_name","title":"<code>get_collection_name(config)</code>","text":"<p>Description: Get the collection name based on the type of data provided in the config.</p> <p>Input: config (dict)</p> <p>Returns: str</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def get_collection_name(config: dict) -&gt; str:\n    \"\"\"\n    Description: Get the collection name based on the type of data provided in the config.\n\n    Input: config (dict)\n\n    Returns: str\n    \"\"\"\n    return {\"dataset\": \"datasets\", \"flow\": \"flows\"}.get(\n        config[\"type_of_data\"], \"default\"\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.get_llm_chain","title":"<code>get_llm_chain(config, local=False)</code>","text":"<p>Description: Get the LLM chain with the specified model and prompt template.</p> <p>Input: config (dict)</p> <p>Returns: LLMChain</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def get_llm_chain(config: dict, local: bool = False) -&gt; LLMChain | bool:\n    \"\"\"\n    Description: Get the LLM chain with the specified model and prompt template.\n\n    Input: config (dict)\n\n    Returns: LLMChain\n    \"\"\"\n    base_url = \"http://127.0.0.1:11434\" if local else \"http://ollama:11434\"\n    llm = Ollama(model=config[\"llm_model\"], base_url=base_url)\n    # llm = Ollama(\n    # model = config[\"llm_model\"]\n    # )\n    # print(llm)\n    map_template = config[\"llm_prompt_template\"]\n    map_prompt = PromptTemplate.from_template(map_template)\n    # return LLMChain(llm=llm, prompt=map_prompt)\n    return map_prompt | llm | StrOutputParser()\n</code></pre>"},{"location":"modules/llm_module/#llm.initialize_llm_chain","title":"<code>initialize_llm_chain(vectordb, config)</code>","text":"<p>Description: Initialize the LLM chain and setup Retrieval QA with the specified configuration.</p> <p>Input: vectordb (Chroma), config (dict)</p> <p>Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def initialize_llm_chain(\n    vectordb: Chroma, config: dict\n) -&gt; langchain.chains.retrieval_qa.base.RetrievalQA:\n    \"\"\"\n    Description: Initialize the LLM chain and setup Retrieval QA with the specified configuration.\n\n    Input: vectordb (Chroma), config (dict)\n\n    Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)\n    \"\"\"\n\n    return vectordb.as_retriever(\n        search_type=config[\"search_type\"],\n        search_kwargs={\"k\": config[\"num_return_documents\"]},\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.load_and_process_data","title":"<code>load_and_process_data(metadata_df, page_content_column)</code>","text":"<p>Description: Load and process the data for the vector store. Split the documents into chunks of 1000 characters.</p> <p>Input: metadata_df (pd.DataFrame), page_content_column (str)</p> <p>Returns: chunked documents (list)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def load_and_process_data(metadata_df: pd.DataFrame, page_content_column: str) -&gt; list:\n    \"\"\"\n    Description: Load and process the data for the vector store. Split the documents into chunks of 1000 characters.\n\n    Input: metadata_df (pd.DataFrame), page_content_column (str)\n\n    Returns: chunked documents (list)\n    \"\"\"\n    # Load data\n    loader = DataFrameLoader(metadata_df, page_content_column=page_content_column)\n    documents = loader.load()\n\n    # Split documents\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n    documents = text_splitter.split_documents(documents)\n\n    return documents\n</code></pre>"},{"location":"modules/llm_module/#llm.load_document_and_create_vector_store","title":"<code>load_document_and_create_vector_store(metadata_df, chroma_client, config)</code>","text":"<p>Loads the documents and creates the vector store. If the training flag is set to True, the documents are added to the vector store. If the training flag is set to False, the vector store is loaded from the persist directory.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_df</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required <code>chroma_client</code> <code>PersistentClient</code> <p>The Chroma client.</p> required <code>config</code> <code>dict</code> <p>The configuration dictionary.</p> required <p>Returns:</p> Name Type Description <code>Chroma</code> <code>Chroma</code> <p>The Chroma vector store.</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def load_document_and_create_vector_store(\n    metadata_df: pd.DataFrame, chroma_client: ClientAPI, config: dict\n) -&gt; Chroma:\n    \"\"\"\n    Loads the documents and creates the vector store. If the training flag is set to True,\n    the documents are added to the vector store. If the training flag is set to False,\n    the vector store is loaded from the persist directory.\n\n    Args:\n        metadata_df (pd.DataFrame): The metadata dataframe.\n        chroma_client (chromadb.PersistentClient): The Chroma client.\n        config (dict): The configuration dictionary.\n\n    Returns:\n        Chroma: The Chroma vector store.\n    \"\"\"\n    embeddings = load_model(config)\n    collection_name = get_collection_name(config)\n\n    if not config[\"training\"]:\n        return load_vector_store(chroma_client, config, embeddings, collection_name)\n\n    return create_vector_store(\n        metadata_df, chroma_client, config, embeddings, collection_name\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.load_model","title":"<code>load_model(config)</code>","text":"<p>Description: Load the model using HuggingFaceEmbeddings.</p> <p>Input: config (dict)</p> <p>Returns: HuggingFaceEmbeddings</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def load_model(config: dict) -&gt; HuggingFaceEmbeddings | None:\n    \"\"\"\n    Description: Load the model using HuggingFaceEmbeddings.\n\n    Input: config (dict)\n\n    Returns: HuggingFaceEmbeddings\n    \"\"\"\n    print(\"[INFO] Loading model...\")\n    model_kwargs = {\"device\": config[\"device\"], \"trust_remote_code\": True}\n    encode_kwargs = {\"normalize_embeddings\": True}\n    embeddings = HuggingFaceEmbeddings(\n        model_name=config[\"embedding_model\"],\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs,\n        show_progress=True,\n        # trust_remote_code=True\n    )\n    print(\"[INFO] Model loaded.\")\n    return embeddings\n</code></pre>"},{"location":"modules/llm_module/#llm.load_vector_store","title":"<code>load_vector_store(chroma_client, config, embeddings, collection_name)</code>","text":"<p>Description: Load the vector store from the persist directory.</p> <p>Input: chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)</p> <p>Returns: Chroma</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def load_vector_store(\n    chroma_client: ClientAPI,\n    config: dict,\n    embeddings: HuggingFaceEmbeddings,\n    collection_name: str,\n) -&gt; Chroma:\n    \"\"\"\n    Description: Load the vector store from the persist directory.\n\n    Input: chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)\n\n    Returns: Chroma\n    \"\"\"\n    if not os.path.exists(config[\"persist_dir\"]):\n        raise Exception(\n            \"Persist directory does not exist. Please run the training pipeline first.\"\n        )\n\n    return Chroma(\n        client=chroma_client,\n        persist_directory=config[\"persist_dir\"],\n        embedding_function=embeddings,\n        collection_name=collection_name,\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.setup_vector_db_and_qa","title":"<code>setup_vector_db_and_qa(config, data_type, client)</code>","text":"<p>Description: Create the vector database using Chroma db with each type of data in its own collection. Doing so allows us to have a single database with multiple collections, reducing the number of databases we need to manage. This also downloads the embedding model if it does not exist. The QA chain is then initialized with the vector store and the configuration.</p> <p>Input: config (dict), data_type (str), client (chromadb.PersistentClient)</p> <p>Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def setup_vector_db_and_qa(\n    config: dict, data_type: str, client: ClientAPI\n) -&gt; Union[langchain.chains.retrieval_qa.base.RetrievalQA, pd.DataFrame]:\n    \"\"\"\n    Description: Create the vector database using Chroma db with each type of data in its own collection. Doing so allows us to have a single database with multiple collections, reducing the number of databases we need to manage.\n    This also downloads the embedding model if it does not exist. The QA chain is then initialized with the vector store and the configuration.\n\n    Input: config (dict), data_type (str), client (chromadb.PersistentClient)\n\n    Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)\n    \"\"\"\n\n    config[\"type_of_data\"] = data_type\n\n    # Download the data if it does not exist\n    openml_data_object, data_id, all_metadata, handler = get_all_metadata_from_openml(\n        config=config\n    )\n    # Create the combined metadata dataframe\n    metadata_df, all_metadata = create_metadata_dataframe(\n        handler, openml_data_object, data_id, all_metadata, config=config\n    )\n    # Create the vector store\n    vectordb = load_document_and_create_vector_store(\n        metadata_df, config=config, chroma_client=client\n    )\n    # Initialize the LLM chain and setup Retrieval QA\n    qa = initialize_llm_chain(vectordb=vectordb, config=config)\n    return qa, all_metadata\n</code></pre>"},{"location":"modules/metadata_module/","title":"Metadata module","text":""},{"location":"modules/metadata_module/#metadata_utils.OpenMLDatasetHandler","title":"<code>OpenMLDatasetHandler</code>","text":"<p>               Bases: <code>OpenMLObjectHandler</code></p> <p>Description: The class for handling OpenML dataset objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>class OpenMLDatasetHandler(OpenMLObjectHandler):\n    \"\"\"\n    Description: The class for handling OpenML dataset objects.\n    \"\"\"\n\n    def get_description(self, data_id: int):\n        return openml.datasets.get_dataset(\n            dataset_id=data_id,\n            download_data=False,\n            download_qualities=True,\n            download_features_meta_data=True,\n        )\n\n    def get_openml_objects(self):\n        return openml.datasets.list_datasets(output_format=\"dataframe\")\n\n    def process_metadata(\n        self,\n        openml_data_object: Sequence[openml.datasets.dataset.OpenMLDataset],\n        data_id: Sequence[int],\n        all_dataset_metadata: pd.DataFrame,\n        file_path: str,\n    ):\n        descriptions = [\n            extract_attribute(attr, \"description\") for attr in openml_data_object\n        ]\n        joined_qualities = [\n            join_attributes(attr, \"qualities\") for attr in openml_data_object\n        ]\n        joined_features = [\n            join_attributes(attr, \"features\") for attr in openml_data_object\n        ]\n\n        all_data_description_df = create_combined_information_df(\n            data_id, descriptions, joined_qualities, joined_features\n        )\n        all_dataset_metadata = combine_metadata(\n            all_dataset_metadata, all_data_description_df\n        )\n\n        all_dataset_metadata.to_csv(file_path)\n\n        return (\n            all_dataset_metadata[[\"did\", \"name\", \"Combined_information\"]],\n            all_dataset_metadata,\n        )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.OpenMLFlowHandler","title":"<code>OpenMLFlowHandler</code>","text":"<p>               Bases: <code>OpenMLObjectHandler</code></p> <p>Description: The class for handling OpenML flow objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>class OpenMLFlowHandler(OpenMLObjectHandler):\n    \"\"\"\n    Description: The class for handling OpenML flow objects.\n    \"\"\"\n\n    def get_description(self, data_id: int):\n        return openml.flows.get_flow(flow_id=data_id)\n\n    def get_openml_objects(self):\n        all_objects = openml.flows.list_flows(output_format=\"dataframe\")\n        return all_objects.rename(columns={\"id\": \"did\"})\n\n    def process_metadata(\n        self,\n        openml_data_object: Sequence[openml.flows.flow.OpenMLFlow],\n        data_id: Sequence[int],\n        all_dataset_metadata: pd.DataFrame,\n        file_path: str,\n    ):\n        descriptions = [\n            extract_attribute(attr, \"description\") for attr in openml_data_object\n        ]\n        names = [extract_attribute(attr, \"name\") for attr in openml_data_object]\n        tags = [extract_attribute(attr, \"tags\") for attr in openml_data_object]\n\n        all_data_description_df = pd.DataFrame(\n            {\n                \"did\": data_id,\n                \"description\": descriptions,\n                \"name\": names,\n                \"tags\": tags,\n            }\n        )\n\n        all_data_description_df[\"Combined_information\"] = all_data_description_df.apply(\n            merge_all_columns_to_string, axis=1\n        )\n        all_data_description_df.to_csv(file_path)\n\n        return (\n            all_data_description_df[[\"did\", \"name\", \"Combined_information\"]],\n            all_data_description_df,\n        )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.OpenMLObjectHandler","title":"<code>OpenMLObjectHandler</code>","text":"<p>Description: The base class for handling OpenML objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>class OpenMLObjectHandler:\n    \"\"\"\n    Description: The base class for handling OpenML objects.\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.collection_name = \"\"\n\n    def get_description(self, data_id: int):\n        \"\"\"\n        Description: Get the description of the OpenML object.\n\n        Input: data_id (int) : The data id\n\n        Returns: The OpenML object.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_openml_objects(self):\n        \"\"\"\n        Description: Get the OpenML objects.\n\n        Input: None\n\n        Returns: The OpenML objects.\n        \"\"\"\n        raise NotImplementedError\n\n    def initialize_cache(self, data_id: Sequence[int]) -&gt; None:\n        \"\"\"\n        Description: Initialize the cache for the OpenML objects.\n\n        Input: data_id (list) : The list of data ids\n\n        Returns: None\n        \"\"\"\n        self.get_description(data_id[0])\n\n    def get_metadata(self, data_id: Sequence[int]):\n        \"\"\"\n        Description: Get metadata from OpenML using parallel processing.\n\n        Input: data_id (list) : The list of data ids\n\n        Returns: The OpenML objects.\n        \"\"\"\n        return pqdm(\n            data_id, self.get_description, n_jobs=self.config[\"data_download_n_jobs\"]\n        )\n\n    def process_metadata(\n        self,\n        openml_data_object,\n        data_id: Sequence[int],\n        all_dataset_metadata: pd.DataFrame,\n        file_path: str,\n    ):\n        \"\"\"\n        Description: Process the metadata.\n\n        Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, all_dataset_metadata (pd.DataFrame) : The metadata table, file_path (str) : The file path\n\n        Returns: The combined metadata dataframe and the updated metadata table.\n        \"\"\"\n        raise NotImplementedError\n\n    def load_metadata(self, file_path: str):\n        \"\"\"\n        Description: Load metadata from a file.\n\n        Input: file_path (str) : The file path\n\n        Returns: The metadata dataframe.\n        \"\"\"\n        try:\n            return pd.read_csv(file_path)\n        except FileNotFoundError:\n            raise Exception(\n                \"Metadata files do not exist. Please run the training pipeline first.\"\n            )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.OpenMLObjectHandler.get_description","title":"<code>get_description(data_id)</code>","text":"<p>Description: Get the description of the OpenML object.</p> <p>Input: data_id (int) : The data id</p> <p>Returns: The OpenML object.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_description(self, data_id: int):\n    \"\"\"\n    Description: Get the description of the OpenML object.\n\n    Input: data_id (int) : The data id\n\n    Returns: The OpenML object.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.OpenMLObjectHandler.get_metadata","title":"<code>get_metadata(data_id)</code>","text":"<p>Description: Get metadata from OpenML using parallel processing.</p> <p>Input: data_id (list) : The list of data ids</p> <p>Returns: The OpenML objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_metadata(self, data_id: Sequence[int]):\n    \"\"\"\n    Description: Get metadata from OpenML using parallel processing.\n\n    Input: data_id (list) : The list of data ids\n\n    Returns: The OpenML objects.\n    \"\"\"\n    return pqdm(\n        data_id, self.get_description, n_jobs=self.config[\"data_download_n_jobs\"]\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.OpenMLObjectHandler.get_openml_objects","title":"<code>get_openml_objects()</code>","text":"<p>Description: Get the OpenML objects.</p> <p>Input: None</p> <p>Returns: The OpenML objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_openml_objects(self):\n    \"\"\"\n    Description: Get the OpenML objects.\n\n    Input: None\n\n    Returns: The OpenML objects.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.OpenMLObjectHandler.initialize_cache","title":"<code>initialize_cache(data_id)</code>","text":"<p>Description: Initialize the cache for the OpenML objects.</p> <p>Input: data_id (list) : The list of data ids</p> <p>Returns: None</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def initialize_cache(self, data_id: Sequence[int]) -&gt; None:\n    \"\"\"\n    Description: Initialize the cache for the OpenML objects.\n\n    Input: data_id (list) : The list of data ids\n\n    Returns: None\n    \"\"\"\n    self.get_description(data_id[0])\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.OpenMLObjectHandler.load_metadata","title":"<code>load_metadata(file_path)</code>","text":"<p>Description: Load metadata from a file.</p> <p>Input: file_path (str) : The file path</p> <p>Returns: The metadata dataframe.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def load_metadata(self, file_path: str):\n    \"\"\"\n    Description: Load metadata from a file.\n\n    Input: file_path (str) : The file path\n\n    Returns: The metadata dataframe.\n    \"\"\"\n    try:\n        return pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise Exception(\n            \"Metadata files do not exist. Please run the training pipeline first.\"\n        )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.OpenMLObjectHandler.process_metadata","title":"<code>process_metadata(openml_data_object, data_id, all_dataset_metadata, file_path)</code>","text":"<p>Description: Process the metadata.</p> <p>Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, all_dataset_metadata (pd.DataFrame) : The metadata table, file_path (str) : The file path</p> <p>Returns: The combined metadata dataframe and the updated metadata table.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def process_metadata(\n    self,\n    openml_data_object,\n    data_id: Sequence[int],\n    all_dataset_metadata: pd.DataFrame,\n    file_path: str,\n):\n    \"\"\"\n    Description: Process the metadata.\n\n    Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, all_dataset_metadata (pd.DataFrame) : The metadata table, file_path (str) : The file path\n\n    Returns: The combined metadata dataframe and the updated metadata table.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.combine_metadata","title":"<code>combine_metadata(all_dataset_metadata, all_data_description_df)</code>","text":"<p>Description: Combine the descriptions with the metadata table.</p> <p>Input: all_dataset_metadata (pd.DataFrame) : The metadata table, all_data_description_df (pd.DataFrame) : The descriptions</p> <p>Returns: The combined metadata table.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def combine_metadata(\n    all_dataset_metadata: pd.DataFrame, all_data_description_df: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Combine the descriptions with the metadata table.\n\n    Input: all_dataset_metadata (pd.DataFrame) : The metadata table,\n    all_data_description_df (pd.DataFrame) : The descriptions\n\n    Returns: The combined metadata table.\n    \"\"\"\n    # Combine the descriptions with the metadata table\n    all_dataset_metadata = pd.merge(\n        all_dataset_metadata, all_data_description_df, on=\"did\", how=\"inner\"\n    )\n\n    # Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n\n    all_dataset_metadata[\"Combined_information\"] = all_dataset_metadata.apply(\n        merge_all_columns_to_string, axis=1\n    )\n    return all_dataset_metadata\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.create_combined_information_df","title":"<code>create_combined_information_df(data_id, descriptions, joined_qualities, joined_features)</code>","text":"<p>Description: Create a dataframe with the combined information of the OpenML object.</p> <p>Input: data_id (int) : The data id, descriptions (list) : The descriptions of the OpenML object, joined_qualities (list) : The joined qualities of the OpenML object, joined_features (list) : The joined features of the OpenML object</p> <p>Returns: The dataframe with the combined information of the OpenML object.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def create_combined_information_df(\n    # data_id, descriptions, joined_qualities, joined_features\n    data_id: int | Sequence[int],\n    descriptions: Sequence[str],\n    joined_qualities: Sequence[str],\n    joined_features: Sequence[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Create a dataframe with the combined information of the OpenML object.\n\n    Input: data_id (int) : The data id, descriptions (list) : The descriptions of the OpenML object, joined_qualities (list) : The joined qualities of the OpenML object, joined_features (list) : The joined features of the OpenML object\n\n    Returns: The dataframe with the combined information of the OpenML object.\n    \"\"\"\n    return pd.DataFrame(\n        {\n            \"did\": data_id,\n            \"description\": descriptions,\n            \"qualities\": joined_qualities,\n            \"features\": joined_features,\n        }\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.create_metadata_dataframe","title":"<code>create_metadata_dataframe(handler, openml_data_object, data_id, all_dataset_metadata, config)</code>","text":"<p>Description: Creates a dataframe with all the metadata, joined columns with all information for the type of data specified in the config. If training is set to False, the dataframes are loaded from the files. If training is set to True, the dataframes are created and then saved to the files.</p> Input <p>handler (OpenMLObjectHandler): The handler for the OpenML objects. openml_data_object (list): The list of OpenML objects. data_id (list): The list of data ids. all_dataset_metadata (pd.DataFrame): The metadata table. config (dict): The config dictionary.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The combined metadata dataframe.</p> <code>DataFrame</code> <p>pd.DataFrame: The updated metadata table.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def create_metadata_dataframe(\n    # openml_data_object, data_id, all_dataset_metadata, config\n    handler: OpenMLObjectHandler,\n    openml_data_object: Sequence[\n        Union[openml.datasets.dataset.OpenMLDataset, openml.flows.flow.OpenMLFlow]\n    ],\n    data_id: Sequence[int],\n    all_dataset_metadata: pd.DataFrame,\n    config: dict,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Description: Creates a dataframe with all the metadata, joined columns with all information\n    for the type of data specified in the config. If training is set to False,\n    the dataframes are loaded from the files. If training is set to True, the\n    dataframes are created and then saved to the files.\n\n    Input:\n        handler (OpenMLObjectHandler): The handler for the OpenML objects.\n        openml_data_object (list): The list of OpenML objects.\n        data_id (list): The list of data ids.\n        all_dataset_metadata (pd.DataFrame): The metadata table.\n        config (dict): The config dictionary.\n\n    Returns:\n        pd.DataFrame: The combined metadata dataframe.\n        pd.DataFrame: The updated metadata table.\n    \"\"\"\n    # use os.path.join to ensure compatibility with different operating systems\n    file_path = os.path.join(\n        config[\"data_dir\"], f\"all_{config['type_of_data']}_description.csv\"\n    )\n\n    if not config[\"training\"]:\n        return handler.load_metadata(file_path), all_dataset_metadata\n\n    return handler.process_metadata(\n        openml_data_object, data_id, all_dataset_metadata, file_path\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.extract_attribute","title":"<code>extract_attribute(attribute, attr_name)</code>","text":"<p>Description: Extract an attribute from the OpenML object.</p> <p>Input: attribute (object) : The OpenML object</p> <p>Returns: The attribute value if it exists, else an empty string.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def extract_attribute(attribute: object, attr_name: str) -&gt; str:\n    \"\"\"\n    Description: Extract an attribute from the OpenML object.\n\n    Input: attribute (object) : The OpenML object\n\n    Returns: The attribute value if it exists, else an empty string.\n    \"\"\"\n    return getattr(attribute, attr_name, \"\")\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_all_metadata_from_openml","title":"<code>get_all_metadata_from_openml(config)</code>","text":"<p>Description: Gets all the metadata from OpenML for the type of data specified in the config. If training is set to False, it loads the metadata from the files. If training is set to True, it gets the metadata from OpenML.</p> <p>This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.</p> <p>Input: config (dict) : The config dictionary</p> <p>Returns: all the data descriptions combined with data ids, data ids, and the raw openml objects in a dataframe.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_all_metadata_from_openml(\n    config: dict,\n) -&gt; Tuple[pd.DataFrame, Sequence[int], pd.DataFrame] | None:\n    \"\"\"\n    Description: Gets all the metadata from OpenML for the type of data specified in the config.\n    If training is set to False, it loads the metadata from the files. If training is set to True, it gets the metadata from OpenML.\n\n    This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.\n\n\n    Input: config (dict) : The config dictionary\n\n    Returns: all the data descriptions combined with data ids, data ids, and the raw openml objects in a dataframe.\n    \"\"\"\n\n    # save_filename = f\"./data/all_{config['type_of_data']}_metadata.pkl\"\n    # use os.path.join to ensure compatibility with different operating systems\n    save_filename = os.path.join(\n        config[\"data_dir\"], f\"all_{config['type_of_data']}_metadata.pkl\"\n    )\n    # If we are not training, we do not need to recreate the cache and can load the metadata from the files. If the files do not exist, raise an exception.\n    # TODO : Check if this behavior is correct, or if data does not exist, send to training pipeline?\n    if config[\"training\"] == False or config[\"ignore_downloading_data\"] == True:\n        # print(\"[INFO] Training is set to False.\")\n        # Check if the metadata files exist for all types of data\n        if not os.path.exists(save_filename):\n            raise Exception(\n                \"Metadata files do not exist. Please run the training pipeline first.\"\n            )\n        print(\"[INFO] Loading metadata from file.\")\n        # Load the metadata files for all types of data\n        return load_metadata_from_file(save_filename)\n\n    # If we are training, we need to recreate the cache and get the metadata from OpenML\n    if config[\"training\"] == True:\n        print(\"[INFO] Training is set to True.\")\n        # Gather all OpenML objects of the type of data\n        handler = (\n            OpenMLDatasetHandler(config)\n            if config[\"type_of_data\"] == \"dataset\"\n            else OpenMLFlowHandler(config)\n        )\n\n        all_objects = handler.get_openml_objects()\n\n        # subset the data for testing\n        if config[\"test_subset\"] == True:\n            print(\"[INFO] Subsetting the data.\")\n            all_objects = all_objects[:500]\n\n        data_id = [int(all_objects.iloc[i][\"did\"]) for i in range(len(all_objects))]\n\n        print(\"[INFO] Initializing cache.\")\n        handler.initialize_cache(data_id)\n\n        print(f\"[INFO] Getting {config['type_of_data']} metadata from OpenML.\")\n        openml_data_object = handler.get_metadata(data_id)\n\n        print(\"[INFO] Saving metadata to file.\")\n        save_metadata_to_file(\n            (openml_data_object, data_id, all_objects, handler), save_filename\n        )\n\n        return openml_data_object, data_id, all_objects, handler\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.join_attributes","title":"<code>join_attributes(attribute, attr_name)</code>","text":"<p>Description: Join the attributes of the OpenML object.</p> <p>Input: attribute (object) : The OpenML object</p> <p>Returns: The joined attributes if they exist, else an empty string. example: \"column - value, column - value, ...\"</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def join_attributes(attribute: object, attr_name: str) -&gt; str:\n    \"\"\"\n    Description: Join the attributes of the OpenML object.\n\n    Input: attribute (object) : The OpenML object\n\n    Returns: The joined attributes if they exist, else an empty string.\n    example: \"column - value, column - value, ...\"\n    \"\"\"\n\n    return (\n        \" \".join([f\"{k} : {v},\" for k, v in getattr(attribute, attr_name, {}).items()])\n        if hasattr(attribute, attr_name)\n        else \"\"\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.load_metadata_from_file","title":"<code>load_metadata_from_file(save_filename)</code>","text":"<p>Load metadata from a file.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def load_metadata_from_file(\n    save_filename: str,\n) -&gt; Tuple[pd.DataFrame, Sequence[int], pd.DataFrame]:\n    \"\"\"\n    Load metadata from a file.\n    \"\"\"\n    with open(save_filename, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.merge_all_columns_to_string","title":"<code>merge_all_columns_to_string(row)</code>","text":"<p>Description: Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"</p> <p>Input: row (pd.Series) : The row of the dataframe</p> <p>Returns: The combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def merge_all_columns_to_string(row: pd.Series) -&gt; str:\n    \"\"\"\n    Description: Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n\n    Input: row (pd.Series) : The row of the dataframe\n\n    Returns: The combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n    \"\"\"\n\n    return \" \".join([f\"{col} - {val},\" for col, val in zip(row.index, row.values)])\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.save_metadata_to_file","title":"<code>save_metadata_to_file(data, save_filename)</code>","text":"<p>Save metadata to a file.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def save_metadata_to_file(data, save_filename: str):\n    \"\"\"\n    Save metadata to a file.\n    \"\"\"\n    with open(save_filename, \"wb\") as f:\n        pickle.dump(data, f)\n</code></pre>"},{"location":"modules/result_gen/","title":"Result gen","text":""},{"location":"modules/result_gen/#results_gen.aggregate_multiple_queries_and_count","title":"<code>aggregate_multiple_queries_and_count(queries, qa_dataset, config, group_cols=['id', 'name'], sort_by='query', count=True)</code>","text":"<p>Description: Aggregate the results of multiple queries into a single dataframe and count the number of times a dataset appears in the results</p> Input <p>queries: List of queries group_cols: List of columns to group by</p> <p>Returns: Combined dataframe with the results of all queries</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def aggregate_multiple_queries_and_count(\n    queries, qa_dataset, config, group_cols=[\"id\", \"name\"], sort_by=\"query\", count=True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Aggregate the results of multiple queries into a single dataframe and count the number of times a dataset appears in the results\n\n    Input:\n        queries: List of queries\n        group_cols: List of columns to group by\n\n    Returns: Combined dataframe with the results of all queries\n    \"\"\"\n    combined_df = pd.DataFrame()\n    for query in tqdm(queries, total=len(queries)):\n        result_data_frame, _ = get_result_from_query(\n            query=query, qa=qa_dataset, type_of_query=\"dataset\", config=config\n        )\n        result_data_frame = result_data_frame[group_cols]\n        # Concat with combined_df with a column to store the query\n        result_data_frame[\"query\"] = query\n        result_data_frame[\"llm_model\"] = config[\"llm_model\"]\n        result_data_frame[\"embedding_model\"] = config[\"embedding_model\"]\n        combined_df = pd.concat([combined_df, result_data_frame])\n    if count:\n        combined_df = (\n            combined_df.groupby(group_cols)\n            .count()\n            .reset_index()\n            .sort_values(by=sort_by, ascending=False)\n        )\n\n    return combined_df\n</code></pre>"},{"location":"modules/result_gen/#results_gen.check_query","title":"<code>check_query(query)</code>","text":"<p>Description: Performs checks on the query - Replaces %20 with space character (browsers do this automatically when spaces are in the URL) - Removes leading and trailing spaces - Limits the query to 150 characters</p> <p>Input: query (str)</p> <p>Returns: None</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def check_query(query: str) -&gt; str:\n    \"\"\"\n    Description: Performs checks on the query\n    - Replaces %20 with space character (browsers do this automatically when spaces are in the URL)\n    - Removes leading and trailing spaces\n    - Limits the query to 150 characters\n\n    Input: query (str)\n\n    Returns: None\n    \"\"\"\n    if query == \"\":\n        raise ValueError(\"Query cannot be empty.\")\n    query = query.replace(\n        \"%20\", \" \"\n    )  # replace %20 with space character (browsers do this automatically when spaces are in the URL)\n    # query = query.replace(\"dataset\", \"\")\n    # query = query.replace(\"flow\", \"\")\n    query = query.strip()\n    query = query[:200]\n    return query\n</code></pre>"},{"location":"modules/result_gen/#results_gen.create_output_dataframe","title":"<code>create_output_dataframe(dict_results, type_of_data, ids_order)</code>","text":"<p>Description: Create an output dataframe with the results. The URLs are API calls to the OpenML API for the specific type of data.</p> <p>Input: dict_results (dict), type_of_data (str)</p> <p>Returns: A dataframe with the results and duplicate names removed.</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def create_output_dataframe(\n    dict_results: dict, type_of_data: str, ids_order: list\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Create an output dataframe with the results. The URLs are API calls to the OpenML API for the specific type of data.\n\n    Input: dict_results (dict), type_of_data (str)\n\n    Returns: A dataframe with the results and duplicate names removed.\n    \"\"\"\n    output_df = pd.DataFrame(dict_results).T.reset_index()\n    # order the rows based on the order of the ids\n    output_df[\"index\"] = output_df[\"index\"].astype(int)\n    output_df = output_df.set_index(\"index\").loc[ids_order].reset_index()\n    # output_df[\"urls\"] = output_df[\"index\"].apply(\n    #     lambda x: f\"https://www.openml.org/api/v1/json/{type_of_data}/{x}\"\n    # )\n    # https://www.openml.org/search?type=data&amp;sort=runs&amp;status=any&amp;id=31\n    output_df[\"urls\"] = output_df[\"index\"].apply(\n        lambda x: f\"https://www.openml.org/search?type={type_of_data}&amp;id={x}\"\n    )\n    output_df[\"urls\"] = output_df[\"urls\"].apply(make_clickable)\n    # data = openml.datasets.get_dataset(\n    # get rows with unique names\n    if type_of_data == \"data\":\n        output_df[\"command\"] = output_df[\"index\"].apply(\n            lambda x: f\"dataset = openml.datasets.get_dataset({x})\"\n        )\n    elif type_of_data == \"flow\":\n        output_df[\"command\"] = output_df[\"index\"].apply(\n            lambda x: f\"flow = openml.flows.get_flow({x})\"\n        )\n    output_df = output_df.drop_duplicates(subset=[\"name\"])\n    # order the columns\n    output_df = output_df[[\"index\", \"name\", \"command\", \"urls\", \"page_content\"]].rename(\n        columns={\"index\": \"id\", \"urls\": \"OpenML URL\", \"page_content\": \"Description\"}\n    )\n    return output_df\n</code></pre>"},{"location":"modules/result_gen/#results_gen.fetch_results","title":"<code>fetch_results(query, qa, type_of_query, config)</code>","text":"<p>Description: Fetch results for the query using the QA chain.</p> <p>Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str), config (dict)</p> <p>Returns: results[\"source_documents\"] (list)</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def fetch_results(\n    query: str,\n    qa: langchain.chains.retrieval_qa.base.RetrievalQA,\n    type_of_query: str,\n    config: dict,\n) -&gt; Sequence[Document]:\n    \"\"\"\n    Description: Fetch results for the query using the QA chain.\n\n    Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str), config (dict)\n\n    Returns: results[\"source_documents\"] (list)\n    \"\"\"\n    results = qa.invoke(\n        input=query,\n        config={\"temperature\": config[\"temperature\"], \"top-p\": config[\"top_p\"]},\n    )\n    if config[\"long_context_reorder\"] == True:\n        results = long_context_reorder(results)\n    id_column = {\"dataset\": \"did\", \"flow\": \"id\", \"data\": \"did\"}\n    id_column = id_column[type_of_query]\n\n    if config[\"reranking\"] == True:\n        try:\n            print(\"[INFO] Reranking results...\")\n            ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/tmp/\")\n            rerankrequest = RerankRequest(\n                query=query,\n                passages=[\n                    {\"id\": result.metadata[id_column], \"text\": result.page_content}\n                    for result in results\n                ],\n            )\n            ranking = ranker.rerank(rerankrequest)\n            ids = [result[\"id\"] for result in ranking]\n            ranked_results = [\n                result for result in results if result.metadata[id_column] in ids\n            ]\n            print(\"[INFO] Reranking complete.\")\n            return ranked_results\n        except Exception as e:\n            print(f\"[ERROR] Reranking failed: {e}\")\n            return results\n\n    else:\n        return results\n</code></pre>"},{"location":"modules/result_gen/#results_gen.get_result_from_query","title":"<code>get_result_from_query(query, qa, type_of_query, config)</code>","text":"<p>Description: Get the result from the query using the QA chain and return the results in a dataframe that is then sent to the frontend.</p> <p>Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str)</p> <p>Returns: output_df (pd.DataFrame)</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def get_result_from_query(\n    query, qa, type_of_query, config\n) -&gt; Tuple[pd.DataFrame, Sequence[Document]]:\n    \"\"\"\n    Description: Get the result from the query using the QA chain and return the results in a dataframe that is then sent to the frontend.\n\n    Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str)\n\n    Returns: output_df (pd.DataFrame)\n    \"\"\"\n    if type_of_query == \"dataset\":\n        # Fixing the key_name for dataset because of the way the OpenML API returns the data\n        type_of_query = \"data\"\n    elif type_of_query == \"flow\":\n        type_of_query = \"flow\"\n    else:\n        raise ValueError(f\"Unsupported type_of_data: {type_of_query}\")\n\n    # Process the query\n    query = check_query(query)\n    if query == \"\":\n        return pd.DataFrame(), []\n    source_documents = fetch_results(\n        query, qa, config=config, type_of_query=type_of_query\n    )\n    dict_results, ids_order = process_documents(source_documents)\n    output_df = create_output_dataframe(dict_results, type_of_query, ids_order)\n\n    return output_df, ids_order\n</code></pre>"},{"location":"modules/result_gen/#results_gen.long_context_reorder","title":"<code>long_context_reorder(results)</code>","text":"<p>Description: Lost in the middle reorder: the less relevant documents will be at the middle of the list and more relevant elements at beginning / end. See: https://arxiv.org/abs//2307.03172</p> <p>Input: results (list)</p> <p>Returns: reorder results (list)</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def long_context_reorder(results: Sequence[Document]) -&gt; Sequence[Document]:\n    \"\"\"\n    Description: Lost in the middle reorder: the less relevant documents will be at the\n    middle of the list and more relevant elements at beginning / end.\n    See: https://arxiv.org/abs//2307.03172\n\n    Input: results (list)\n\n    Returns: reorder results (list)\n    \"\"\"\n    print(\"[INFO] Reordering results...\")\n    reordering = LongContextReorder()\n    results = reordering.transform_documents(results)\n    print(\"[INFO] Reordering complete.\")\n    return results\n</code></pre>"},{"location":"modules/result_gen/#results_gen.make_clickable","title":"<code>make_clickable(val)</code>","text":"<p>Description: Make the URL clickable in the dataframe.</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def make_clickable(val: str) -&gt; str:\n    \"\"\"\n    Description: Make the URL clickable in the dataframe.\n    \"\"\"\n    return '&lt;a href=\"{}\"&gt;{}&lt;/a&gt;'.format(val, val)\n</code></pre>"},{"location":"modules/result_gen/#results_gen.process_documents","title":"<code>process_documents(source_documents)</code>","text":"<p>Description: Process the source documents and create a dictionary with the key_name as the key and the name and page content as the values.</p> <p>Input: source_documents (list), key_name (str)</p> <p>Returns: dict_results (dict)</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def process_documents(source_documents: Sequence[Document]) -&gt; Tuple[OrderedDict, list]:\n    \"\"\"\n    Description: Process the source documents and create a dictionary with the key_name as the key and the name and page content as the values.\n\n    Input: source_documents (list), key_name (str)\n\n    Returns: dict_results (dict)\n    \"\"\"\n    dict_results = OrderedDict()\n    for result in source_documents:\n        dict_results[result.metadata[\"did\"]] = {\n            \"name\": result.metadata[\"name\"],\n            \"page_content\": result.page_content,\n        }\n    ids = [result.metadata[\"did\"] for result in source_documents]\n    return dict_results, ids\n</code></pre>"}]}