{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAG pipeline for OpenML","text":"<ul> <li>This repository contains the code for the RAG pipeline for OpenML. </li> <li>Project roadmap</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Clone the repository</li> <li>Create a virtual environment and activate it</li> <li>Install the requirements using <code>pip install -r requirements.txt</code></li> <li>Run training.py (for the first time/to update the model). This takes care of basically everything. (Refer to the training section for more details)</li> <li>Install Ollama (https://ollama.com/) for your machine</li> </ul> <ul> <li>For a local setup, you can run ./start_local.sh to start Olama, FastAPI and Streamlit servers. The Streamlit server will be available at http://localhost:8501</li> <li>For docker, refer to Docker</li> <li>For a complete usage example refer to pipeline usage</li> <li>Enjoy :)</li> </ul>"},{"location":"#example-usage","title":"Example usage","text":"<ul> <li>Note that in this picture, I am using a very very tiny model for demonstration purposes. The actual results would be a lot better :)</li> <li></li> </ul>"},{"location":"#where-do-i-go-from-here","title":"Where do I go from here?","text":""},{"location":"#i-am-a-developer-and-i-want-to-contribute-to-the-project","title":"I am a developer and I want to contribute to the project","text":"<ul> <li>Hello! We are glad you are here. To get started, refer to the tutorials in the developer tutorial section.</li> <li>If you have any questions, feel free to ask or post an issue.</li> </ul>"},{"location":"#i-just-want-to-use-the-pipeline","title":"I just want to use the pipeline","text":"<ul> <li>You can use the pipeline by running the Streamlit frontend. Refer to the getting started section above for more details.</li> </ul>"},{"location":"#i-am-on-the-wrong-page","title":"I am on the wrong page","text":""},{"location":"configuration/","title":"Configuration","text":"<ul> <li>The main config file is <code>config.json</code> </li> <li>Possible options are as follows:</li> <li>rqa_prompt_template: The template for the RAG pipeline search prompt. This is used by the model to query the database. </li> <li>llm_prompt_template: The template for the summary generator LLM prompt.</li> <li>num_return_documents: Number of documents to return for a query. Too high a number can lead to Out of Memory errors. (Defaults to 50)</li> <li>embedding_model: The model to use for generating embeddings. This is used to generate embeddings for the documents as a means of comparison using the LLM's embeddings. (Defaults to BAAI/bge-large-en-v1.5)<ul> <li>Other possible tested models<ul> <li>BAAI/bge-base-en-v1.5</li> <li>BAAI/bge-large-en-v1.5</li> <li>WhereIsAI/UAE-Large-V1</li> </ul> </li> </ul> </li> <li>llm_model: The model used for generating the result summary. (Defaults to qwen2:1.5b)</li> <li>data_dir: The directory to store the intermediate data like tables/databases etc. (Defaults to ./data/)</li> <li>persist_dir: The directory to store the cached data. Defaults to ./data/chroma_db/ and stores the embeddings for the documents with a unique hash. (Defaults to ./data/chroma_db/)</li> <li>testing_flag: Enables testing mode by using subsets of the data for quick debugging. This is used to test the pipeline and is not recommended for normal use. (Defaults to False)</li> <li>data_download_n_jobs: Number of jobs to run in parallel for downloading data. (Defaults to 20)</li> <li>training: Whether to train the model or not. (Defaults to False) this is automatically set to True when when running the training.py script. Do NOT set this to True manually.</li> <li>search_type : The type of vector comparison to use. (Defaults to \"similarity\")</li> <li>reraanking: Whether to rerank the results using the FlashRank algorithm. (Defaults to False)</li> <li>long_context_reordering: Whether to reorder the results using the Long Context Reordering algorithm. (Defaults to False)</li> </ul>"},{"location":"docker/","title":"Docker container","text":""},{"location":"docker/#building","title":"Building","text":"<ul> <li>Run <code>docker compose build --progress=plain</code></li> </ul>"},{"location":"docker/#running","title":"Running","text":"<ul> <li>Run <code>./start_docker.sh</code></li> <li>This uses the docker compose file to run the docker process in the background.</li> <li>The required LLM model is also pulled from the docker hub and the container is started.</li> </ul>"},{"location":"docker/#stopping","title":"Stopping","text":"<ul> <li>Run <code>./stop_docker.sh</code></li> </ul>"},{"location":"docker/#potential-errors","title":"Potential Errors","text":"<ul> <li>Permission errors : Run <code>chmod +x *.sh</code></li> <li>If you get a memory error you can run <code>docker system prune</code>. Please be careful with this command as it will remove all stopped containers, all dangling images, and all unused networks. So ensure you have no important data in any of the containers before running this command.</li> <li>On docker desktop for Mac, increase memory limits to as much as your system can handle.</li> </ul>"},{"location":"inference/","title":"Inference","text":"<ul> <li>Just run ./start_local.sh and it will take care of everything.</li> <li>The UI should either pop up or you can navigate to http://localhost:8501/ in your browser.</li> <li>Note that it takes a decent bit of time to load everything. (Approximately 10-15 mins on a decent Macbook Pro, and much slower with Docker)</li> </ul>"},{"location":"inference/#stopping","title":"Stopping","text":"<ul> <li>Run ./stop_local.sh</li> <li>./start_local.sh stores the PIDs of all the processes it starts in files in all the directories it starts them in. stop_local.sh reads these files and kills the processes.</li> </ul>"},{"location":"inference/#errors","title":"Errors","text":"<ul> <li>If you get an error about file permissions, run <code>chmod +x start_local.sh</code> and <code>chmod +x stop_local.sh</code> to make them executable.</li> </ul>"},{"location":"testing/","title":"Testing","text":""},{"location":"testing/#unit-testing","title":"Unit Testing","text":"<ul> <li>Run <code>python -m unittest tests/unit_testing.py</code> to run the unit tests.</li> </ul>"},{"location":"testing/#load-testing","title":"Load Testing","text":"<ul> <li>Load testing can be done using Locust, a load testing tool that allows you to simulate users querying the API and measure the performance of the API under load from numerous users.</li> <li>It is possible to configure the number of users, the hatch rate, and the time to run the test for.</li> </ul>"},{"location":"testing/#running-the-load-test","title":"Running the load test","text":"<ul> <li>Start the FastAPI server using <code>uvicorn main:app</code> (or <code>./start_local.sh</code> )</li> <li>Load testing using Locust (<code>locust -f tests/locust_test.py --host http://127.0.0.1:8000</code> ) using a different terminal</li> </ul>"},{"location":"testing/#all-tests","title":"All tests","text":"<p>               Bases: <code>TestCase</code></p> Source code in <code>tests/unit_testing.py</code> <pre><code>class TestConfig(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.client = chromadb.PersistentClient(path=config[\"persist_dir\"])\n        self.config_keys = [\"rqa_prompt_template\", \"llm_prompt_template\",\n        \"num_return_documents\", \"embedding_model\", \"llm_model\", \"num_documents_for_llm\", \"data_dir\", \"persist_dir\", \"testing_flag\", \"ignore_downloading_data\", \"test_subset_2000\", \"data_download_n_jobs\", \"training\", \"temperature\", \"top_p\", \"search_type\", \"reranking\", \"long_context_reorder\"]\n        self.query_test_dict = {\n            \"dataset\": \"Find me a dataset about flowers that has a high number of instances.\",\n            \"flow\": \"Find me a flow that uses the RandomForestClassifier.\",\n        }\n    def test_check_data_dirs(self):\n        \"\"\"\n        Description: Check if the data directory exists.\n        Returns: None\n        \"\"\"\n        self.assertTrue(os.path.exists(config[\"data_dir\"]))\n        self.assertTrue(os.path.exists(config[\"persist_dir\"]))\n\n    def test_config(self):\n        \"\"\"\n        Description: Check if the config has the required keys.\n        Returns: None\n        \"\"\"\n        for key in self.config_keys:\n            self.assertIn(key, config.keys())\n\n    def test_setup_vector_db_and_qa(self):\n        \"\"\"\n        Description: Check if the setup_vector_db_and_qa function works as expected.\n        Returns: None\n        \"\"\"\n        for type_of_data in [\"dataset\", \"flow\"]:\n            self.qa = setup_vector_db_and_qa(\n                config=config, data_type=type_of_data, client=self.client\n            )\n            self.assertIsNotNone(self.qa)\n            self.result_data_frame = get_result_from_query(\n                query=self.query_test_dict[type_of_data],\n                qa=self.qa,\n                type_of_query=type_of_data,\n                config=config,\n            )\n            self.assertIsNotNone(self.result_data_frame)\n</code></pre>"},{"location":"testing/#unit_testing.TestConfig.test_check_data_dirs","title":"<code>test_check_data_dirs()</code>","text":"<p>Description: Check if the data directory exists. Returns: None</p> Source code in <code>tests/unit_testing.py</code> <pre><code>def test_check_data_dirs(self):\n    \"\"\"\n    Description: Check if the data directory exists.\n    Returns: None\n    \"\"\"\n    self.assertTrue(os.path.exists(config[\"data_dir\"]))\n    self.assertTrue(os.path.exists(config[\"persist_dir\"]))\n</code></pre>"},{"location":"testing/#unit_testing.TestConfig.test_config","title":"<code>test_config()</code>","text":"<p>Description: Check if the config has the required keys. Returns: None</p> Source code in <code>tests/unit_testing.py</code> <pre><code>def test_config(self):\n    \"\"\"\n    Description: Check if the config has the required keys.\n    Returns: None\n    \"\"\"\n    for key in self.config_keys:\n        self.assertIn(key, config.keys())\n</code></pre>"},{"location":"testing/#unit_testing.TestConfig.test_setup_vector_db_and_qa","title":"<code>test_setup_vector_db_and_qa()</code>","text":"<p>Description: Check if the setup_vector_db_and_qa function works as expected. Returns: None</p> Source code in <code>tests/unit_testing.py</code> <pre><code>def test_setup_vector_db_and_qa(self):\n    \"\"\"\n    Description: Check if the setup_vector_db_and_qa function works as expected.\n    Returns: None\n    \"\"\"\n    for type_of_data in [\"dataset\", \"flow\"]:\n        self.qa = setup_vector_db_and_qa(\n            config=config, data_type=type_of_data, client=self.client\n        )\n        self.assertIsNotNone(self.qa)\n        self.result_data_frame = get_result_from_query(\n            query=self.query_test_dict[type_of_data],\n            qa=self.qa,\n            type_of_query=type_of_data,\n            config=config,\n        )\n        self.assertIsNotNone(self.result_data_frame)\n</code></pre>"},{"location":"training/","title":"Training","text":"<ul> <li>While we are not creating a new model, we are using the existing model to create embeddings. The name might be misleading but this was chosen as an attempt to keep the naming consistent with other codebases.</li> <li>(Perhaps we might fine tune the model in the future)</li> <li>The training script is present in <code>training.py</code>. Running this script will take care of everything.</li> </ul>"},{"location":"training/#what-does-the-training-script-do","title":"What does the training script do?","text":"<ul> <li>Load the config file and set the necessary variables</li> <li>If <code>testing_flag</code> is set to True, the script will use a subset of the data for quick debugging</li> <li>testing_flag is set to True</li> <li>persist_dir is set to ./data/chroma_db_testing</li> <li>test_subset_2000 is set to True</li> <li>data_dir is set to ./data/testing_data/</li> <li>If <code>testing_flag</code> is set to False, the script will use the entire dataset</li> <li>For all datasets in the OpenML dataset list:</li> <li>Download the dataset</li> <li>Create the vector dataset with computed embeddings</li> <li>Create a vectordb retriever </li> <li>Run some test queries</li> </ul>"},{"location":"developer%20tutorials/","title":"Developer Tutorials","text":"<ul> <li>Hello there, future OpenML contributor! It is nice meeting you here. This page is a collection of tutorials that will help you get started with contributing to the OpenML RAG pipeline.</li> <li>The tutorials show you how to perform common tasks and should make it a lot easier to get started with contributing to this project.</li> <li>Note that you would have had to setup the project before you begin. If you missed this step, please refer to index</li> </ul>"},{"location":"developer%20tutorials/change%20model/","title":"Change model","text":"<pre><code>from __future__ import annotations\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import load_config_and_device\nfrom modules.llm import setup_vector_db_and_qa\n</code></pre> <pre><code>config = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../backend/data/\"\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = True\n# load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\nprint(config)\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: cpu\n{'rqa_prompt_template': 'This database is a list of metadata. Use the following pieces of context to find the relevant document. Answer only from the context given using the {question} given. If you do not know the answer, say you do not know. {context}', 'llm_prompt_template': 'The following is a set of documents {docs}. Based on these docs, please summarize the content concisely. Also give a list of main concepts found in the documents. Do not add any new information. Helpful Answer: ', 'num_return_documents': 50, 'embedding_model': 'BAAI/bge-large-en-v1.5', 'llm_model': 'qwen2:1.5b', 'num_documents_for_llm': 10, 'data_dir': '../backend/data/', 'persist_dir': '../backend/data/chroma_db/', 'testing_flag': False, 'ignore_downloading_data': False, 'test_subset_2000': False, 'data_download_n_jobs': 20, 'training': True, 'temperature': 0.95, 'top_p': 0.95, 'search_type': 'similarity', 'reranking': False, 'long_context_reorder': False, 'device': 'cpu', 'type_of_data': 'dataset'}\n</code>\n</pre> <pre><code>config[\"embedding_model\"] = \"HuggingFaceH4/capybara\"\n</code></pre> <ul> <li>Pick a model from Ollama - https://ollama.com/library?sort=popular</li> <li>eg : mistral</li> </ul> <pre><code>config[\"llm_model\"] = \"mistral\"\n</code></pre> <pre><code>qa = setup_vector_db_and_qa(\n        config=config, data_type=config[\"type_of_data\"], client=client\n    )\n</code></pre>"},{"location":"developer%20tutorials/change%20model/#tutorial-on-changing-models","title":"Tutorial on changing models","text":"<ul> <li>How would you use a different embedding and llm model?</li> </ul>"},{"location":"developer%20tutorials/change%20model/#initial-config","title":"Initial config","text":""},{"location":"developer%20tutorials/change%20model/#embedding-model","title":"Embedding model","text":"<ul> <li>Pick a model from HF</li> </ul>"},{"location":"developer%20tutorials/change%20model/#llm-model","title":"LLM model","text":""},{"location":"developer%20tutorials/change%20model/#important","title":"IMPORTANT","text":"<ul> <li>Do NOT forget to change the model to the best model in ollama/get_ollama.sh</li> </ul>"},{"location":"developer%20tutorials/create%20vectordb/","title":"Create vectordb","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import get_all_metadata_from_openml, create_metadata_dataframe, load_config_and_device\nfrom modules.llm import load_document_and_create_vector_store, setup_vector_db_and_qa\n</code></pre> <pre><code>config = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../backend/data/\"\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = True\n\n# load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\nprint(config)\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: cpu\n{'rqa_prompt_template': 'This database is a list of metadata. Use the following pieces of context to find the relevant document. Answer only from the context given using the {question} given. If you do not know the answer, say you do not know. {context}', 'llm_prompt_template': 'The following is a set of documents {docs}. Based on these docs, please summarize the content concisely. Also give a list of main concepts found in the documents. Do not add any new information. Helpful Answer: ', 'num_return_documents': 50, 'embedding_model': 'BAAI/bge-large-en-v1.5', 'llm_model': 'qwen2:1.5b', 'num_documents_for_llm': 10, 'data_dir': '../backend/data/', 'persist_dir': '../backend/data/chroma_db/', 'testing_flag': False, 'ignore_downloading_data': False, 'test_subset_2000': False, 'data_download_n_jobs': 20, 'training': True, 'temperature': 0.95, 'top_p': 0.95, 'search_type': 'similarity', 'reranking': False, 'long_context_reorder': False, 'device': 'cpu', 'type_of_data': 'dataset'}\n</code>\n</pre> <pre><code># Download the data if it does not exist\nopenml_data_object, data_id, all_metadata = get_all_metadata_from_openml(\n    config=config\n)\n# Create the combined metadata dataframe\nmetadata_df, all_metadata = create_metadata_dataframe(\n    openml_data_object, data_id, all_metadata, config=config\n)\n# Create the vector store\nvectordb = load_document_and_create_vector_store(\n    metadata_df, config=config, chroma_client=client\n)\n</code></pre> <pre><code>qa = setup_vector_db_and_qa(\n        config=config, data_type=config[\"type_of_data\"], client=client\n    )\n</code></pre>"},{"location":"developer%20tutorials/create%20vectordb/#tutorial-on-creating-a-vector-database-with-openml-objects","title":"Tutorial on creating a vector database with openml objects","text":"<ul> <li>How would you use the API to create a vector database with openml objects (datasets, flows etc)</li> </ul>"},{"location":"developer%20tutorials/create%20vectordb/#manually","title":"Manually","text":""},{"location":"developer%20tutorials/create%20vectordb/#api","title":"API","text":""},{"location":"developer%20tutorials/get%20an%20llm%20summary/","title":"Get an llm summary","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.llm import get_llm_chain, get_llm_result_from_string\nfrom modules.utils import load_config_and_device\n</code></pre> <pre><code># Config and DB\n\n# load the configuration and device\nconfig = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../../backend/data/\"\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: cpu\n</code>\n</pre> <pre><code>config[\"llm_prompt_template\"] = \"The following is a set of documents {docs}. Based on these docs, please summarize the content concisely. Also give a list of main concepts found in the documents. Do not add any new information. Helpful Answer: \"\nconfig[\"llm_model\"] = \"qwen2:1.5b\"\n</code></pre> <pre><code># get the llm chain and set the cache\nllm_chain = get_llm_chain(config=config, local=True)\n# use os path to ensure compatibility with all operating systems\nset_llm_cache(SQLiteCache(database_path=os.path.join(config[\"data_dir\"], \".langchain.db\")))\n</code></pre> <pre><code>get_llm_result_from_string(llm_chain, \"This document is about eating disorders and this one is about eating nice food\")\n</code></pre> <pre>\n<code>'Eating Disorders\\n\\n- Eating disorders refer to psychological and emotional conditions characterized by compulsive behaviors such as overeating or excessive restriction.\\n- These behaviors lead to significant weight loss, malnutrition, and serious health complications.\\n\\nEating Nice Food\\n\\n- This document focuses on the importance of eating good food for maintaining a healthy and balanced diet.\\n- It highlights how selecting nutrient-dense foods can aid in overall physical and mental well-being.'</code>\n</pre>"},{"location":"developer%20tutorials/get%20an%20llm%20summary/#getting-an-llm-summary-using-the-api","title":"Getting an LLM summary using the API","text":"<ul> <li>How would you use the API and an LLM model + prompt to generate a summary of the results obtained from the RAG pipeline?</li> </ul>"},{"location":"developer%20tutorials/get%20an%20llm%20summary/#get-llm-summary-of-a-string","title":"Get LLM summary of a string","text":"<ul> <li>Ensure that Ollama is running before this works <code>bash ollama/.get_ollama.sh</code> (or use the desktop Ollama app for testing)</li> <li>As you can tell, the data needs to be a string. To then get the results from a bunch of langchain documents, you must first concatenate the text you care about into a single string.</li> </ul>"},{"location":"developer%20tutorials/load%20vectordb%20and%20get%20results/","title":"Load vectordb and get results","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import load_config_and_device\nfrom modules.llm import setup_vector_db_and_qa\nfrom modules.results_gen import get_result_from_query\n</code></pre> <pre><code># Config and DB\n\n# load the configuration and device\nconfig = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../../backend/data/\"\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: cpu\n</code>\n</pre> <pre><code># load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\n</code></pre> <pre><code># Setup llm chain, initialize the retriever and llm, and setup Retrieval QA\nqa_dataset = setup_vector_db_and_qa(config=config, data_type=\"dataset\", client=client)\n</code></pre> <pre>\n<code>[INFO] Loading metadata from file.\n[INFO] Loading model...\n</code>\n</pre> <pre>\n<code>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n</code>\n</pre> <pre>\n<code>[INFO] Model loaded.\n</code>\n</pre> <pre><code>query = \"give me datasets about mushrooms\"\n</code></pre> <pre><code>res = qa_dataset.invoke(input = query, top_k=5)[:10]\nres\n</code></pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>[Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 25  \\n**\\\\# Images**: 15122  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44335, 'name': 'Meta_Album_FNG_Extended'}),\n Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 25  \\n**\\\\# Images**: 1000  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44302, 'name': 'Meta_Album_FNG_Mini'}),\n Document(page_content=\"### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\", metadata={'did': 24, 'name': 'mushroom'}),\n Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n Document(page_content='**Source**: Danish Fungi Dataset  \\n**Source URL**: https://sites.google.com/view/danish-fungi-dataset  \\n  \\n**Original Author**: Lukas Picek, Milan Sulc, Jiri Matas, Jacob Heilmann-Clausen, Thomas S. Jeppesen, Thomas Laessoe, Tobias Froslev  \\n**Original contact**: lukaspicek@gmail.com  \\n\\n**Meta Album author**: Felix Herron  \\n**Created Date**: 01 March 2022  \\n**Contact Name**: Ihsan Ullah  \\n**Contact Email**: meta-album@chalearn.org  \\n**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)  \\n\\n\\n\\n### **Cite this dataset**\\n```\\n@article{picek2021danish,\\n    title={Danish Fungi 2020 - Not Just Another Image Recognition Dataset},\\n    author={Lukas Picek and Milan Sulc and Jiri Matas and Jacob Heilmann-Clausen and Thomas S. Jeppesen and Thomas Laessoe and Tobias Froslev},\\n    year={2021},\\n    eprint={2103.10107},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.CV}\\n}\\n```', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n Document(page_content='did - 24, name - mushroom, version - 1, uploader - 1, status - active, format - ARFF, MajorityClassSize - 4208.0, MaxNominalAttDistinctValues - 12.0, MinorityClassSize - 3916.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 8124.0, NumberOfInstancesWithMissingValues - 2480.0, NumberOfMissingValues - 2480.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - **Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \\n**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n\\n### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.', metadata={'did': 24, 'name': 'mushroom'}),\n Document(page_content='Meta-Album Fungi dataset is created by sampling the Danish Fungi 2020 dataset(https://arxiv.org/abs/2103.10107), itself a sampling of the Atlas of Danish Fungi repository. The images and labels which enter this database are sourced by a group consisting of 3 300 citizen botanists, then verified by their peers using a ranking of each person reliability, then finally verified by experts working at the Atlas. Of the 128 classes in the original Danish Fungi 2020 dataset, FNG retains the 25 most populous classes, belonging to six genera, for a total of 15 122 images total, with min 372, and max 1 221 images per class. Each image contains a colored 128x128 image of a fungus or a piece of a fungus from the corresponding class. Because the initial data were of widely varying sizes, we needed to crop a significant portion of the images, which we implemented by taking the largest possible square with center at the middle of the initial image. We then scaled each squared image to the 128x128', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n Document(page_content='did - 44272, name - Meta_Album_FNG_Micro, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 40.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 40.0, NumberOfClasses - 20.0, NumberOfFeatures - 3.0, NumberOfInstances - 800.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Fungi Dataset (Micro)**\\n***', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n Document(page_content='did - 44335, name - Meta_Album_FNG_Extended, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 1221.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 372.0, NumberOfClasses - 25.0, NumberOfFeatures - 3.0, NumberOfInstances - 15122.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Fungi Dataset (Extended)**\\n***', metadata={'did': 44335, 'name': 'Meta_Album_FNG_Extended'}),\n Document(page_content='did - 43922, name - mushroom, version - 3, uploader - 30861, status - active, format - ARFF, MajorityClassSize - 4208.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 3916.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 8124.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - Mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf, qualities - AutoCorrelation : 0.726332635725717, Dimensionality : 0.002831117676021664, MajorityClassPercentage : 51.7971442639094, MajorityClassSize : 4208.0, MinorityClassPercentage : 48.20285573609059, MinorityClassSize : 3916.0, NumberOfBinaryFeatures : 6.0, NumberOfClasses : 2.0, NumberOfFeatures : 23.0, NumberOfInstances : 8124.0, NumberOfInstancesWithMissingValues : 0.0, NumberOfMissingValues : 0.0, NumberOfNumericFeatures : 0.0,', metadata={'did': 43922, 'name': 'mushroom'})]</code>\n</pre> <pre><code>res[0].metadata\n</code></pre> <pre>\n<code>{'did': 44335, 'name': 'Meta_Album_FNG_Extended'}</code>\n</pre> <pre><code>print(res[0].page_content)\n</code></pre> <pre>\n<code>### **Dataset Details**\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\n\n**Meta Album ID**: PLT.FNG  \n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \n**Domain ID**: PLT  \n**Domain Name**: Plants  \n**Dataset ID**: FNG  \n**Dataset Name**: Fungi  \n**Short Description**: Fungi dataset from Denmark  \n**\\# Classes**: 25  \n**\\# Images**: 15122  \n**Keywords**: fungi, ecology, plants  \n**Data Format**: images  \n**Image size**: 128x128  \n\n**License (original data release)**: BSD-3-Clause License  \n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\n\n**License (Meta-Album data release)**: BSD-3-Clause License  \n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)\n</code>\n</pre> <pre><code># Fetch the result data frame based on the query\nresult_data_frame, result_documents = get_result_from_query(\n    query=query, qa=qa_dataset, type_of_query=\"dataset\", config=config\n)\n</code></pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre><code>result_data_frame.head()\n</code></pre> id name command OpenML URL Description 0 44335 Meta_Album_FNG_Extended dataset = openml.datasets.get_dataset(44335) &lt;a href=\"https://www.openml.org/search?type=da... did - 44335, name - Meta_Album_FNG_Extended, v... 1 44302 Meta_Album_FNG_Mini dataset = openml.datasets.get_dataset(44302) &lt;a href=\"https://www.openml.org/search?type=da... ### **Dataset Details**\\n![](https://meta-albu... 2 24 mushroom dataset = openml.datasets.get_dataset(24) &lt;a href=\"https://www.openml.org/search?type=da... did - 24, name - mushroom, version - 1, upload... 3 44272 Meta_Album_FNG_Micro dataset = openml.datasets.get_dataset(44272) &lt;a href=\"https://www.openml.org/search?type=da... did - 44272, name - Meta_Album_FNG_Micro, vers... 10 1558 bank-marketing dataset = openml.datasets.get_dataset(1558) &lt;a href=\"https://www.openml.org/search?type=da... * Dataset:"},{"location":"developer%20tutorials/load%20vectordb%20and%20get%20results/#load-the-chroma-db-and-get-retrieval-results-for-a-given-query","title":"Load the Chroma Db and get retrieval results for a given query","text":"<ul> <li>How would you load the Chroma Db and get retrieval results for a given query?</li> </ul>"},{"location":"developer%20tutorials/load%20vectordb%20and%20get%20results/#just-get-documents","title":"Just get documents","text":""},{"location":"developer%20tutorials/load%20vectordb%20and%20get%20results/#process-the-results-and-return-a-dataframe-instead","title":"Process the results and return a dataframe instead","text":""},{"location":"developer%20tutorials/run%20multiple%20queries%20and%20aggregate/","title":"Run multiple queries and aggregate","text":"<pre><code>from __future__ import annotations\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import load_config_and_device\nfrom modules.llm import setup_vector_db_and_qa\nfrom modules.results_gen import aggregate_multiple_queries_and_count\n</code></pre> <pre><code># Config and DB\n\n# load the configuration and device\nconfig = load_config_and_device(\"../../backend/config.json\")\nconfig[\"persist_dir\"] = \"../../backend/data/chroma_db/\"\nconfig[\"data_dir\"] = \"../../backend/data/\"\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: cpu\n</code>\n</pre> <pre><code># load the persistent database using ChromaDB\nclient = chromadb.PersistentClient(path=config[\"persist_dir\"])\n</code></pre> <pre><code># Setup llm chain, initialize the retriever and llm, and setup Retrieval QA\nqa_dataset = setup_vector_db_and_qa(config=config, data_type=\"dataset\", client=client)\n</code></pre> <pre>\n<code>[INFO] Loading metadata from file.\n[INFO] Loading model...\n</code>\n</pre> <pre>\n<code>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n</code>\n</pre> <pre>\n<code>[INFO] Model loaded.\n</code>\n</pre> <pre><code>queries = [\"Find datasets related to COVID-19\", \"Find datasets related to COVID-19 and India\", \"COVID-19 dataset\", \"COVID-19 dataset India\", \"Mexico historical covid\"]\ncombined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\", count = True)\n</code></pre> <pre>\n<code>  0%|          | 0/5 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code> 20%|\u2588\u2588        | 1/5 [00:02&lt;00:08,  2.14s/it]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:02&lt;00:03,  1.17s/it]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:03&lt;00:01,  1.14it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:03&lt;00:00,  1.32it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:03&lt;00:00,  1.26it/s]\n</code>\n</pre> <pre><code>combined_df.head()\n</code></pre> id name query 36 43495 COVID-19-Mexico-Clean--Order-by-States 4 52 43844 Coronavirus-Worldwide-Dataset 4 26 43349 COVID-19-World-Vaccination-Progress 4 27 43365 Covid-19-Case-Surveillance-Public-Use-Dataset 4 28 43367 COVID-19-Indonesia-Dataset 4 <pre><code>queries = [\"Find datasets related to COVID-19\", \"Find datasets related to COVID-19 and India\", \"COVID-19 dataset\", \"COVID-19 dataset India\", \"Mexico historical covid\"]\ncombined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\", count = False)\n</code></pre> <pre>\n<code>  0%|          | 0/5 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code> 20%|\u2588\u2588        | 1/5 [00:00&lt;00:02,  1.43it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:01&lt;00:01,  1.99it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:01&lt;00:01,  1.50it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:02&lt;00:00,  1.85it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:02&lt;00:00,  2.01it/s]\n</code>\n</pre> <pre><code>combined_df.head()\n</code></pre> id name query 0 43410 Coronavirus-Disease-(COVID-19) Find datasets related to COVID-19 1 43412 COVID-19-Visualisation-and-Epidemic-Analysis-Data Find datasets related to COVID-19 2 43365 Covid-19-Case-Surveillance-Public-Use-Dataset Find datasets related to COVID-19 3 43367 COVID-19-Indonesia-Dataset Find datasets related to COVID-19 4 43684 COVID-19-Stats-and-Mobility-Trends Find datasets related to COVID-19 <pre>\nThe Kernel crashed while executing code in the current cell or a previous cell. \n\nPlease review the code in the cell(s) to identify a possible cause of the failure. \n\nClick &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;here&lt;/a&gt; for more info. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.</pre>"},{"location":"developer%20tutorials/run%20multiple%20queries%20and%20aggregate/#aggregate-results","title":"Aggregate results","text":""},{"location":"developer%20tutorials/run%20multiple%20queries%20and%20aggregate/#just-collate","title":"Just collate","text":""},{"location":"developer%20tutorials/train%20and%20evaluate%20models/","title":"Train and evaluate models","text":"<pre><code>from __future__ import annotations\nfrom langchain_community.cache import SQLiteCache\nimport os\nimport sys\nimport chromadb\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport pandas as pd\n# change the path to the backend directory\nsys.path.append(os.path.join(os.path.dirname(\".\"), '../../backend/'))\n</code></pre> <pre><code>from modules.utils import load_config_and_device\nfrom modules.llm import setup_vector_db_and_qa\nfrom modules.results_gen import aggregate_multiple_queries_and_count\n</code></pre> <pre><code>new_path = Path(\"../../backend/\")\n\nconfig = load_config_and_device(str(new_path / \"config.json\"), training = True)\n\nconfig[\"type_of_data\"] = \"dataset\"\nconfig[\"training\"] = True\n</code></pre> <pre>\n<code>[INFO] Finding device.\n[INFO] Device found: mps\n</code>\n</pre> <pre><code>config[\"device\"] = \"cpu\" # for testing\n</code></pre> <pre><code>list_of_embedding_models = [\"BAAI/bge-small-en-v1.5\"]\nlist_of_llm_models = [\"qwen2:1.5b\", \"phi3\"]\n</code></pre> <pre><code>def process_embedding_model_name_hf(name : str) -&amp;gt; str:\n    \"\"\"\n    Description: This function processes the name of the embedding model from Hugging Face to use as experiment name.\n\n    Input: name (str) - name of the embedding model from Hugging Face.\n\n    Returns: name (str) - processed name of the embedding model.\n    \"\"\"\n    return name.replace(\"/\", \"_\")\n\ndef process_llm_model_name_ollama(name : str) -&amp;gt; str:\n    \"\"\"\n    Description: This function processes the name of the llm model from Ollama to use as experiment name.\n\n    Input: name (str) - name of the llm model from Ollama.\n\n    Returns: name (str) - processed name of the llm model.\n    \"\"\"\n    return name.replace(\":\", \"_\")\n</code></pre> <pre><code>queries = [\"Find datasets related to COVID-19\", \"Find datasets related to COVID-19 and India\", \"COVID-19 dataset\", \"COVID-19 dataset India\", \"Mexico historical covid\"]\n</code></pre> <pre><code># download the ollama llm models\n\n# os.system(\"curl -fsSL https://ollama.com/install.sh | sh\")\nos.system(\"ollama serve&amp;amp;\")\nprint(\"Waiting for Ollama server to be active...\")  \nwhile os.system(\"ollama list | grep 'NAME'\") == \"\":\n    pass\n\nfor llm_model in list_of_llm_models:\n    os.system(f\"ollama pull {llm_model}\")\n</code></pre> <pre>\n<code>Waiting for Ollama server to be active...\nNAME            ID              SIZE    MODIFIED       \n</code>\n</pre> <pre>\n<code>Error: listen tcp 127.0.0.1:11434: bind: address already in use\npulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \u283c pulling manifest \u2834 pulling manifest \npulling 405b56374e02... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 934 MB                         \npulling 62fbfd9ed093... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  182 B                         \npulling c156170b718e... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  11 KB                         \npulling f02dd72bb242... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   59 B                         \npulling c9f5e9ffbc5f... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  485 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \nsuccess \npulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \u283c pulling manifest \npulling b26e6713dc74... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.4 GB                         \npulling fa8235e5b48f... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.1 KB                         \npulling 542b217f179c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  148 B                         \npulling 8dde1baf1db0... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   78 B                         \npulling f91db7a2deb9... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  485 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \nsuccess \n</code>\n</pre> <pre><code># use a tiny subset of the data for testing\nconfig[\"test_subset_2000\"] = True\n</code></pre> <pre><code>for embedding_model in tqdm(list_of_embedding_models, desc=\"Embedding Models\", total=len(list_of_embedding_models)):\n    for llm_model in tqdm(list_of_llm_models, desc=\"LLM Models\", total=len(list_of_llm_models)):\n        # update the config with the new embedding and llm models\n        config[\"embedding_model\"] = embedding_model\n        config[\"llm_model\"] = llm_model\n\n        # create a new experiment directory using a combination of the embedding model and llm model names\n        experiment_name = f\"{process_embedding_model_name_hf(embedding_model)}_{process_llm_model_name_ollama(llm_model)}\"\n        experiment_path = new_path/Path(f\"data/experiments/{experiment_name}\")\n\n        # create the experiment directory if it does not exist\n        os.makedirs(experiment_path, exist_ok=True)\n\n        # update the config with the new experiment directories\n        config[\"data_dir\"] = str(experiment_path)\n        config[\"persist_dir\"] = str(experiment_path / \"chroma_db\")\n\n        # save training details and config in a dataframe\n        config_df = pd.DataFrame.from_dict(config, orient='index').reset_index()\n        config_df.columns = ['Hyperparameter', 'Value']\n        config_df.to_csv(experiment_path / \"config.csv\", index=False)\n\n        # load the persistent database using ChromaDB\n        client = chromadb.PersistentClient(path=config[\"persist_dir\"])\n\n        # Run \"training\"\n        qa_dataset = setup_vector_db_and_qa(\n            config=config, data_type=config[\"type_of_data\"], client=client\n        )\n\n        # Run an evaluation by aggregating multiple queries and counting the results\n        # TODO : Replace this evaluation with a more meaningful one\n        combined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\", count = True)\n\n        combined_df.to_csv(experiment_path / \"results.csv\")\n</code></pre> <pre>\n<code>Embedding Models:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>[INFO] Training is set to True.\n[INFO] Subsetting the data to 100 rows.\n[INFO] Initializing cache.\n[INFO] Getting dataset metadata from OpenML.\n</code>\n</pre> <pre>\n<code>QUEUEING TASKS | :   0%|          | 0/100 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>PROCESSING TASKS | :   0%|          | 0/100 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>COLLECTING RESULTS | :   0%|          | 0/100 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>[INFO] Saving metadata to file.\n[INFO] Loading model...\n</code>\n</pre> <pre>\n<code>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n</code>\n</pre> <pre>\n<code>[INFO] Model loaded.\n[INFO] Generating unique documents. Total documents: 992\nNumber of unique documents: 967 vs Total documents: 992\n</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/16 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/15 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [03:06&lt;00:00, 93.04s/it]\n\n</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/1 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00,  8.16it/s]\n</code>\n</pre> <pre>\n<code>[INFO] Training is set to True.\n[INFO] Subsetting the data to 100 rows.\n[INFO] Initializing cache.\n[INFO] Getting dataset metadata from OpenML.\n</code>\n</pre> <pre>\n<code>QUEUEING TASKS | :   0%|          | 0/100 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>PROCESSING TASKS | :   0%|          | 0/100 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>COLLECTING RESULTS | :   0%|          | 0/100 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>[INFO] Saving metadata to file.\n[INFO] Loading model...\n</code>\n</pre> <pre>\n<code>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n</code>\n</pre> <pre>\n<code>[INFO] Model loaded.\n[INFO] Generating unique documents. Total documents: 992\nNumber of unique documents: 967 vs Total documents: 992\n</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre>\n<code>Batches:   0%|          | 0/16 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>  0%|          | 0/2 [00:43&lt;?, ?it/s]\nLLM Models:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [04:37&lt;04:37, 277.59s/it]\nEmbedding Models:   0%|          | 0/1 [04:37&lt;?, ?it/s]\n</code>\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[11], line 27\n     24 client = chromadb.PersistentClient(path=config[\"persist_dir\"])\n     26 # Run \"training\"\n---&gt; 27 qa_dataset = setup_vector_db_and_qa(\n     28     config=config, data_type=config[\"type_of_data\"], client=client\n     29 )\n     31 # Run an evaluation by aggregating multiple queries and counting the results\n     32 # TODO : Replace this evaluation with a more meaningful one\n     33 combined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\", count = True)\n\nFile ~/Documents/CODE/Github/ai_search/docs/developer tutorials/../../backend/modules/llm.py:268, in setup_vector_db_and_qa(config, data_type, client)\n    264 metadata_df, all_metadata = create_metadata_dataframe(\n    265     openml_data_object, data_id, all_metadata, config=config\n    266 )\n    267 # Create the vector store\n--&gt; 268 vectordb = load_document_and_create_vector_store(\n    269     metadata_df, config=config, chroma_client=client\n    270 )\n    271 # Initialize the LLM chain and setup Retrieval QA\n    272 qa = initialize_llm_chain(vectordb=vectordb, config=config)\n\nFile ~/Documents/CODE/Github/ai_search/docs/developer tutorials/../../backend/modules/llm.py:107, in load_document_and_create_vector_store(metadata_df, chroma_client, config)\n    104 if not config[\"training\"]:\n    105     return load_vector_store(chroma_client, config, embeddings, collection_name)\n--&gt; 107 return create_vector_store(\n    108     metadata_df, chroma_client, config, embeddings, collection_name\n    109 )\n\nFile ~/Documents/CODE/Github/ai_search/docs/developer tutorials/../../backend/modules/llm.py:222, in create_vector_store(metadata_df, chroma_client, config, embeddings, collection_name)\n    219     return db\n    220 else:\n    221     # db.add_documents(unique_docs, ids=unique_ids)\n--&gt; 222     add_documents_to_db(db, unique_docs, unique_ids)\n    224 return db\n\nFile ~/Documents/CODE/Github/ai_search/docs/developer tutorials/../../backend/modules/llm.py:180, in add_documents_to_db(db, unique_docs, unique_ids)\n    178 else:\n    179     for i in tqdm(range(0, len(unique_docs), bs)):\n--&gt; 180         db.add_documents(unique_docs[i : i + bs], ids=unique_ids[i : i + bs])\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/vectorstores.py:147, in VectorStore.add_documents(self, documents, **kwargs)\n    145 texts = [doc.page_content for doc in documents]\n    146 metadatas = [doc.metadata for doc in documents]\n--&gt; 147 return self.add_texts(texts, metadatas, **kwargs)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:276, in Chroma.add_texts(self, texts, metadatas, ids, **kwargs)\n    274 texts = list(texts)\n    275 if self._embedding_function is not None:\n--&gt; 276     embeddings = self._embedding_function.embed_documents(texts)\n    277 if metadatas:\n    278     # fill metadatas with empty dicts if somebody\n    279     # did not specify metadata for all texts\n    280     length_diff = len(texts) - len(metadatas)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/embeddings/huggingface.py:98, in HuggingFaceEmbeddings.embed_documents(self, texts)\n     96     sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n     97 else:\n---&gt; 98     embeddings = self.client.encode(\n     99         texts, show_progress_bar=self.show_progress, **self.encode_kwargs\n    100     )\n    102 return embeddings.tolist()\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:371, in SentenceTransformer.encode(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\n    368 features.update(extra_features)\n    370 with torch.no_grad():\n--&gt; 371     out_features = self.forward(features)\n    372     out_features[\"sentence_embedding\"] = truncate_embeddings(\n    373         out_features[\"sentence_embedding\"], self.truncate_dim\n    374     )\n    376     if output_value == \"token_embeddings\":\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/container.py:217, in Sequential.forward(self, input)\n    215 def forward(self, input):\n    216     for module in self:\n--&gt; 217         input = module(input)\n    218     return input\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-&gt; 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:98, in Transformer.forward(self, features)\n     95 if \"token_type_ids\" in features:\n     96     trans_features[\"token_type_ids\"] = features[\"token_type_ids\"]\n---&gt; 98 output_states = self.auto_model(**trans_features, return_dict=False)\n     99 output_tokens = output_states[0]\n    101 features.update({\"token_embeddings\": output_tokens, \"attention_mask\": features[\"attention_mask\"]})\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-&gt; 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1137, in BertModel.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\n   1130 # Prepare head mask if needed\n   1131 # 1.0 in head_mask indicate we keep the head\n   1132 # attention_probs has shape bsz x n_heads x N x N\n   1133 # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n   1134 # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n   1135 head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-&gt; 1137 encoder_outputs = self.encoder(\n   1138     embedding_output,\n   1139     attention_mask=extended_attention_mask,\n   1140     head_mask=head_mask,\n   1141     encoder_hidden_states=encoder_hidden_states,\n   1142     encoder_attention_mask=encoder_extended_attention_mask,\n   1143     past_key_values=past_key_values,\n   1144     use_cache=use_cache,\n   1145     output_attentions=output_attentions,\n   1146     output_hidden_states=output_hidden_states,\n   1147     return_dict=return_dict,\n   1148 )\n   1149 sequence_output = encoder_outputs[0]\n   1150 pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-&gt; 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:690, in BertEncoder.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\n    679     layer_outputs = self._gradient_checkpointing_func(\n    680         layer_module.__call__,\n    681         hidden_states,\n   (...)\n    687         output_attentions,\n    688     )\n    689 else:\n--&gt; 690     layer_outputs = layer_module(\n    691         hidden_states,\n    692         attention_mask,\n    693         layer_head_mask,\n    694         encoder_hidden_states,\n    695         encoder_attention_mask,\n    696         past_key_value,\n    697         output_attentions,\n    698     )\n    700 hidden_states = layer_outputs[0]\n    701 if use_cache:\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-&gt; 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:622, in BertLayer.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\n    619     cross_attn_present_key_value = cross_attention_outputs[-1]\n    620     present_key_value = present_key_value + cross_attn_present_key_value\n--&gt; 622 layer_output = apply_chunking_to_forward(\n    623     self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n    624 )\n    625 outputs = (layer_output,) + outputs\n    627 # if decoder, return the attn key/values as the last output\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/pytorch_utils.py:238, in apply_chunking_to_forward(forward_fn, chunk_size, chunk_dim, *input_tensors)\n    235     # concatenate output at same dimension\n    236     return torch.cat(output_chunks, dim=chunk_dim)\n--&gt; 238 return forward_fn(*input_tensors)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:635, in BertLayer.feed_forward_chunk(self, attention_output)\n    633 def feed_forward_chunk(self, attention_output):\n    634     intermediate_output = self.intermediate(attention_output)\n--&gt; 635     layer_output = self.output(intermediate_output, attention_output)\n    636     return layer_output\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-&gt; 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:547, in BertOutput.forward(self, hidden_states, input_tensor)\n    546 def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n--&gt; 547     hidden_states = self.dense(hidden_states)\n    548     hidden_states = self.dropout(hidden_states)\n    549     hidden_states = self.LayerNorm(hidden_states + input_tensor)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-&gt; 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/linear.py:116, in Linear.forward(self, input)\n    115 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 116     return F.linear(input, self.weight, self.bias)\n\nKeyboardInterrupt: </pre>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#tutorial-on-using-multiple-models-for-evaluation","title":"Tutorial on using multiple models for evaluation","text":"<ul> <li>This tutorial is an example of how to test multiple models on the openml data to see which one performs the best.</li> <li>The evaluation is still a bit basic, but it is a good starting point for future research.</li> </ul>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#setting-the-config","title":"Setting the config","text":""},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#defining-the-models-used","title":"Defining the models used","text":"<ul> <li>Embedding models are any from Huggingface hub</li> <li>LLM models are any from Ollama library</li> </ul>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#defining-the-evaluation-queries","title":"Defining the evaluation queries","text":"<ul> <li>replace this with a proper dataframe for a more comprehensive evaluation</li> </ul>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#downloading-the-models","title":"Downloading the models","text":"<ul> <li>PLEASE MAKE SURE YOU HAVE DOWNLOADED OLLAMA (<code>curl -fsSL https://ollama.com/install.sh | sh</code>)</li> </ul>"},{"location":"developer%20tutorials/train%20and%20evaluate%20models/#running-the-steps","title":"Running the steps","text":"<ul> <li>Create an experiment directory</li> <li>Save a config file with the models and the queries in the experiment directory</li> <li>Download openml data for each dataset and format into a string</li> <li>Create vectorb and embed the data</li> <li>Get the predictions for each model for a list of queries and evaluate the performance</li> <li>(note) At the moment, this runs for a very small subset of the entire data. To disable this behavior and run on the entire data, set <code>config[\"test_subset_2000\"] = False</code></li> </ul>"},{"location":"modules/general_utils/","title":"General utils","text":""},{"location":"modules/general_utils/#general_utils.find_device","title":"<code>find_device(training=False)</code>","text":"<p>Description: Find the device to use for the pipeline. If cuda is available, use it. If not, check if MPS is available and use it. If not, use CPU.</p> <p>Input: training (bool) : Whether the pipeline is being used for training or not.</p> <p>Returns: device (str) : The device to use for the pipeline.</p> Source code in <code>backend/modules/general_utils.py</code> <pre><code>def find_device(training: bool = False ) -&gt; str:\n    \"\"\"\n    Description: Find the device to use for the pipeline. If cuda is available, use it. If not, check if MPS is available and use it. If not, use CPU.\n\n    Input: training (bool) : Whether the pipeline is being used for training or not.\n\n    Returns: device (str) : The device to use for the pipeline.\n    \"\"\"\n    print(\"[INFO] Finding device.\")\n    if torch.cuda.is_available():\n        return \"cuda\"\n    elif torch.backends.mps.is_available():\n        if training == False:\n            # loading metadata on mps for inference is quite slow. So disabling for now.\n            return \"cpu\"\n        return \"mps\"\n    else:\n        return \"cpu\"\n</code></pre>"},{"location":"modules/general_utils/#general_utils.load_config_and_device","title":"<code>load_config_and_device(config_file, training=False)</code>","text":"<p>Description: Load the config file and find the device to use for the pipeline.</p> <p>Input: config_file (str) : The path to the config file. training (bool) : Whether the pipeline is being used for training or not.</p> <p>Returns: config (dict) : The config dictionary + device (str) : The device to use for the pipeline.</p> Source code in <code>backend/modules/general_utils.py</code> <pre><code>def load_config_and_device(config_file: str, training: bool = False) -&gt; dict:\n    \"\"\"\n    Description: Load the config file and find the device to use for the pipeline.\n\n    Input: config_file (str) : The path to the config file.\n    training (bool) : Whether the pipeline is being used for training or not.\n\n    Returns: config (dict) : The config dictionary + device (str) : The device to use for the pipeline.\n    \"\"\"\n    # Check if the config file exists and load it\n    if not os.path.exists(config_file):\n        raise Exception(\"Config file does not exist.\")\n    with open(config_file, \"r\") as f:\n        config = json.load(f)\n\n    # Find device and set it in the config between cpu and cuda and mps if available\n    config[\"device\"] = find_device(training)\n    print(f\"[INFO] Device found: {config['device']}\")\n    return config\n</code></pre>"},{"location":"modules/llm_module/","title":"Llm module","text":""},{"location":"modules/llm_module/#llm.add_documents_to_db","title":"<code>add_documents_to_db(db, unique_docs, unique_ids)</code>","text":"<p>Description: Add documents to the vector store in batches of 200.</p> <p>Input: db (Chroma), unique_docs (list), unique_ids (list)</p> <p>Returns: None</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def add_documents_to_db(db, unique_docs, unique_ids):\n    \"\"\"\n    Description: Add documents to the vector store in batches of 200.\n\n    Input: db (Chroma), unique_docs (list), unique_ids (list)\n\n    Returns: None\n    \"\"\"\n    bs = 512\n    if len(unique_docs) &lt; bs:\n        db.add_documents(unique_docs, ids=unique_ids)\n    else:\n        for i in tqdm(range(0, len(unique_docs), bs)):\n            db.add_documents(unique_docs[i : i + bs], ids=unique_ids[i : i + bs])\n</code></pre>"},{"location":"modules/llm_module/#llm.create_vector_store","title":"<code>create_vector_store(metadata_df, chroma_client, config, embeddings, collection_name)</code>","text":"<p>Description: Create the vector store using Chroma db. The documents are loaded and processed, unique documents are generated, and the documents are added to the vector store.</p> <p>Input: metadata_df (pd.DataFrame), chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)</p> <p>Returns: db (Chroma)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def create_vector_store(\n    metadata_df: pd.DataFrame, chroma_client:ClientAPI, config: dict, embeddings: HuggingFaceEmbeddings, collection_name: str \n) -&gt; Chroma:\n    \"\"\"\n    Description: Create the vector store using Chroma db. The documents are loaded and processed, unique documents are generated, and the documents are added to the vector store.\n\n    Input: metadata_df (pd.DataFrame), chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)\n\n    Returns: db (Chroma)\n    \"\"\"\n\n    db = Chroma(\n        client=chroma_client,\n        embedding_function=embeddings,\n        persist_directory=config[\"persist_dir\"],\n        collection_name=collection_name,\n    )\n\n    documents = load_and_process_data(\n        metadata_df, page_content_column=\"Combined_information\"\n    )\n    if config[\"testing_flag\"]:\n        # subset the data for testing\n        if config[\"test_subset_2000\"] == True:\n            print(\"[INFO] Subsetting the data to 100 rows.\")\n            documents = documents[:100]\n    unique_docs, unique_ids = generate_unique_documents(documents, db)\n\n    print(\n        f\"Number of unique documents: {len(unique_docs)} vs Total documents: {len(documents)}\"\n    )\n    if len(unique_docs) == 0:\n        print(\"No new documents to add.\")\n        return db\n    else:\n        # db.add_documents(unique_docs, ids=unique_ids)\n        add_documents_to_db(db, unique_docs, unique_ids)\n\n    return db\n</code></pre>"},{"location":"modules/llm_module/#llm.generate_unique_documents","title":"<code>generate_unique_documents(documents, db)</code>","text":"Generate unique documents by removing duplicates. This is done by generating unique IDs for the documents and keeping only one of the duplicate IDs. <p>Source: https://stackoverflow.com/questions/76265631/chromadb-add-single-document-only-if-it-doesnt-exist</p> <p>Input: documents (list)</p> <p>Returns: unique_docs (list), unique_ids (list)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def generate_unique_documents(documents: list, db: Chroma) -&gt; tuple:\n    \"\"\"\n    Description: Generate unique documents by removing duplicates. This is done by generating unique IDs for the documents and keeping only one of the duplicate IDs.\n        Source: https://stackoverflow.com/questions/76265631/chromadb-add-single-document-only-if-it-doesnt-exist\n\n    Input: documents (list)\n\n    Returns: unique_docs (list), unique_ids (list)\n    \"\"\"\n\n    # Remove duplicates based on ID (from database)\n    new_document_ids = set([str(x.metadata[\"did\"]) for x in documents])\n    print(f\"[INFO] Generating unique documents. Total documents: {len(documents)}\")\n    try:\n        old_dids = set([str(x[\"did\"]) for x in db.get()[\"metadatas\"]])\n    except KeyError:\n        old_dids = set([str(x[\"id\"]) for x in db.get()[\"metadatas\"]])\n\n    new_dids = new_document_ids - old_dids\n    documents = [x for x in documents if str(x.metadata[\"did\"]) in new_dids]\n    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS,doc.page_content)) for doc in documents]\n\n    # Remove duplicates based on document content (from new documents)\n    unique_ids = list(set(ids))\n    seen_ids = set()\n    unique_docs = [\n            doc\n            for doc, id in zip(documents, ids)\n            if id not in seen_ids and (seen_ids.add(id) or True)\n        ]\n\n    return unique_docs, unique_ids\n</code></pre>"},{"location":"modules/llm_module/#llm.get_collection_name","title":"<code>get_collection_name(config)</code>","text":"<p>Description: Get the collection name based on the type of data provided in the config.</p> <p>Input: config (dict)</p> <p>Returns: str</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def get_collection_name(config: dict) -&gt; str:\n    \"\"\"\n    Description: Get the collection name based on the type of data provided in the config.\n\n    Input: config (dict)\n\n    Returns: str\n    \"\"\"\n    return {\"dataset\": \"datasets\", \"flow\": \"flows\"}.get(\n        config[\"type_of_data\"], \"default\"\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.get_llm_chain","title":"<code>get_llm_chain(config, local=False)</code>","text":"<p>Description: Get the LLM chain with the specified model and prompt template.</p> <p>Input: config (dict)</p> <p>Returns: LLMChain</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def get_llm_chain(config: dict, local:bool =False) -&gt; LLMChain|bool:\n    \"\"\"\n    Description: Get the LLM chain with the specified model and prompt template.\n\n    Input: config (dict)\n\n    Returns: LLMChain\n    \"\"\"\n    base_url = \"http://127.0.0.1:11434\" if local else \"http://ollama:11434\"\n    llm = Ollama(\n        model = config[\"llm_model\"] , base_url = base_url\n    )  \n    # llm = Ollama(\n        # model = config[\"llm_model\"]\n    # )\n    # print(llm)\n    map_template = config[\"llm_prompt_template\"]\n    map_prompt = PromptTemplate.from_template(map_template)\n    # return LLMChain(llm=llm, prompt=map_prompt)\n    return map_prompt | llm | StrOutputParser()\n</code></pre>"},{"location":"modules/llm_module/#llm.initialize_llm_chain","title":"<code>initialize_llm_chain(vectordb, config)</code>","text":"<p>Description: Initialize the LLM chain and setup Retrieval QA with the specified configuration.</p> <p>Input: vectordb (Chroma), config (dict)</p> <p>Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def initialize_llm_chain(\n    vectordb: Chroma,\n    config : dict\n) -&gt; langchain.chains.retrieval_qa.base.RetrievalQA:\n    \"\"\"\n    Description: Initialize the LLM chain and setup Retrieval QA with the specified configuration.\n\n    Input: vectordb (Chroma), config (dict)\n\n    Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)\n    \"\"\"\n\n    return vectordb.as_retriever(\n        search_type=config[\"search_type\"],\n        search_kwargs={\"k\": config[\"num_return_documents\"]},\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.load_and_process_data","title":"<code>load_and_process_data(metadata_df, page_content_column)</code>","text":"<p>Description: Load and process the data for the vector store. Split the documents into chunks of 1000 characters.</p> <p>Input: metadata_df (pd.DataFrame), page_content_column (str)</p> <p>Returns: chunked documents (list)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def load_and_process_data(metadata_df: pd.DataFrame, page_content_column: str) -&gt; list:\n    \"\"\"\n    Description: Load and process the data for the vector store. Split the documents into chunks of 1000 characters.\n\n    Input: metadata_df (pd.DataFrame), page_content_column (str)\n\n    Returns: chunked documents (list)\n    \"\"\"\n    # Load data\n    loader = DataFrameLoader(metadata_df, page_content_column=page_content_column)\n    documents = loader.load()\n\n    # Split documents\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n    documents = text_splitter.split_documents(documents)\n\n    return documents\n</code></pre>"},{"location":"modules/llm_module/#llm.load_document_and_create_vector_store","title":"<code>load_document_and_create_vector_store(metadata_df, chroma_client, config)</code>","text":"<p>Loads the documents and creates the vector store. If the training flag is set to True, the documents are added to the vector store. If the training flag is set to False, the vector store is loaded from the persist directory.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_df</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required <code>chroma_client</code> <code>PersistentClient</code> <p>The Chroma client.</p> required <code>config</code> <code>dict</code> <p>The configuration dictionary.</p> required <p>Returns:</p> Name Type Description <code>Chroma</code> <code>Chroma</code> <p>The Chroma vector store.</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def load_document_and_create_vector_store(metadata_df: pd.DataFrame, chroma_client:ClientAPI , config: dict) -&gt; Chroma:\n    \"\"\"\n    Loads the documents and creates the vector store. If the training flag is set to True,\n    the documents are added to the vector store. If the training flag is set to False,\n    the vector store is loaded from the persist directory.\n\n    Args:\n        metadata_df (pd.DataFrame): The metadata dataframe.\n        chroma_client (chromadb.PersistentClient): The Chroma client.\n        config (dict): The configuration dictionary.\n\n    Returns:\n        Chroma: The Chroma vector store.\n    \"\"\"\n    embeddings = load_model(config)\n    collection_name = get_collection_name(config)\n\n    if not config[\"training\"]:\n        return load_vector_store(chroma_client, config, embeddings, collection_name)\n\n    return create_vector_store(\n        metadata_df, chroma_client, config, embeddings, collection_name\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.load_model","title":"<code>load_model(config)</code>","text":"<p>Description: Load the model using HuggingFaceEmbeddings.</p> <p>Input: config (dict)</p> <p>Returns: HuggingFaceEmbeddings</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def load_model(config: dict) -&gt; HuggingFaceEmbeddings | None:\n    \"\"\"\n    Description: Load the model using HuggingFaceEmbeddings.\n\n    Input: config (dict)\n\n    Returns: HuggingFaceEmbeddings\n    \"\"\"\n    print(\"[INFO] Loading model...\")\n    model_kwargs = {\"device\": config[\"device\"], \"trust_remote_code\": True}\n    encode_kwargs = {\"normalize_embeddings\": True}\n    embeddings = HuggingFaceEmbeddings(\n        model_name=config[\"embedding_model\"],\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs,\n        show_progress = True,\n        # trust_remote_code=True\n    )\n    print(\"[INFO] Model loaded.\")\n    return embeddings\n</code></pre>"},{"location":"modules/llm_module/#llm.load_vector_store","title":"<code>load_vector_store(chroma_client, config, embeddings, collection_name)</code>","text":"<p>Description: Load the vector store from the persist directory.</p> <p>Input: chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)</p> <p>Returns: Chroma</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def load_vector_store(chroma_client: ClientAPI, config: dict, embeddings: HuggingFaceEmbeddings, collection_name: str) -&gt; Chroma:\n    \"\"\"\n    Description: Load the vector store from the persist directory.\n\n    Input: chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)\n\n    Returns: Chroma\n    \"\"\"\n    if not os.path.exists(config[\"persist_dir\"]):\n        raise Exception(\n            \"Persist directory does not exist. Please run the training pipeline first.\"\n        )\n\n    return Chroma(\n        client=chroma_client,\n        persist_directory=config[\"persist_dir\"],\n        embedding_function=embeddings,\n        collection_name=collection_name,\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.setup_vector_db_and_qa","title":"<code>setup_vector_db_and_qa(config, data_type, client)</code>","text":"<p>Description: Create the vector database using Chroma db with each type of data in its own collection. Doing so allows us to have a single database with multiple collections, reducing the number of databases we need to manage. This also downloads the embedding model if it does not exist. The QA chain is then initialized with the vector store and the configuration.</p> <p>Input: config (dict), data_type (str), client (chromadb.PersistentClient)</p> <p>Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)</p> Source code in <code>backend/modules/llm.py</code> <pre><code>def setup_vector_db_and_qa(config: dict, data_type: str, client:ClientAPI) -&gt; langchain.chains.retrieval_qa.base.RetrievalQA:\n    \"\"\"\n    Description: Create the vector database using Chroma db with each type of data in its own collection. Doing so allows us to have a single database with multiple collections, reducing the number of databases we need to manage.\n    This also downloads the embedding model if it does not exist. The QA chain is then initialized with the vector store and the configuration.\n\n    Input: config (dict), data_type (str), client (chromadb.PersistentClient)\n\n    Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)\n    \"\"\"\n\n    config[\"type_of_data\"] = data_type\n    # Download the data if it does not exist\n    openml_data_object, data_id, all_metadata = get_all_metadata_from_openml(\n        config=config\n    )\n    # Create the combined metadata dataframe\n    metadata_df, all_metadata = create_metadata_dataframe(\n        openml_data_object, data_id, all_metadata, config=config\n    )\n    # Create the vector store\n    vectordb = load_document_and_create_vector_store(\n        metadata_df, config=config, chroma_client=client\n    )\n    # Initialize the LLM chain and setup Retrieval QA\n    qa = initialize_llm_chain(vectordb=vectordb, config=config)\n    return qa\n</code></pre>"},{"location":"modules/metadata_module/","title":"Metadata module","text":""},{"location":"modules/metadata_module/#metadata_utils.combine_metadata","title":"<code>combine_metadata(all_dataset_metadata, all_data_description_df)</code>","text":"<p>Description: Combine the descriptions with the metadata table.</p> <p>Input: all_dataset_metadata (pd.DataFrame) : The metadata table, all_data_description_df (pd.DataFrame) : The descriptions</p> <p>Returns: The combined metadata table.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def combine_metadata(all_dataset_metadata: pd.DataFrame, all_data_description_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Combine the descriptions with the metadata table.\n\n    Input: all_dataset_metadata (pd.DataFrame) : The metadata table,\n    all_data_description_df (pd.DataFrame) : The descriptions\n\n    Returns: The combined metadata table.\n    \"\"\"\n    # Combine the descriptions with the metadata table\n    all_dataset_metadata = pd.merge(\n        all_dataset_metadata, all_data_description_df, on=\"did\", how=\"inner\"\n    )\n\n    # Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n\n    all_dataset_metadata[\"Combined_information\"] = all_dataset_metadata.apply(\n        merge_all_columns_to_string, axis=1\n    )\n    return all_dataset_metadata\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.create_combined_information_df","title":"<code>create_combined_information_df(data_id, descriptions, joined_qualities, joined_features)</code>","text":"<p>Description: Create a dataframe with the combined information of the OpenML object.</p> <p>Input: data_id (int) : The data id, descriptions (list) : The descriptions of the OpenML object, joined_qualities (list) : The joined qualities of the OpenML object, joined_features (list) : The joined features of the OpenML object</p> <p>Returns: The dataframe with the combined information of the OpenML object.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def create_combined_information_df(\n    # data_id, descriptions, joined_qualities, joined_features\n    data_id: int| Sequence[int], descriptions: Sequence[str], joined_qualities: Sequence[str], joined_features: Sequence[str]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Create a dataframe with the combined information of the OpenML object.\n\n    Input: data_id (int) : The data id, descriptions (list) : The descriptions of the OpenML object, joined_qualities (list) : The joined qualities of the OpenML object, joined_features (list) : The joined features of the OpenML object\n\n    Returns: The dataframe with the combined information of the OpenML object.\n    \"\"\"\n    return pd.DataFrame(\n        {\n            \"did\": data_id,\n            \"description\": descriptions,\n            \"qualities\": joined_qualities,\n            \"features\": joined_features,\n        }\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.create_metadata_dataframe","title":"<code>create_metadata_dataframe(openml_data_object, data_id, all_dataset_metadata, config)</code>","text":"<p>Creates a dataframe with all the metadata, joined columns with all information for the type of data specified in the config. If training is set to False, the dataframes are loaded from the files. If training is set to True, the dataframes are created and then saved to the files.</p> <p>Parameters:</p> Name Type Description Default <code>openml_data_object</code> <code>list</code> <p>The list of OpenML objects.</p> required <code>data_id</code> <code>list</code> <p>The list of data ids.</p> required <code>all_dataset_metadata</code> <code>DataFrame</code> <p>The metadata table.</p> required <code>config</code> <code>dict</code> <p>The config dictionary.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The combined metadata dataframe.</p> <code>DataFrame</code> <p>pd.DataFrame: The updated metadata table.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def create_metadata_dataframe(\n    # openml_data_object, data_id, all_dataset_metadata, config\n    openml_data_object: Sequence[Union[openml.datasets.dataset.OpenMLDataset, openml.flows.flow.OpenMLFlow]], data_id: Sequence[int], all_dataset_metadata: pd.DataFrame, config: dict\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Creates a dataframe with all the metadata, joined columns with all information\n    for the type of data specified in the config. If training is set to False,\n    the dataframes are loaded from the files. If training is set to True, the\n    dataframes are created and then saved to the files.\n\n    Args:\n        openml_data_object (list): The list of OpenML objects.\n        data_id (list): The list of data ids.\n        all_dataset_metadata (pd.DataFrame): The metadata table.\n        config (dict): The config dictionary.\n\n    Returns:\n        pd.DataFrame: The combined metadata dataframe.\n        pd.DataFrame: The updated metadata table.\n    \"\"\"\n    # use os.path.join to ensure compatibility with different operating systems\n    file_path = os.path.join(\n        config[\"data_dir\"], f\"all_{config['type_of_data']}_description.csv\"\n    )\n\n    if not config[\"training\"]:\n        return load_metadata(file_path), all_dataset_metadata\n\n    if config[\"type_of_data\"] == \"dataset\":\n        return process_dataset_metadata(\n            openml_data_object, data_id, all_dataset_metadata, file_path\n        )\n\n    if config[\"type_of_data\"] == \"flow\":\n        return process_flow_metadata(openml_data_object, data_id, file_path)\n\n    raise ValueError(f\"Unsupported type_of_data: {config['type_of_data']}\")\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.extract_attribute","title":"<code>extract_attribute(attribute, attr_name)</code>","text":"<p>Description: Extract an attribute from the OpenML object.</p> <p>Input: attribute (object) : The OpenML object</p> <p>Returns: The attribute value if it exists, else an empty string.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def extract_attribute(attribute: object, attr_name: str) -&gt; str:\n    \"\"\"\n    Description: Extract an attribute from the OpenML object.\n\n    Input: attribute (object) : The OpenML object\n\n    Returns: The attribute value if it exists, else an empty string.\n    \"\"\"\n    return getattr(attribute, attr_name, \"\")\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_all_metadata_from_openml","title":"<code>get_all_metadata_from_openml(config)</code>","text":"<p>Description: Gets all the metadata from OpenML for the type of data specified in the config. If training is set to False, it loads the metadata from the files. If training is set to True, it gets the metadata from OpenML.</p> <p>This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.</p> <p>Input: config (dict) : The config dictionary</p> <p>Returns: all the data descriptions combined with data ids, data ids, and the raw openml objects in a dataframe.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_all_metadata_from_openml(config: dict) -&gt; Tuple[pd.DataFrame, Sequence[int], pd.DataFrame] | None:\n    \"\"\"\n    Description: Gets all the metadata from OpenML for the type of data specified in the config.\n    If training is set to False, it loads the metadata from the files. If training is set to True, it gets the metadata from OpenML.\n\n    This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.\n\n\n    Input: config (dict) : The config dictionary\n\n    Returns: all the data descriptions combined with data ids, data ids, and the raw openml objects in a dataframe.\n    \"\"\"\n\n    # save_filename = f\"./data/all_{config['type_of_data']}_metadata.pkl\"\n    # use os.path.join to ensure compatibility with different operating systems\n    save_filename = os.path.join(\n        config[\"data_dir\"], f\"all_{config['type_of_data']}_metadata.pkl\"\n    )\n    # If we are not training, we do not need to recreate the cache and can load the metadata from the files. If the files do not exist, raise an exception.\n    # TODO : Check if this behavior is correct, or if data does not exist, send to training pipeline?\n    if config[\"training\"] == False or config[\"ignore_downloading_data\"] == True:\n        # print(\"[INFO] Training is set to False.\")\n        # Check if the metadata files exist for all types of data\n        if not os.path.exists(save_filename):\n            raise Exception(\n                \"Metadata files do not exist. Please run the training pipeline first.\"\n            )\n        print(\"[INFO] Loading metadata from file.\")\n        # Load the metadata files for all types of data\n        return load_metadata_from_file(save_filename)\n\n    # If we are training, we need to recreate the cache and get the metadata from OpenML\n    if config[\"training\"] == True:\n        print(\"[INFO] Training is set to True.\")\n        # Gather all OpenML objects of the type of data\n        all_objects = get_openml_objects(config[\"type_of_data\"])\n\n        # subset the data for testing\n        if config[\"test_subset_2000\"] == True:\n            print(\"[INFO] Subsetting the data to 100 rows.\")\n            all_objects = all_objects[:100]\n\n        data_id = [int(all_objects.iloc[i][\"did\"]) for i in range(len(all_objects))]\n\n        print(\"[INFO] Initializing cache.\")\n        initialize_cache(config[\"type_of_data\"], data_id)\n\n        print(f\"[INFO] Getting {config['type_of_data']} metadata from OpenML.\")\n        openml_data_object = get_metadata_from_openml(config, data_id)\n\n        print(\"[INFO] Saving metadata to file.\")\n        save_metadata_to_file((openml_data_object, data_id, all_objects), save_filename)\n\n        return openml_data_object, data_id, all_objects\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_dataset_description","title":"<code>get_dataset_description(dataset_id)</code>","text":"<p>Get the dataset description from OpenML using the dataset id</p> <p>Input: dataset_id (int) : The dataset id</p> <p>Returns: data (openml.datasets.dataset.OpenMLDataset) : The dataset object from OpenML</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_dataset_description(dataset_id) -&gt; openml.datasets.dataset.OpenMLDataset:\n    \"\"\"\n    Get the dataset description from OpenML using the dataset id\n\n    Input: dataset_id (int) : The dataset id\n\n    Returns: data (openml.datasets.dataset.OpenMLDataset) : The dataset object from OpenML\n    \"\"\"\n    # TODO : Check for objects that do not have qualities being not downloaded properly\n    # try:\n    data = openml.datasets.get_dataset(\n        dataset_id=dataset_id,\n        download_data=False,\n        download_qualities=True,\n        download_features_meta_data=True,\n    )\n\n    return data\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_flow_description","title":"<code>get_flow_description(flow_id)</code>","text":"<p>Get the flow description from OpenML using the flow id</p> <p>Input: flow_id (int) : The flow id</p> <p>Returns: data (openml.flows.flow.OpenMLFlow) : The flow object from OpenML</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_flow_description(flow_id: int) -&gt; openml.flows.flow.OpenMLFlow:\n    \"\"\"\n    Get the flow description from OpenML using the flow id\n\n    Input: flow_id (int) : The flow id\n\n    Returns: data (openml.flows.flow.OpenMLFlow) : The flow object from OpenML\n    \"\"\"\n    return openml.flows.get_flow(flow_id=flow_id)\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_metadata_from_openml","title":"<code>get_metadata_from_openml(config, data_id)</code>","text":"<p>Get metadata from OpenML using parallel processing.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_metadata_from_openml(config, data_id: Sequence[int]):\n    \"\"\"\n    Get metadata from OpenML using parallel processing.\n    \"\"\"\n    if config[\"type_of_data\"] == \"dataset\":\n        return pqdm(\n            data_id, get_dataset_description, n_jobs=config[\"data_download_n_jobs\"]\n        )\n    elif config[\"type_of_data\"] == \"flow\":\n        return pqdm(\n            data_id, get_flow_description, n_jobs=config[\"data_download_n_jobs\"]\n        )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_openml_objects","title":"<code>get_openml_objects(type_of_data)</code>","text":"<p>Get OpenML objects based on the type of data.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def get_openml_objects(type_of_data: str):\n    \"\"\"\n    Get OpenML objects based on the type of data.\n    \"\"\"\n    if type_of_data == \"dataset\":\n        return openml.datasets.list_datasets(output_format=\"dataframe\")\n    elif type_of_data == \"flow\":\n        all_objects = openml.flows.list_flows(output_format=\"dataframe\")\n        return all_objects.rename(columns={\"id\": \"did\"})\n    else:\n        raise ValueError(\"Invalid type_of_data specified\")\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.initialize_cache","title":"<code>initialize_cache(type_of_data, data_id)</code>","text":"<p>Initialize cache for the OpenML objects.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def initialize_cache(type_of_data: str, data_id: Sequence[int]) -&gt; None:\n    \"\"\"\n    Initialize cache for the OpenML objects.\n    \"\"\"\n    if type_of_data == \"dataset\":\n        get_dataset_description(data_id[0])\n    elif type_of_data == \"flow\":\n        get_flow_description(data_id[0])\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.join_attributes","title":"<code>join_attributes(attribute, attr_name)</code>","text":"<p>Description: Join the attributes of the OpenML object.</p> <p>Input: attribute (object) : The OpenML object</p> <p>Returns: The joined attributes if they exist, else an empty string. example: \"column - value, column - value, ...\"</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def join_attributes(attribute: object, attr_name: str) -&gt; str:\n    \"\"\"\n    Description: Join the attributes of the OpenML object.\n\n    Input: attribute (object) : The OpenML object\n\n    Returns: The joined attributes if they exist, else an empty string.\n    example: \"column - value, column - value, ...\"\n    \"\"\"\n\n    return (\n        \" \".join([f\"{k} : {v},\" for k, v in getattr(attribute, attr_name, {}).items()])\n        if hasattr(attribute, attr_name)\n        else \"\"\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.load_metadata_from_file","title":"<code>load_metadata_from_file(save_filename)</code>","text":"<p>Load metadata from a file.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def load_metadata_from_file(save_filename: str) -&gt; Tuple[pd.DataFrame, Sequence[int], pd.DataFrame]:\n    \"\"\"\n    Load metadata from a file.\n    \"\"\"\n    with open(save_filename, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.merge_all_columns_to_string","title":"<code>merge_all_columns_to_string(row)</code>","text":"<p>Description: Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"</p> <p>Input: row (pd.Series) : The row of the dataframe</p> <p>Returns: The combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def merge_all_columns_to_string(row: pd.Series) -&gt; str:\n    \"\"\"\n    Description: Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n\n    Input: row (pd.Series) : The row of the dataframe\n\n    Returns: The combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n    \"\"\"\n\n    return \" \".join([f\"{col} - {val},\" for col, val in zip(row.index, row.values)])\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.process_dataset_metadata","title":"<code>process_dataset_metadata(openml_data_object, data_id, all_dataset_metadata, file_path)</code>","text":"<p>Description: Process the dataset metadata.</p> <p>Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, all_dataset_metadata (pd.DataFrame) : The metadata table, file_path (str) : The file path</p> <p>Returns: The combined metadata dataframe and the updated metadata table.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def process_dataset_metadata(\n    openml_data_object: Sequence[openml.datasets.dataset.OpenMLDataset], data_id: Sequence[int], all_dataset_metadata: pd.DataFrame, file_path: str\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Description: Process the dataset metadata.\n\n    Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, all_dataset_metadata (pd.DataFrame) : The metadata table, file_path (str) : The file path\n\n    Returns: The combined metadata dataframe and the updated metadata table.\n    \"\"\"\n    descriptions = [\n        extract_attribute(attr, \"description\") for attr in openml_data_object\n    ]\n    joined_qualities = [\n        join_attributes(attr, \"qualities\") for attr in openml_data_object\n    ]\n    joined_features = [join_attributes(attr, \"features\") for attr in openml_data_object]\n\n    all_data_description_df = create_combined_information_df(\n        data_id, descriptions, joined_qualities, joined_features\n    )\n    all_dataset_metadata = combine_metadata(\n        all_dataset_metadata, all_data_description_df\n    )\n\n    all_dataset_metadata.to_csv(file_path)\n\n    return (\n        all_dataset_metadata[[\"did\", \"name\", \"Combined_information\"]],\n        all_dataset_metadata,\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.process_flow_metadata","title":"<code>process_flow_metadata(openml_data_object, data_id, file_path)</code>","text":"<p>Description: Process the flow metadata.</p> <p>Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, file_path (str) : The file path</p> <p>Returns: The combined metadata dataframe and the updated metadata table.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def process_flow_metadata(openml_data_object: Sequence[openml.flows.flow.OpenMLFlow], data_id: Sequence[int], file_path: str) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Description: Process the flow metadata.\n\n    Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, file_path (str) : The file path\n\n    Returns: The combined metadata dataframe and the updated metadata table.\n    \"\"\"\n    descriptions = [\n        extract_attribute(attr, \"description\") for attr in openml_data_object\n    ]\n    names = [extract_attribute(attr, \"name\") for attr in openml_data_object]\n    tags = [extract_attribute(attr, \"tags\") for attr in openml_data_object]\n\n    all_data_description_df = pd.DataFrame(\n        {\n            \"did\": data_id,\n            \"description\": descriptions,\n            \"name\": names,\n            \"tags\": tags,\n        }\n    )\n\n    all_data_description_df[\"Combined_information\"] = all_data_description_df.apply(\n        merge_all_columns_to_string, axis=1\n    )\n    all_data_description_df.to_csv(file_path)\n\n    return (\n        all_data_description_df[[\"did\", \"name\", \"Combined_information\"]],\n        all_data_description_df,\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.save_metadata_to_file","title":"<code>save_metadata_to_file(data, save_filename)</code>","text":"<p>Save metadata to a file.</p> Source code in <code>backend/modules/metadata_utils.py</code> <pre><code>def save_metadata_to_file(data, save_filename: str):\n    \"\"\"\n    Save metadata to a file.\n    \"\"\"\n    with open(save_filename, \"wb\") as f:\n        pickle.dump(data, f)\n</code></pre>"},{"location":"modules/result_gen/","title":"Result gen","text":""},{"location":"modules/result_gen/#results_gen.aggregate_multiple_queries_and_count","title":"<code>aggregate_multiple_queries_and_count(queries, qa_dataset, config, group_cols=['id', 'name'], sort_by='query', count=True)</code>","text":"<p>Description: Aggregate the results of multiple queries into a single dataframe and count the number of times a dataset appears in the results</p> Input <p>queries: List of queries group_cols: List of columns to group by</p> <p>Returns: Combined dataframe with the results of all queries</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def aggregate_multiple_queries_and_count(\n    queries, qa_dataset, config, group_cols=[\"id\", \"name\"], sort_by=\"query\", count = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Aggregate the results of multiple queries into a single dataframe and count the number of times a dataset appears in the results\n\n    Input:\n        queries: List of queries\n        group_cols: List of columns to group by\n\n    Returns: Combined dataframe with the results of all queries\n    \"\"\"\n    combined_df = pd.DataFrame()\n    for query in tqdm(queries, total=len(queries)):\n        result_data_frame, _ = get_result_from_query(\n            query=query, qa=qa_dataset, type_of_query=\"dataset\", config=config\n        )\n        result_data_frame = result_data_frame[group_cols]\n        # Concat with combined_df with a column to store the query\n        result_data_frame[\"query\"] = query\n        combined_df = pd.concat([combined_df, result_data_frame])\n    if count:\n        combined_df = (\n        combined_df.groupby(group_cols)\n        .count()\n        .reset_index()\n        .sort_values(by=sort_by, ascending=False)\n    )\n\n    return combined_df\n</code></pre>"},{"location":"modules/result_gen/#results_gen.check_query","title":"<code>check_query(query)</code>","text":"<p>Description: Performs checks on the query - Replaces %20 with space character (browsers do this automatically when spaces are in the URL) - Removes leading and trailing spaces - Limits the query to 150 characters</p> <p>Input: query (str)</p> <p>Returns: None</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def check_query(query: str) -&gt; str:\n    \"\"\"\n    Description: Performs checks on the query\n    - Replaces %20 with space character (browsers do this automatically when spaces are in the URL)\n    - Removes leading and trailing spaces\n    - Limits the query to 150 characters\n\n    Input: query (str)\n\n    Returns: None\n    \"\"\"\n    if query == \"\":\n        raise ValueError(\"Query cannot be empty.\")\n    query = query.replace(\n        \"%20\", \" \"\n    )  # replace %20 with space character (browsers do this automatically when spaces are in the URL)\n    # query = query.replace(\"dataset\", \"\")\n    # query = query.replace(\"flow\", \"\")\n    query = query.strip()\n    query = query[:200]\n    return query\n</code></pre>"},{"location":"modules/result_gen/#results_gen.create_output_dataframe","title":"<code>create_output_dataframe(dict_results, type_of_data, ids_order)</code>","text":"<p>Description: Create an output dataframe with the results. The URLs are API calls to the OpenML API for the specific type of data.</p> <p>Input: dict_results (dict), type_of_data (str)</p> <p>Returns: A dataframe with the results and duplicate names removed.</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def create_output_dataframe(dict_results: dict, type_of_data: str, ids_order: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Create an output dataframe with the results. The URLs are API calls to the OpenML API for the specific type of data.\n\n    Input: dict_results (dict), type_of_data (str)\n\n    Returns: A dataframe with the results and duplicate names removed.\n    \"\"\"\n    output_df = pd.DataFrame(dict_results).T.reset_index()\n    # order the rows based on the order of the ids\n    output_df[\"index\"] = output_df[\"index\"].astype(int)\n    output_df = output_df.set_index(\"index\").loc[ids_order].reset_index()\n    # output_df[\"urls\"] = output_df[\"index\"].apply(\n    #     lambda x: f\"https://www.openml.org/api/v1/json/{type_of_data}/{x}\"\n    # )\n    # https://www.openml.org/search?type=data&amp;sort=runs&amp;status=any&amp;id=31\n    output_df[\"urls\"] = output_df[\"index\"].apply(\n        lambda x: f\"https://www.openml.org/search?type={type_of_data}&amp;id={x}\"\n    )\n    output_df[\"urls\"] = output_df[\"urls\"].apply(make_clickable)\n    # data = openml.datasets.get_dataset(\n    # get rows with unique names\n    if type_of_data == \"data\":\n        output_df[\"command\"] = output_df[\"index\"].apply(\n            lambda x: f\"dataset = openml.datasets.get_dataset({x})\"\n        )\n    elif type_of_data == \"flow\":\n        output_df[\"command\"] = output_df[\"index\"].apply(\n            lambda x: f\"flow = openml.flows.get_flow({x})\"\n        )\n    output_df = output_df.drop_duplicates(subset=[\"name\"])\n    # order the columns\n    output_df = output_df[[\"index\", \"name\", \"command\", \"urls\", \"page_content\"]].rename(\n        columns={\"index\": \"id\", \"urls\": \"OpenML URL\", \"page_content\": \"Description\"}\n    )\n    return output_df\n</code></pre>"},{"location":"modules/result_gen/#results_gen.fetch_results","title":"<code>fetch_results(query, qa, type_of_query, config)</code>","text":"<p>Description: Fetch results for the query using the QA chain.</p> <p>Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str), config (dict)</p> <p>Returns: results[\"source_documents\"] (list)</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def fetch_results(query: str, qa: langchain.chains.retrieval_qa.base.RetrievalQA, type_of_query: str, config: dict) -&gt; Sequence[Document]:\n    \"\"\"\n    Description: Fetch results for the query using the QA chain.\n\n    Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str), config (dict)\n\n    Returns: results[\"source_documents\"] (list)\n    \"\"\"\n    results = qa.invoke(\n        input=query,\n        config={\"temperature\": config[\"temperature\"], \"top-p\": config[\"top_p\"]},\n    )\n    if config[\"long_context_reorder\"] == True:\n        results = long_context_reorder(results)\n    id_column = {\"dataset\": \"did\", \"flow\": \"id\", \"data\": \"did\"}\n    id_column = id_column[type_of_query]\n\n    if config[\"reranking\"] == True:\n        try:\n            print(\"[INFO] Reranking results...\")\n            ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/tmp/\")\n            rerankrequest = RerankRequest(\n                query=query,\n                passages=[\n                    {\"id\": result.metadata[id_column], \"text\": result.page_content}\n                    for result in results\n                ],\n            )\n            ranking = ranker.rerank(rerankrequest)\n            ids = [result[\"id\"] for result in ranking]\n            ranked_results = [\n                result for result in results if result.metadata[id_column] in ids\n            ]\n            print(\"[INFO] Reranking complete.\")\n            return ranked_results\n        except Exception as e:\n            print(f\"[ERROR] Reranking failed: {e}\")\n            return results\n\n    else:\n        return results\n</code></pre>"},{"location":"modules/result_gen/#results_gen.get_result_from_query","title":"<code>get_result_from_query(query, qa, type_of_query, config)</code>","text":"<p>Description: Get the result from the query using the QA chain and return the results in a dataframe that is then sent to the frontend.</p> <p>Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str)</p> <p>Returns: output_df (pd.DataFrame)</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def get_result_from_query(query, qa, type_of_query, config) -&gt; Tuple[pd.DataFrame, Sequence[Document]]:\n    \"\"\"\n    Description: Get the result from the query using the QA chain and return the results in a dataframe that is then sent to the frontend.\n\n    Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str)\n\n    Returns: output_df (pd.DataFrame)\n    \"\"\"\n    if type_of_query == \"dataset\":\n        # Fixing the key_name for dataset because of the way the OpenML API returns the data\n        type_of_query = \"data\"\n    elif type_of_query == \"flow\":\n        type_of_query = \"flow\"\n    else:\n        raise ValueError(f\"Unsupported type_of_data: {type_of_query}\")\n\n    # Process the query\n    query = check_query(query)\n    if query == \"\":\n        return pd.DataFrame(), []\n    source_documents = fetch_results(\n        query, qa, config=config, type_of_query=type_of_query\n    )\n    dict_results, ids_order = process_documents(source_documents)\n    output_df = create_output_dataframe(dict_results, type_of_query, ids_order)\n\n    return output_df, source_documents\n</code></pre>"},{"location":"modules/result_gen/#results_gen.long_context_reorder","title":"<code>long_context_reorder(results)</code>","text":"<p>Description: Lost in the middle reorder: the less relevant documents will be at the middle of the list and more relevant elements at beginning / end. See: https://arxiv.org/abs//2307.03172</p> <p>Input: results (list)</p> <p>Returns: reorder results (list)</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def long_context_reorder(results: Sequence[Document]) -&gt; Sequence[Document]:\n    \"\"\"\n    Description: Lost in the middle reorder: the less relevant documents will be at the\n    middle of the list and more relevant elements at beginning / end.\n    See: https://arxiv.org/abs//2307.03172\n\n    Input: results (list)\n\n    Returns: reorder results (list)\n    \"\"\"\n    print(\"[INFO] Reordering results...\")\n    reordering = LongContextReorder()\n    results = reordering.transform_documents(results)\n    print(\"[INFO] Reordering complete.\")\n    return results\n</code></pre>"},{"location":"modules/result_gen/#results_gen.make_clickable","title":"<code>make_clickable(val)</code>","text":"<p>Description: Make the URL clickable in the dataframe.</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def make_clickable(val : str) -&gt; str:\n    \"\"\"\n    Description: Make the URL clickable in the dataframe.\n    \"\"\"\n    return '&lt;a href=\"{}\"&gt;{}&lt;/a&gt;'.format(val, val)\n</code></pre>"},{"location":"modules/result_gen/#results_gen.process_documents","title":"<code>process_documents(source_documents)</code>","text":"<p>Description: Process the source documents and create a dictionary with the key_name as the key and the name and page content as the values.</p> <p>Input: source_documents (list), key_name (str)</p> <p>Returns: dict_results (dict)</p> Source code in <code>backend/modules/results_gen.py</code> <pre><code>def process_documents(source_documents : Sequence[Document]) -&gt; Tuple[OrderedDict, list]:\n    \"\"\"\n    Description: Process the source documents and create a dictionary with the key_name as the key and the name and page content as the values.\n\n    Input: source_documents (list), key_name (str)\n\n    Returns: dict_results (dict)\n    \"\"\"\n    dict_results = OrderedDict()\n    for result in source_documents:\n        dict_results[result.metadata[\"did\"]] = {\n            \"name\": result.metadata[\"name\"],\n            \"page_content\": result.page_content,\n        }\n    ids = [result.metadata[\"did\"] for result in source_documents]\n    return dict_results, ids\n</code></pre>"}]}