# !!!! IMPORTANT !!!!
# - Run the `./start_llm_service.sh` (root of this repository) before running this notebook. This will start the Language Model service. Without this, parts of the code will not work.

# %%
from __future__ import annotations

import json
from pathlib import Path
import os
from backend.modules.utils import load_config_and_device
from training_utils import *
from experiments import *

if __name__ == "__main__":
    # %%
    new_path = Path("../backend/")
    eval_path = Path("../data/evaluation/")

    config = load_config_and_device(str(new_path / "config.json"), training=True)

    config["type_of_data"] = "dataset"
    config["training"] = True

    # %% [markdown]
    # ## Defining the models used
    # - Embedding models are any from Huggingface hub
    # - LLM models are any from Ollama library

    # %%
    list_of_embedding_models = [
        "BAAI/bge-large-en-v1.5",
        "BAAI/bge-base-en-v1.5",
        "Snowflake/snowflake-arctic-embed-l",
        "Alibaba-NLP/gte-large-en-v1.5",
    ]
    list_of_llm_models = ["llama3"]

    # %% [markdown]
    # ## Downloading the LLM models
    # - PLEASE MAKE SURE YOU HAVE DOWNLOADED OLLAMA
    # - Linux/Unix : ```curl -fsSL https://ollama.com/install.sh | sh```

    # %%
    ollama_setup(list_of_llm_models)

    # %% [markdown]
    # ## Setup evaluation data
    # ### If you used tools/labellingapp.py to generate evaluation data
    # - You can ignore this and use the data generated by the tool
    # ### If you did not
    # - You can use evaluation data of the format {"id": ["tag1", "tag2"] } and save it as a json file
    # - eg: ```{"43843": ["Climate change"], "43365": ["COVID-19"], "43684": ["COVID-19"]}```

    # %%

    with open(eval_path / "query_templates.txt", "r") as f:
        query_templates = f.readlines()
        query_templates = [x.strip() for x in query_templates]
    # %%
    # with open(eval_path/"merged_labels.", "r") as f:
    # merged_labels = json.load(f)
    # # get the dataset ids we want out evaluation to be based on (these are dataset ids for the openml datasets)
    # subset_ids = list(merged_labels.keys())

    # get subset ids
    load_eval_queries = pd.read_csv(eval_path / "merged_labels.csv")[
        ["Topics", "Dataset IDs"]
    ]
    # %%
    subset_ids = [row.split(",") for row in load_eval_queries["Dataset IDs"].to_list()]
    # flatten the list and get unique values
    subset_ids = list(set([int(item) for sublist in subset_ids for item in sublist]))
    # %%
    # get the queries for the datasets
    query_key_dict = get_queries(
        query_templates=query_templates, load_eval_queries=load_eval_queries
    )
    json.dump(query_key_dict, open(eval_path / "query_key_dict.json", "w"))

    # Run experiments on just queries and not filters

    # Get results from elastic search
    exp_0(process_query_elastic_search, eval_path, query_key_dict)

    # Experiment 1 : Run the base experiments using different models and embeddings
    exp_1(
        eval_path,
        config,
        list_of_embedding_models,
        list_of_llm_models,
        subset_ids,
        query_key_dict,
    )

    # Experiment 2 : Evaluating temperature = 1 (default was 0.95)
    exp_2(eval_path, config, subset_ids, query_key_dict)

    # Experiment 3 : Evaluating search type [mmr, similarity_score_threshold] (default was similarity)
    exp_3(eval_path, config, subset_ids, query_key_dict)

    # Experiment 4 : Evaluating chunk size. The default is 1000, trying out 512,128
    exp_4(eval_path, config, subset_ids, query_key_dict)
