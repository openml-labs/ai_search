# !!!! IMPORTANT !!!!
# - Run the `./start_llm_service.sh` (root of this repository) before running this notebook. This will start the Language Model service. Without this, parts of the code will not work.

# %%
from __future__ import annotations

import json
from pathlib import Path

from backend.modules.utils import load_config_and_device
from training_utils import *

if __name__ == "__main__":
    # %%
    new_path = Path("../backend/")
    eval_path = Path("../data/evaluation/")

    config = load_config_and_device(str(new_path / "config.json"), training=True)

    config["type_of_data"] = "dataset"
    config["training"] = True

    # %% [markdown]
    # ## Defining the models used
    # - Embedding models are any from Huggingface hub
    # - LLM models are any from Ollama library

    # %%
    list_of_embedding_models = [
        "BAAI/bge-large-en-v1.5",
        "BAAI/bge-base-en-v1.5",
        "Snowflake/snowflake-arctic-embed-l",
        "Alibaba-NLP/gte-large-en-v1.5"
    ]
    list_of_llm_models = ["llama3", "phi3"]

    # %% [markdown]
    # ## Downloading the LLM models
    # - PLEASE MAKE SURE YOU HAVE DOWNLOADED OLLAMA
    # - Linux/Unix : ```curl -fsSL https://ollama.com/install.sh | sh```

    # %%
    ollama_setup(list_of_llm_models)

    # %% [markdown]
    # ## Setup evaluation data
    # ### If you used tools/labellingapp.py to generate evaluation data
    # - You can ignore this and use the data generated by the tool
    # ### If you did not
    # - You can use evaluation data of the format {"id": ["tag1", "tag2"] } and save it as a json file
    # - eg: ```{"43843": ["Climate change"], "43365": ["COVID-19"], "43684": ["COVID-19"]}```

    # %%

    with open(eval_path / "query_templates.txt", "r") as f:
        query_templates = f.readlines()
        query_templates = [x.strip() for x in query_templates]
    # %%
    # with open(eval_path/"merged_labels.", "r") as f:
    # merged_labels = json.load(f)
    # # get the dataset ids we want out evaluation to be based on (these are dataset ids for the openml datasets)
    # subset_ids = list(merged_labels.keys())

    # get subset ids
    load_eval_queries = pd.read_csv(eval_path / "merged_labels.csv")[
        ["Topics", "Dataset IDs"]
    ]
    # %%
    subset_ids = [row.split(",") for row in load_eval_queries["Dataset IDs"].to_list()]
    # flatten the list and get unique values
    subset_ids = list(set([int(item) for sublist in subset_ids for item in sublist]))
    # %%
    # get the queries for the datasets
    query_key_dict = {}
    for template in query_templates:
        for row in load_eval_queries.itertuples():
            new_query = f"{template} {row[1]}".strip()
            # load_eval_queries.at[query, "Query"] = new_query
            if new_query not in query_key_dict:
                query_key_dict[new_query.strip()] = row[2]

    json.dump(query_key_dict, open(eval_path / "query_key_dict.json", "w"))

    """
    EXPERIMENT 1
    Main evaluation loop that is used to run the base experiments using different models and embeddings.
    Takes into account the following:
    original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing
    list_of_embedding_models = [
        "BAAI/bge-large-en-v1.5",
        "BAAI/bge-base-en-v1.5",
        "Snowflake/snowflake-arctic-embed-l",
    ]
    list_of_llm_models = ["llama3", "phi3"]
    types_of_llm_apply : llm applied as filter before the RAG pipeline, llm applied as reranker after the RAG pipeline, llm not used at all
    """

    expRunner = ExperimentRunner(
        config=config,
        eval_path=eval_path,
        queries=query_key_dict.keys(),
        list_of_embedding_models=list_of_embedding_models,
        list_of_llm_models=list_of_llm_models,
        subset_ids=subset_ids,
        use_cached_experiment=True,
    )
    expRunner.run_experiments()

    """
    EXPERIMENT 2
    
    Evaluating temperature = 1 (default was 0.95)
    Takes into account the following:
    original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing
    list_of_embedding_models = [
        "BAAI/bge-large-en-v1.5",
    ]
    list_of_llm_models = ["llama3"]
    types_of_llm_apply : llm applied as filter before the RAG pipeline, llm applied as reranker after the RAG pipeline, llm not used at all
    """

    list_of_embedding_models = [
        "BAAI/bge-large-en-v1.5",
    ]
    list_of_llm_models = ["llama3"]
    config["temperature"] = 1

    expRunner = ExperimentRunner(
        config=config,
        eval_path=eval_path,
        queries=query_key_dict.keys(),
        list_of_embedding_models=list_of_embedding_models,
        list_of_llm_models=list_of_llm_models,
        subset_ids=subset_ids,
        use_cached_experiment=True,
        custom_name="temperature_1",
    )
    expRunner.run_experiments()

    # reset the temperature to the default value
    config["temperature"] = 0.95

    """
    EXPERIMENT 3
    
    Evaluating search type [mmr, similarity_score_threshold] (default was similarity)
    Takes into account the following:
    original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing
    list_of_embedding_models = [
        "BAAI/bge-large-en-v1.5",
    ]
    list_of_llm_models = ["llama3"]
    types_of_llm_apply : llm applied as reranker after the RAG pipeline
    """


    list_of_embedding_models = [
        "BAAI/bge-large-en-v1.5",
    ]
    list_of_llm_models = ["llama3"]
    types_of_llm_apply = [False]
    types_of_search = ["mmr", "similarity_score_threshold"]

    for type_of_search in types_of_search:
        config["search_type"] = type_of_search
        expRunner = ExperimentRunner(
            config=config,
            eval_path=eval_path,
            queries=query_key_dict.keys(),
            list_of_embedding_models=list_of_embedding_models,
            list_of_llm_models=list_of_llm_models,
            subset_ids=subset_ids,
            use_cached_experiment=True,
            custom_name=f"{type_of_search}_search",
            types_of_llm_apply=types_of_llm_apply,
        )
        expRunner.run_experiments()

    # reset the search type to the default value
    config["search_type"] = "similarity"


    """
    EXPERIMENT 4
    
    Evaluating chunk size. The default is 1000, trying out 512,128
    Takes into account the following:
    original data ingestion pipeline : combine a string of all metadata fields and the dataset description and embeds them with no pre-processing
    list_of_embedding_models = [
        "BAAI/bge-large-en-v1.5",
    ]
    list_of_llm_models = ["llama3"]
    types_of_llm_apply : llm applied as reranker after the RAG pipeline
    """


    list_of_embedding_models = [
        "BAAI/bge-large-en-v1.5",
    ]
    list_of_llm_models = ["llama3"]
    types_of_llm_apply = [False]
    types_of_chunk = [512, 128]
    for type_of_chunk in types_of_chunk:
        config["chunk_size"] = type_of_chunk
        expRunner = ExperimentRunner(
            config=config,
            eval_path=eval_path,
            queries=query_key_dict.keys(),
            list_of_embedding_models=list_of_embedding_models,
            list_of_llm_models=list_of_llm_models,
            subset_ids=subset_ids,
            use_cached_experiment=True,
            custom_name=f"{type_of_chunk}_chunk",
            types_of_llm_apply=types_of_llm_apply,
        )
        expRunner.run_experiments()

    # reset the search type to the default value
    config["chunk_size"] = 1000
